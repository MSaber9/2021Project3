{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data2018v1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WKWZTWdPyDPm",
        "t0qdjkFuxi6A",
        "gBlRuIuq2wfS",
        "C6nLbynH4wIf",
        "txKefY_YUYhd",
        "0VZD2kAnVPvp",
        "FzyAYhzW8H9j",
        "HNMU79TpuzHW",
        "gqJnFIjh-dwC",
        "iyXiaKPDU5sk",
        "4J2nORvtVHba",
        "quud7ZacYiRT",
        "FkLHKWlE-CJy",
        "MRcWpTc0FcHa",
        "eKFd28BLFm5R",
        "cDdmmlz4yb_y",
        "7kOXwchaylzp",
        "AmOQWiIvzIlP",
        "zwoYPof4KbDF",
        "YEbbw6yKmfz2",
        "l7gowTjdx2g-",
        "rWCODwf6yCgQ",
        "RIsJNky9t9dL",
        "D0dch6hSt9dO",
        "BP6HRui5t9dP",
        "FgyMedYOt9dS",
        "OWipfrqst9dV",
        "HPQowDrQt9dV",
        "BjmRC32Ut9dW",
        "7OagVZhIt9dX",
        "wJO0yXGDt9da",
        "T4hEG2_Gt9dd",
        "ECR-gi7Ct9de",
        "EKwzBZ9Ot9dg"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSaber9/2021Project3/blob/main/Data2018v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKWZTWdPyDPm"
      },
      "source": [
        "### Install "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FngN5UE0Fi9k",
        "outputId": "c6f3c46f-4b45-421c-91d3-e752f011086c"
      },
      "source": [
        "pip install catboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-0.26.1-cp37-none-manylinux1_x86_64.whl (67.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 67.4 MB 80 kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.26.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d2j5SIgvqVX",
        "outputId": "bd356e33-df4f-434b-c88f-4883c9325a9a"
      },
      "source": [
        "pip install shap"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.39.0.tar.gz (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.1.5)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.62.0)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.0.1)\n",
            "Building wheels for collected packages: shap\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491645 sha256=6c0f609ae6e2fc6ae82b06355a3aac02d4854219feac88f1b6f6f118ee0a2cf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n",
            "Successfully built shap\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.39.0 slicer-0.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smc8lri5DdS9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA1zOvmPwKUR",
        "outputId": "00b60b74-35e0-42ec-996f-300d0df72811"
      },
      "source": [
        "pip install scikit-plot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Installing collected packages: scikit-plot\n",
            "Successfully installed scikit-plot-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySrHGT2YxP5G",
        "outputId": "c9adc07e-f298-403e-88f3-2878eff92608"
      },
      "source": [
        "pip install eli5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting eli5\n",
            "  Downloading eli5-0.11.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 106 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.7/dist-packages (from eli5) (21.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from eli5) (2.11.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from eli5) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from eli5) (1.4.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from eli5) (0.8.9)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from eli5) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from eli5) (0.22.2.post1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from eli5) (0.10.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->eli5) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->eli5) (2.0.1)\n",
            "Installing collected packages: eli5\n",
            "Successfully installed eli5-0.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr14ail6x53l",
        "outputId": "1ce40093-400e-45ec-d08b-1ae5194bbef1"
      },
      "source": [
        "pip install catboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.7/dist-packages (0.26.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6VVlo6Bm5vs"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKqEnCuot9cs",
        "outputId": "4fe2df0b-161b-4fcb-f81d-af18a18188f5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "\n",
        "import glob\n",
        "import os\n",
        "#import shap\n",
        "import scikitplot as skplt\n",
        "import eli5\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, average_precision_score\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_recall_fscore_support\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from catboost import Pool\n",
        "from eli5.sklearn import PermutationImportance\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.cluster import hierarchy\n",
        "from collections import defaultdict\n",
        "from scipy.stats import ks_2samp\n",
        "from scipy.stats import describe\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from collections import Counter\n",
        "from IPython.display import display\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEPvth8TubFt",
        "outputId": "864bbd47-3013-446d-b678-cefa5dd87210"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlQg1X4yua2W",
        "outputId": "d34ddbe3-5232-4dcc-a8e3-838a2b0e0d31"
      },
      "source": [
        "!ls '/content/drive/My Drive/Colab Notebooks/Newi/2021Project3/Dataup'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/drive/My Drive/Colab Notebooks/Newi/2021Project3/Dataup': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M66cOd1ufwj",
        "outputId": "4f981481-8677-487f-b011-02e567165013"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Newi/2021Project3/Dataupdate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3/Dataupdate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeNACQk3ufsx",
        "outputId": "d8b3b3a9-1fd2-4bcd-c57c-dff3e25e164d"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data2018\n",
            "bot_02-03-2018.csv\n",
            "bruteforce-ftp-ssh_14-02-2018.csv\n",
            "bruteforce-web-xss_sql-injection_22-02-2018.csv\n",
            "bruteforce-web-xss_sql-injection_23-02-2018.csv\n",
            "ddos-loic-http-loic-udp_20-02-2018.csv\n",
            "ddos-loic-udp_hoic_21-02-2018.csv\n",
            "dos-goldeneye-slowloris_15-02-2018.csv\n",
            "dos-slowhttp-hulk_16-02-2018.csv\n",
            "infiltration_01-03-2018.csv\n",
            "infiltration_28-02-2018.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zVUwC1Gufpw"
      },
      "source": [
        "dataset_base_path = '/content/drive/My Drive/Colab Notebooks/Newi/2021Project3/Dataupdate' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cabGjdqPt9cu"
      },
      "source": [
        "types = { \n",
        "    'dst_port': 'uint32',\n",
        "    'protocol': 'uint8',\n",
        "    'timestamp': 'object',\n",
        "    'flow_duration': 'int64',\n",
        "    'tot_fwd_pkts': 'uint32',\n",
        "    'tot_bwd_pkts': 'uint32',\n",
        "    'totlen_fwd_pkts': 'uint32',\n",
        "    'totlen_bwd_pkts': 'uint32',\n",
        "    'fwd_pkt_len_max': 'uint16',\n",
        "    'fwd_pkt_len_min': 'uint16',\n",
        "    'fwd_pkt_len_mean': 'float32',\n",
        "    'fwd_pkt_len_std': 'float32',\n",
        "    'bwd_pkt_len_max': 'uint16',\n",
        "    'bwd_pkt_len_min': 'uint16',\n",
        "    'bwd_pkt_len_mean': 'float32',\n",
        "    'bwd_pkt_len_std': 'float32',\n",
        "    'flow_byts_s': 'float64',\n",
        "    'flow_pkts_s': 'float64',\n",
        "    'flow_iat_mean': 'float32',\n",
        "    'flow_iat_std': 'float32',\n",
        "    'flow_iat_max': 'int64',\n",
        "    'flow_iat_min': 'int64',\n",
        "    'fwd_iat_tot': 'int64',\n",
        "    'fwd_iat_mean': 'float32',\n",
        "    'fwd_iat_std': 'float32',\n",
        "    'fwd_iat_max': 'int64',\n",
        "    'fwd_iat_min': 'int64',\n",
        "    'bwd_iat_tot': 'uint32',\n",
        "    'bwd_iat_mean': 'float32',\n",
        "    'bwd_iat_std': 'float32',\n",
        "    'bwd_iat_max': 'uint32',\n",
        "    'bwd_iat_min': 'uint32',\n",
        "    'fwd_psh_flags': 'uint8',\n",
        "    'bwd_psh_flags': 'uint8',\n",
        "    'fwd_urg_flags': 'uint8',\n",
        "    'bwd_urg_flags': 'uint8',\n",
        "    'fwd_header_len': 'uint32',\n",
        "    'bwd_header_len': 'uint32',\n",
        "    'fwd_pkts_s': 'float32',\n",
        "    'bwd_pkts_s': 'float32',\n",
        "    'pkt_len_min': 'uint16',\n",
        "    'pkt_len_max': 'uint16',\n",
        "    'pkt_len_mean': 'float32',\n",
        "    'pkt_len_std': 'float32',\n",
        "    'pkt_len_var': 'float32',\n",
        "    'fin_flag_cnt': 'uint8',\n",
        "    'syn_flag_cnt': 'uint8',\n",
        "    'rst_flag_cnt': 'uint8',\n",
        "    'psh_flag_cnt': 'uint8',\n",
        "    'ack_flag_cnt': 'uint8',\n",
        "    'urg_flag_cnt': 'uint8',\n",
        "    'cwe_flag_count': 'uint8',\n",
        "    'ece_flag_cnt': 'uint8',\n",
        "    'down_up_ratio': 'uint16',\n",
        "    'pkt_size_avg': 'float32',\n",
        "    'fwd_seg_size_avg': 'float32',\n",
        "    'bwd_seg_size_avg': 'float32',\n",
        "    'fwd_byts_b_avg': 'uint8',\n",
        "    'fwd_pkts_b_avg': 'uint8',\n",
        "    'fwd_blk_rate_avg': 'uint8',\n",
        "    'bwd_byts_b_avg': 'uint8',\n",
        "    'bwd_pkts_b_avg': 'uint8',\n",
        "    'bwd_blk_rate_avg': 'uint8',\n",
        "    'subflow_fwd_pkts': 'uint32',\n",
        "    'subflow_fwd_byts': 'uint32',\n",
        "    'subflow_bwd_pkts': 'uint32',\n",
        "    'subflow_bwd_byts': 'uint32',\n",
        "    'init_fwd_win_byts': 'int32',\n",
        "    'init_bwd_win_byts': 'int32',\n",
        "    'fwd_act_data_pkts': 'uint32',\n",
        "    'fwd_seg_size_min': 'uint8',\n",
        "    'active_mean': 'float32',\n",
        "    'active_std': 'float32',\n",
        "    'active_max': 'uint32',\n",
        "    'active_min': 'uint32',\n",
        "    'idle_mean': 'float32',\n",
        "    'idle_std': 'float32',\n",
        "    'idle_max': 'uint64',\n",
        "    'idle_min': 'uint64',\n",
        "    'label': 'category'\n",
        "}\n",
        "\n",
        "def replace_infinity_with_mean(df): #inf\n",
        "    inf_columns = [c for c in df.columns if df[df[c] == np.inf][c].count() > 0]\n",
        "    for col in inf_columns:\n",
        "        df[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "        mean = df[col].mean()\n",
        "        df[col].fillna(mean, inplace=True)\n",
        "    return df  \n",
        "\n",
        "\n",
        "def replace_negative_values_with_mean(df):\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.values\n",
        "    \n",
        "    columns = [c for c in numeric_cols if df[df[c] < 0][c].count() > 0]\n",
        "    for col in columns:\n",
        "        mask = df[col] < 0\n",
        "        df.loc[mask, col] = np.nan\n",
        "        mean = df[col].mean()\n",
        "        df[col].fillna(mean, inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_dataset(files, dtypes, cols=None):\n",
        "    df = pd.concat((pd.read_csv(f, dtype=dtypes, usecols=cols) for f in files))\n",
        "    \n",
        "    df = replace_infinity_with_mean(df)\n",
        "    df = replace_negative_values_with_mean(df)\n",
        "        \n",
        "    #df['label_cat'] = df.label.astype('category').cat.codes\n",
        "    #df['label_is_attack'] = (df.label != 'Benign').astype('int')\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPMVJ9Xxufhd",
        "outputId": "63394a3d-24fe-4160-c1cd-4742296d7e53"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data2018\n",
            "bot_02-03-2018.csv\n",
            "bruteforce-ftp-ssh_14-02-2018.csv\n",
            "bruteforce-web-xss_sql-injection_22-02-2018.csv\n",
            "bruteforce-web-xss_sql-injection_23-02-2018.csv\n",
            "ddos-loic-http-loic-udp_20-02-2018.csv\n",
            "ddos-loic-udp_hoic_21-02-2018.csv\n",
            "dos-goldeneye-slowloris_15-02-2018.csv\n",
            "dos-slowhttp-hulk_16-02-2018.csv\n",
            "infiltration_01-03-2018.csv\n",
            "infiltration_28-02-2018.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0qdjkFuxi6A"
      },
      "source": [
        "### 1. Preprocessing Dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLeLdcDuon_U"
      },
      "source": [
        "# csv_files = glob.glob(os.path.join(dataset_base_path, '*.csv'))\n",
        "# df = load_dataset(csv_files, types)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR0RtJD6oqG4"
      },
      "source": [
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# labelencoder = LabelEncoder()\n",
        "# df.iloc[:, -1] = labelencoder.fit_transform(df.iloc[:, -1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYA0G9JOeMkl",
        "outputId": "05e86227-4762-472f-ad09-6f5d405b7dd0"
      },
      "source": [
        " %cd /content/drive/My Drive/Colab Notebooks/Newi/2021Project3/Dataupdate/all_data2018"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3/Dataupdate/all_data2018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE2ec2VOUrLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45dfeea-348a-496f-ad20-b5605df800da"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data2018.csv  catboost_info  df_new.csv  X_new.csv\ty_new.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-7HAbYdxtEe"
      },
      "source": [
        "#df.to_csv(\"all_data2018.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sciD2FKcYPlf"
      },
      "source": [
        "df = pd.read_csv(\"all_data2018.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4M1JdBqAgeM",
        "outputId": "fc7fd78e-68b0-4db7-f83f-d3186998982d"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['dst_port', 'protocol', 'timestamp', 'flow_duration', 'tot_fwd_pkts',\n",
              "       'tot_bwd_pkts', 'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max',\n",
              "       'fwd_pkt_len_min', 'fwd_pkt_len_mean', 'fwd_pkt_len_std',\n",
              "       'bwd_pkt_len_max', 'bwd_pkt_len_min', 'bwd_pkt_len_mean',\n",
              "       'bwd_pkt_len_std', 'flow_byts_s', 'flow_pkts_s', 'flow_iat_mean',\n",
              "       'flow_iat_std', 'flow_iat_max', 'flow_iat_min', 'fwd_iat_tot',\n",
              "       'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max', 'fwd_iat_min',\n",
              "       'bwd_iat_tot', 'bwd_iat_mean', 'bwd_iat_std', 'bwd_iat_max',\n",
              "       'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags',\n",
              "       'bwd_urg_flags', 'fwd_header_len', 'bwd_header_len', 'fwd_pkts_s',\n",
              "       'bwd_pkts_s', 'pkt_len_min', 'pkt_len_max', 'pkt_len_mean',\n",
              "       'pkt_len_std', 'pkt_len_var', 'fin_flag_cnt', 'syn_flag_cnt',\n",
              "       'rst_flag_cnt', 'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt',\n",
              "       'cwe_flag_count', 'ece_flag_cnt', 'down_up_ratio', 'pkt_size_avg',\n",
              "       'fwd_seg_size_avg', 'bwd_seg_size_avg', 'fwd_byts_b_avg',\n",
              "       'fwd_pkts_b_avg', 'fwd_blk_rate_avg', 'bwd_byts_b_avg',\n",
              "       'bwd_pkts_b_avg', 'bwd_blk_rate_avg', 'subflow_fwd_pkts',\n",
              "       'subflow_fwd_byts', 'subflow_bwd_pkts', 'subflow_bwd_byts',\n",
              "       'init_fwd_win_byts', 'init_bwd_win_byts', 'fwd_act_data_pkts',\n",
              "       'fwd_seg_size_min', 'active_mean', 'active_std', 'active_max',\n",
              "       'active_min', 'idle_mean', 'idle_std', 'idle_max', 'idle_min', 'label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "487wzyFg83rX"
      },
      "source": [
        "# Min-max normalization\n",
        "# numeric_features = df.dtypes[df.dtypes != 'object'].index\n",
        "# df[numeric_features] = df[numeric_features].apply(\n",
        "#    lambda x: (x - x.min()) / (x.max()-x.min()))\n",
        "# Fill empty values by 0\n",
        "# df = df.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9wiIVF_t9cy"
      },
      "source": [
        "# X = df.drop(columns=['label', 'label_cat', 'label_is_attack'])\n",
        "# y = df[['label_is_attack', 'label_cat', 'label']]\n",
        "# y = df[['label']].values.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhhbza-mst3k"
      },
      "source": [
        "# X = df.drop(['label'],axis=1).values \n",
        "X = df.drop(columns=['label'])\n",
        "y = df.iloc[:, -1].values.reshape(-1,1)\n",
        "y = np.ravel(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUaREArcxm0q"
      },
      "source": [
        "#stats = X.describe()\n",
        "#std = stats.loc['std']\n",
        "#features_no_variance = std[std == 0.0].index\n",
        "#pd.Series(features_no_variance).sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4IZq3tlj4Jy"
      },
      "source": [
        "features_no_variance = ['bwd_blk_rate_avg','bwd_byts_b_avg','bwd_pkts_b_avg','bwd_psh_flags','bwd_urg_flags','fwd_blk_rate_avg','fwd_pkts_b_avg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfnht-exkYTR"
      },
      "source": [
        "#X = X.drop(columns=features_no_variance) # No Variance\n",
        "X = X.drop(columns=['timestamp', 'dst_port']) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzlwMSMahu9Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zmqi8MGB1uEZ"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2, random_state = 0, stratify = y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsLZbDplVVCt"
      },
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_test,y_test, train_size = 0.8, test_size = 0.2, random_state = 0, stratify = y_test)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_test1,y_test1, train_size = 0.8, test_size = 0.2, random_state = 0, stratify = y_test1)\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_test2,y_test2, train_size = 0.8, test_size = 0.2, random_state = 0, stratify = y_test2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_t6-cL4qyrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17526b1b-0a9f-4c41-c963-c85d60acbbae"
      },
      "source": [
        "X_train3.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(103891, 77)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtzoMBEyA1HL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0118a77-f223-44bc-ede9-70307a956f81"
      },
      "source": [
        "len(X.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2hpQOr_hOPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61147c6e-646c-4c17-9b02-55e25f0c79df"
      },
      "source": [
        "print (type(X_train3),type(X_test3),type(y_train3),type(y_test3),type(y),type(X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkvFYtp2EhtJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "9860d793-829b-4fed-a070-bb9ba55772c3"
      },
      "source": [
        "X_train3.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>protocol</th>\n",
              "      <th>flow_duration</th>\n",
              "      <th>tot_fwd_pkts</th>\n",
              "      <th>tot_bwd_pkts</th>\n",
              "      <th>totlen_fwd_pkts</th>\n",
              "      <th>totlen_bwd_pkts</th>\n",
              "      <th>fwd_pkt_len_max</th>\n",
              "      <th>fwd_pkt_len_min</th>\n",
              "      <th>fwd_pkt_len_mean</th>\n",
              "      <th>fwd_pkt_len_std</th>\n",
              "      <th>bwd_pkt_len_max</th>\n",
              "      <th>bwd_pkt_len_min</th>\n",
              "      <th>bwd_pkt_len_mean</th>\n",
              "      <th>bwd_pkt_len_std</th>\n",
              "      <th>flow_byts_s</th>\n",
              "      <th>flow_pkts_s</th>\n",
              "      <th>flow_iat_mean</th>\n",
              "      <th>flow_iat_std</th>\n",
              "      <th>flow_iat_max</th>\n",
              "      <th>flow_iat_min</th>\n",
              "      <th>fwd_iat_tot</th>\n",
              "      <th>fwd_iat_mean</th>\n",
              "      <th>fwd_iat_std</th>\n",
              "      <th>fwd_iat_max</th>\n",
              "      <th>fwd_iat_min</th>\n",
              "      <th>bwd_iat_tot</th>\n",
              "      <th>bwd_iat_mean</th>\n",
              "      <th>bwd_iat_std</th>\n",
              "      <th>bwd_iat_max</th>\n",
              "      <th>bwd_iat_min</th>\n",
              "      <th>fwd_psh_flags</th>\n",
              "      <th>bwd_psh_flags</th>\n",
              "      <th>fwd_urg_flags</th>\n",
              "      <th>bwd_urg_flags</th>\n",
              "      <th>fwd_header_len</th>\n",
              "      <th>bwd_header_len</th>\n",
              "      <th>fwd_pkts_s</th>\n",
              "      <th>bwd_pkts_s</th>\n",
              "      <th>pkt_len_min</th>\n",
              "      <th>pkt_len_max</th>\n",
              "      <th>pkt_len_mean</th>\n",
              "      <th>pkt_len_std</th>\n",
              "      <th>pkt_len_var</th>\n",
              "      <th>fin_flag_cnt</th>\n",
              "      <th>syn_flag_cnt</th>\n",
              "      <th>rst_flag_cnt</th>\n",
              "      <th>psh_flag_cnt</th>\n",
              "      <th>ack_flag_cnt</th>\n",
              "      <th>urg_flag_cnt</th>\n",
              "      <th>cwe_flag_count</th>\n",
              "      <th>ece_flag_cnt</th>\n",
              "      <th>down_up_ratio</th>\n",
              "      <th>pkt_size_avg</th>\n",
              "      <th>fwd_seg_size_avg</th>\n",
              "      <th>bwd_seg_size_avg</th>\n",
              "      <th>fwd_byts_b_avg</th>\n",
              "      <th>fwd_pkts_b_avg</th>\n",
              "      <th>fwd_blk_rate_avg</th>\n",
              "      <th>bwd_byts_b_avg</th>\n",
              "      <th>bwd_pkts_b_avg</th>\n",
              "      <th>bwd_blk_rate_avg</th>\n",
              "      <th>subflow_fwd_pkts</th>\n",
              "      <th>subflow_fwd_byts</th>\n",
              "      <th>subflow_bwd_pkts</th>\n",
              "      <th>subflow_bwd_byts</th>\n",
              "      <th>init_fwd_win_byts</th>\n",
              "      <th>init_bwd_win_byts</th>\n",
              "      <th>fwd_act_data_pkts</th>\n",
              "      <th>fwd_seg_size_min</th>\n",
              "      <th>active_mean</th>\n",
              "      <th>active_std</th>\n",
              "      <th>active_max</th>\n",
              "      <th>active_min</th>\n",
              "      <th>idle_mean</th>\n",
              "      <th>idle_std</th>\n",
              "      <th>idle_max</th>\n",
              "      <th>idle_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10486085</th>\n",
              "      <td>6</td>\n",
              "      <td>2281890.0</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>1052</td>\n",
              "      <td>1429</td>\n",
              "      <td>581</td>\n",
              "      <td>0</td>\n",
              "      <td>131.50000</td>\n",
              "      <td>196.37210</td>\n",
              "      <td>1149</td>\n",
              "      <td>0</td>\n",
              "      <td>204.14285</td>\n",
              "      <td>420.49908</td>\n",
              "      <td>1087.256616</td>\n",
              "      <td>6.573498</td>\n",
              "      <td>162992.14</td>\n",
              "      <td>249620.94</td>\n",
              "      <td>953401.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2281890.0</td>\n",
              "      <td>325984.28</td>\n",
              "      <td>403106.62</td>\n",
              "      <td>1222281.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2062411</td>\n",
              "      <td>343735.16</td>\n",
              "      <td>299566.90</td>\n",
              "      <td>953401</td>\n",
              "      <td>199828</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>152</td>\n",
              "      <td>3.505866</td>\n",
              "      <td>3.067632</td>\n",
              "      <td>0</td>\n",
              "      <td>1149</td>\n",
              "      <td>155.062500</td>\n",
              "      <td>302.897000</td>\n",
              "      <td>91746.5900</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>165.40000</td>\n",
              "      <td>131.50000</td>\n",
              "      <td>204.14285</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1052</td>\n",
              "      <td>7</td>\n",
              "      <td>1429</td>\n",
              "      <td>8192.000000</td>\n",
              "      <td>62948.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5222695</th>\n",
              "      <td>6</td>\n",
              "      <td>9441907.0</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>1148</td>\n",
              "      <td>1581</td>\n",
              "      <td>677</td>\n",
              "      <td>0</td>\n",
              "      <td>104.36364</td>\n",
              "      <td>202.29448</td>\n",
              "      <td>1173</td>\n",
              "      <td>0</td>\n",
              "      <td>225.85715</td>\n",
              "      <td>430.09860</td>\n",
              "      <td>289.030595</td>\n",
              "      <td>1.906395</td>\n",
              "      <td>555406.30</td>\n",
              "      <td>941207.30</td>\n",
              "      <td>3513961.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>9441907.0</td>\n",
              "      <td>944190.70</td>\n",
              "      <td>1139554.20</td>\n",
              "      <td>3513961.0</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>6669606</td>\n",
              "      <td>1111601.00</td>\n",
              "      <td>1482133.40</td>\n",
              "      <td>4081124</td>\n",
              "      <td>261029</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>232</td>\n",
              "      <td>152</td>\n",
              "      <td>1.165019</td>\n",
              "      <td>0.741376</td>\n",
              "      <td>0</td>\n",
              "      <td>1173</td>\n",
              "      <td>143.631580</td>\n",
              "      <td>298.520260</td>\n",
              "      <td>89114.3600</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>151.61111</td>\n",
              "      <td>104.36364</td>\n",
              "      <td>225.85715</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>1148</td>\n",
              "      <td>7</td>\n",
              "      <td>1581</td>\n",
              "      <td>8192.000000</td>\n",
              "      <td>62852.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14777852</th>\n",
              "      <td>6</td>\n",
              "      <td>4040804.0</td>\n",
              "      <td>14</td>\n",
              "      <td>9</td>\n",
              "      <td>1459</td>\n",
              "      <td>1731</td>\n",
              "      <td>741</td>\n",
              "      <td>0</td>\n",
              "      <td>104.21429</td>\n",
              "      <td>195.11345</td>\n",
              "      <td>1179</td>\n",
              "      <td>0</td>\n",
              "      <td>192.33333</td>\n",
              "      <td>386.07446</td>\n",
              "      <td>789.446853</td>\n",
              "      <td>5.691937</td>\n",
              "      <td>183672.90</td>\n",
              "      <td>236023.64</td>\n",
              "      <td>953144.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>4040804.0</td>\n",
              "      <td>310831.06</td>\n",
              "      <td>328454.03</td>\n",
              "      <td>1264125.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>4021691</td>\n",
              "      <td>502711.38</td>\n",
              "      <td>341529.28</td>\n",
              "      <td>953144</td>\n",
              "      <td>107635</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>292</td>\n",
              "      <td>192</td>\n",
              "      <td>3.464657</td>\n",
              "      <td>2.227279</td>\n",
              "      <td>0</td>\n",
              "      <td>1179</td>\n",
              "      <td>132.916670</td>\n",
              "      <td>275.704770</td>\n",
              "      <td>76013.1250</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>138.69565</td>\n",
              "      <td>104.21429</td>\n",
              "      <td>192.33333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1459</td>\n",
              "      <td>9</td>\n",
              "      <td>1731</td>\n",
              "      <td>8192.000000</td>\n",
              "      <td>62541.000000</td>\n",
              "      <td>7</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8482847</th>\n",
              "      <td>17</td>\n",
              "      <td>1691.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>102</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "      <td>42.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>102</td>\n",
              "      <td>102</td>\n",
              "      <td>102.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>85156.712005</td>\n",
              "      <td>1182.732111</td>\n",
              "      <td>1691.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1691.0</td>\n",
              "      <td>1691.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>591.366000</td>\n",
              "      <td>591.366000</td>\n",
              "      <td>42</td>\n",
              "      <td>102</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>34.641018</td>\n",
              "      <td>1200.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>93.00000</td>\n",
              "      <td>42.00000</td>\n",
              "      <td>102.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>1</td>\n",
              "      <td>102</td>\n",
              "      <td>12088.350041</td>\n",
              "      <td>17682.268468</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8075319</th>\n",
              "      <td>17</td>\n",
              "      <td>819.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>97</td>\n",
              "      <td>33</td>\n",
              "      <td>33</td>\n",
              "      <td>33.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>97</td>\n",
              "      <td>97</td>\n",
              "      <td>97.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>158730.158730</td>\n",
              "      <td>2442.002442</td>\n",
              "      <td>819.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>819.0</td>\n",
              "      <td>819.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1221.001200</td>\n",
              "      <td>1221.001200</td>\n",
              "      <td>33</td>\n",
              "      <td>97</td>\n",
              "      <td>54.333332</td>\n",
              "      <td>36.950417</td>\n",
              "      <td>1365.3334</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>81.50000</td>\n",
              "      <td>33.00000</td>\n",
              "      <td>97.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "      <td>97</td>\n",
              "      <td>12088.350041</td>\n",
              "      <td>17682.268468</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          protocol  flow_duration  tot_fwd_pkts  ...  idle_std  idle_max  idle_min\n",
              "10486085         6      2281890.0             8  ...       0.0         0         0\n",
              "5222695          6      9441907.0            11  ...       0.0         0         0\n",
              "14777852         6      4040804.0            14  ...       0.0         0         0\n",
              "8482847         17         1691.0             1  ...       0.0         0         0\n",
              "8075319         17          819.0             1  ...       0.0         0         0\n",
              "\n",
              "[5 rows x 77 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt7sbrcUEilt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfRRfoswEiDJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WweTIWywdxqo"
      },
      "source": [
        "### 2. Pre-Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8hPJpk33iRL"
      },
      "source": [
        "def report_acc (model_score, y_true, y_predict):\n",
        "  print('Accuracy of Model: ' + str(model_score))\n",
        "  precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
        "  print('Precision of Model: ' + (str(precision)))\n",
        "  print('Recall of Model: ' + (str(recall)))\n",
        "  print('F1-score of Model: ' + (str(fscore)))\n",
        "  print(classification_report(y_true,y_predict))\n",
        "\n",
        "  #cm = confusion_matrix(y_true,y_predict)\n",
        "  #f,ax = plt.subplots(figsize=(8,8))\n",
        "  #sns.heatmap(cm,annot=True,linewidth = 0.5,linecolor=\"blue\",fmt=\".0f\",ax=ax)\n",
        "  #plt.xlabel(\"y_pred\")\n",
        "  #plt.ylabel(\"y_true\")\n",
        "  #plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6GzoYqtPB7Y"
      },
      "source": [
        "rf = RandomForestClassifier(random_state = 0)\n",
        "xg = xgb.XGBClassifier(n_estimators = 10)\n",
        "cat = CatBoostClassifier()\n",
        "light = lgb.LGBMClassifier()\n",
        "\n",
        "def build_model (model):\n",
        "    print(model)\n",
        "    model.fit(X_train3, y_train3)\n",
        "\n",
        "    model_score = model.score(X_test3,y_test3)\n",
        "    y_predict = model.predict(X_test3)\n",
        "    y_true = y_test3\n",
        "\n",
        "    report_acc (model_score, y_true, y_predict)\n",
        "\n",
        "    model_train=model.predict(X_train3)\n",
        "    model_test=model.predict(X_test3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ecOsZ6AXFQB"
      },
      "source": [
        "def save_model(model,name_model):\n",
        "    # Save the model to disk \n",
        "    %cd /content/drive/My Drive/Colab Notebooks/Newi/2021Project3/\n",
        "\n",
        "    filename = name_model + '.sav' ; \n",
        "    pickle.dump(model, open('./result_model/' + filename, 'wb'))\n",
        "    print(\"Done save model in drive: \" , filename)\n",
        "    #---------------------------------------------------------------------------\n",
        "    # Load the model from disk\n",
        "    loaded_model = pickle.load(open('./result_model/' + filename, 'rb'))\n",
        "    result = loaded_model.score(X_test3, y_test3)\n",
        "    print(\"Accuracy Model: \" , result)\n",
        "    #---------------------------------------------------------------------------\n",
        "    train = name_model + 'train'\n",
        "    train = loaded_model.predict(X_train3)\n",
        "    test = loaded_model.predict(X_test3)\n",
        "\n",
        "    return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNn8LyPzvzYb"
      },
      "source": [
        "### 3. Apply Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0x9I-L0PQ2n",
        "outputId": "ff09a112-4173-49d8-c778-b3865ce2e23d"
      },
      "source": [
        "build_model(rf)\n",
        "rf_train , rf_test = save_model(rf,'rf_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
            "                       warm_start=False)\n",
            "Accuracy of Model: 0.9822507989065568\n",
            "Precision of Model: 0.9778573026714411\n",
            "Recall of Model: 0.9822507989065568\n",
            "F1-score of Model: 0.9788614563678204\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     21576\n",
            "           1       0.98      1.00      0.99       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           4       1.00      1.00      1.00      1098\n",
            "           5       0.75      1.00      0.86         3\n",
            "           6       1.00      0.99      0.99       922\n",
            "           7       0.97      0.98      0.98        66\n",
            "           8       1.00      1.00      1.00       739\n",
            "           9       0.78      0.48      0.59       224\n",
            "          10       1.00      1.00      1.00        18\n",
            "          11       0.70      0.90      0.79       309\n",
            "          12       0.35      0.10      0.15       259\n",
            "          14       1.00      1.00      1.00       300\n",
            "\n",
            "    accuracy                           0.98     25973\n",
            "   macro avg       0.81      0.80      0.80     25973\n",
            "weighted avg       0.98      0.98      0.98     25973\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  rf_model.sav\n",
            "Accuracy Model:  0.9822507989065568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPPqHuXTUhXw",
        "outputId": "399531cd-a450-42cf-bf69-a81e9e7aeb5a"
      },
      "source": [
        "build_model(xg)\n",
        "xg_train , xg_test = save_model(xg,'xg_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
            "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
            "              nthread=None, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n",
            "Accuracy of Model: 0.9727024217456589\n",
            "Precision of Model: 0.963110078914485\n",
            "Recall of Model: 0.9727024217456589\n",
            "F1-score of Model: 0.9660995799292463\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99     21576\n",
            "           1       1.00      0.48      0.65       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           4       1.00      1.00      1.00      1098\n",
            "           5       0.75      1.00      0.86         3\n",
            "           6       0.99      0.98      0.99       922\n",
            "           7       0.95      0.94      0.95        66\n",
            "           8       0.99      0.99      0.99       739\n",
            "           9       0.66      0.57      0.61       224\n",
            "          10       1.00      0.67      0.80        18\n",
            "          11       0.72      0.79      0.75       309\n",
            "          12       0.00      0.00      0.00       259\n",
            "          14       0.99      0.99      0.99       300\n",
            "\n",
            "    accuracy                           0.97     25973\n",
            "   macro avg       0.77      0.72      0.74     25973\n",
            "weighted avg       0.96      0.97      0.97     25973\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  xg_model.sav\n",
            "Accuracy Model:  0.9822507989065568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCfjh70XUhSz",
        "outputId": "bcefad22-2958-49dd-f49e-a1485311a1f8"
      },
      "source": [
        "build_model(cat)\n",
        "cat_train , cat_test = save_model(cat,'cat_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<catboost.core.CatBoostClassifier object at 0x7f1c04097dd0>\n",
            "Learning rate set to 0.100269\n",
            "0:\tlearn: 1.5693948\ttotal: 130ms\tremaining: 2m 9s\n",
            "1:\tlearn: 1.2611272\ttotal: 245ms\tremaining: 2m 2s\n",
            "2:\tlearn: 1.0635590\ttotal: 354ms\tremaining: 1m 57s\n",
            "3:\tlearn: 0.9165500\ttotal: 464ms\tremaining: 1m 55s\n",
            "4:\tlearn: 0.7930125\ttotal: 575ms\tremaining: 1m 54s\n",
            "5:\tlearn: 0.6947223\ttotal: 692ms\tremaining: 1m 54s\n",
            "6:\tlearn: 0.6170233\ttotal: 812ms\tremaining: 1m 55s\n",
            "7:\tlearn: 0.5525697\ttotal: 928ms\tremaining: 1m 55s\n",
            "8:\tlearn: 0.4979633\ttotal: 1.04s\tremaining: 1m 54s\n",
            "9:\tlearn: 0.4516341\ttotal: 1.15s\tremaining: 1m 53s\n",
            "10:\tlearn: 0.4097827\ttotal: 1.26s\tremaining: 1m 53s\n",
            "11:\tlearn: 0.3751337\ttotal: 1.37s\tremaining: 1m 52s\n",
            "12:\tlearn: 0.3439940\ttotal: 1.48s\tremaining: 1m 52s\n",
            "13:\tlearn: 0.3180090\ttotal: 1.59s\tremaining: 1m 52s\n",
            "14:\tlearn: 0.2933681\ttotal: 1.7s\tremaining: 1m 51s\n",
            "15:\tlearn: 0.2731135\ttotal: 1.81s\tremaining: 1m 51s\n",
            "16:\tlearn: 0.2533203\ttotal: 1.92s\tremaining: 1m 50s\n",
            "17:\tlearn: 0.2365758\ttotal: 2.03s\tremaining: 1m 50s\n",
            "18:\tlearn: 0.2223525\ttotal: 2.14s\tremaining: 1m 50s\n",
            "19:\tlearn: 0.2091342\ttotal: 2.25s\tremaining: 1m 50s\n",
            "20:\tlearn: 0.1975005\ttotal: 2.36s\tremaining: 1m 50s\n",
            "21:\tlearn: 0.1873058\ttotal: 2.47s\tremaining: 1m 49s\n",
            "22:\tlearn: 0.1770506\ttotal: 2.58s\tremaining: 1m 49s\n",
            "23:\tlearn: 0.1685618\ttotal: 2.69s\tremaining: 1m 49s\n",
            "24:\tlearn: 0.1614607\ttotal: 2.8s\tremaining: 1m 49s\n",
            "25:\tlearn: 0.1549871\ttotal: 2.93s\tremaining: 1m 49s\n",
            "26:\tlearn: 0.1486726\ttotal: 3.04s\tremaining: 1m 49s\n",
            "27:\tlearn: 0.1419576\ttotal: 3.15s\tremaining: 1m 49s\n",
            "28:\tlearn: 0.1360255\ttotal: 3.26s\tremaining: 1m 49s\n",
            "29:\tlearn: 0.1313139\ttotal: 3.39s\tremaining: 1m 49s\n",
            "30:\tlearn: 0.1269563\ttotal: 3.51s\tremaining: 1m 49s\n",
            "31:\tlearn: 0.1230126\ttotal: 3.64s\tremaining: 1m 50s\n",
            "32:\tlearn: 0.1198870\ttotal: 3.75s\tremaining: 1m 50s\n",
            "33:\tlearn: 0.1167235\ttotal: 3.88s\tremaining: 1m 50s\n",
            "34:\tlearn: 0.1139142\ttotal: 4.01s\tremaining: 1m 50s\n",
            "35:\tlearn: 0.1114318\ttotal: 4.13s\tremaining: 1m 50s\n",
            "36:\tlearn: 0.1088264\ttotal: 4.25s\tremaining: 1m 50s\n",
            "37:\tlearn: 0.1060023\ttotal: 4.36s\tremaining: 1m 50s\n",
            "38:\tlearn: 0.1034412\ttotal: 4.48s\tremaining: 1m 50s\n",
            "39:\tlearn: 0.1019235\ttotal: 4.59s\tremaining: 1m 50s\n",
            "40:\tlearn: 0.1003644\ttotal: 4.7s\tremaining: 1m 49s\n",
            "41:\tlearn: 0.0987936\ttotal: 4.81s\tremaining: 1m 49s\n",
            "42:\tlearn: 0.0972644\ttotal: 4.92s\tremaining: 1m 49s\n",
            "43:\tlearn: 0.0953976\ttotal: 5.03s\tremaining: 1m 49s\n",
            "44:\tlearn: 0.0940168\ttotal: 5.14s\tremaining: 1m 49s\n",
            "45:\tlearn: 0.0925251\ttotal: 5.26s\tremaining: 1m 49s\n",
            "46:\tlearn: 0.0914805\ttotal: 5.37s\tremaining: 1m 48s\n",
            "47:\tlearn: 0.0903986\ttotal: 5.49s\tremaining: 1m 48s\n",
            "48:\tlearn: 0.0894463\ttotal: 5.6s\tremaining: 1m 48s\n",
            "49:\tlearn: 0.0884555\ttotal: 5.72s\tremaining: 1m 48s\n",
            "50:\tlearn: 0.0875575\ttotal: 5.83s\tremaining: 1m 48s\n",
            "51:\tlearn: 0.0863762\ttotal: 5.95s\tremaining: 1m 48s\n",
            "52:\tlearn: 0.0854340\ttotal: 6.06s\tremaining: 1m 48s\n",
            "53:\tlearn: 0.0847399\ttotal: 6.17s\tremaining: 1m 48s\n",
            "54:\tlearn: 0.0839033\ttotal: 6.28s\tremaining: 1m 47s\n",
            "55:\tlearn: 0.0832636\ttotal: 6.38s\tremaining: 1m 47s\n",
            "56:\tlearn: 0.0826277\ttotal: 6.5s\tremaining: 1m 47s\n",
            "57:\tlearn: 0.0818338\ttotal: 6.61s\tremaining: 1m 47s\n",
            "58:\tlearn: 0.0812533\ttotal: 6.73s\tremaining: 1m 47s\n",
            "59:\tlearn: 0.0806993\ttotal: 6.84s\tremaining: 1m 47s\n",
            "60:\tlearn: 0.0799619\ttotal: 6.95s\tremaining: 1m 47s\n",
            "61:\tlearn: 0.0794373\ttotal: 7.07s\tremaining: 1m 46s\n",
            "62:\tlearn: 0.0788244\ttotal: 7.18s\tremaining: 1m 46s\n",
            "63:\tlearn: 0.0785006\ttotal: 7.29s\tremaining: 1m 46s\n",
            "64:\tlearn: 0.0782283\ttotal: 7.41s\tremaining: 1m 46s\n",
            "65:\tlearn: 0.0776909\ttotal: 7.51s\tremaining: 1m 46s\n",
            "66:\tlearn: 0.0773367\ttotal: 7.63s\tremaining: 1m 46s\n",
            "67:\tlearn: 0.0767838\ttotal: 7.74s\tremaining: 1m 46s\n",
            "68:\tlearn: 0.0763583\ttotal: 7.85s\tremaining: 1m 45s\n",
            "69:\tlearn: 0.0758787\ttotal: 7.96s\tremaining: 1m 45s\n",
            "70:\tlearn: 0.0756466\ttotal: 8.07s\tremaining: 1m 45s\n",
            "71:\tlearn: 0.0754450\ttotal: 8.18s\tremaining: 1m 45s\n",
            "72:\tlearn: 0.0750737\ttotal: 8.29s\tremaining: 1m 45s\n",
            "73:\tlearn: 0.0748601\ttotal: 8.39s\tremaining: 1m 45s\n",
            "74:\tlearn: 0.0746337\ttotal: 8.5s\tremaining: 1m 44s\n",
            "75:\tlearn: 0.0743869\ttotal: 8.62s\tremaining: 1m 44s\n",
            "76:\tlearn: 0.0739774\ttotal: 8.73s\tremaining: 1m 44s\n",
            "77:\tlearn: 0.0738179\ttotal: 8.84s\tremaining: 1m 44s\n",
            "78:\tlearn: 0.0737048\ttotal: 8.95s\tremaining: 1m 44s\n",
            "79:\tlearn: 0.0735103\ttotal: 9.06s\tremaining: 1m 44s\n",
            "80:\tlearn: 0.0731586\ttotal: 9.17s\tremaining: 1m 44s\n",
            "81:\tlearn: 0.0729757\ttotal: 9.28s\tremaining: 1m 43s\n",
            "82:\tlearn: 0.0727132\ttotal: 9.38s\tremaining: 1m 43s\n",
            "83:\tlearn: 0.0724300\ttotal: 9.49s\tremaining: 1m 43s\n",
            "84:\tlearn: 0.0722522\ttotal: 9.6s\tremaining: 1m 43s\n",
            "85:\tlearn: 0.0719397\ttotal: 9.71s\tremaining: 1m 43s\n",
            "86:\tlearn: 0.0717413\ttotal: 9.81s\tremaining: 1m 42s\n",
            "87:\tlearn: 0.0716238\ttotal: 9.92s\tremaining: 1m 42s\n",
            "88:\tlearn: 0.0713996\ttotal: 10s\tremaining: 1m 42s\n",
            "89:\tlearn: 0.0712130\ttotal: 10.1s\tremaining: 1m 42s\n",
            "90:\tlearn: 0.0711562\ttotal: 10.3s\tremaining: 1m 42s\n",
            "91:\tlearn: 0.0710013\ttotal: 10.4s\tremaining: 1m 42s\n",
            "92:\tlearn: 0.0708827\ttotal: 10.5s\tremaining: 1m 42s\n",
            "93:\tlearn: 0.0707196\ttotal: 10.6s\tremaining: 1m 41s\n",
            "94:\tlearn: 0.0706435\ttotal: 10.7s\tremaining: 1m 41s\n",
            "95:\tlearn: 0.0705451\ttotal: 10.8s\tremaining: 1m 41s\n",
            "96:\tlearn: 0.0704288\ttotal: 10.9s\tremaining: 1m 41s\n",
            "97:\tlearn: 0.0703017\ttotal: 11s\tremaining: 1m 41s\n",
            "98:\tlearn: 0.0701940\ttotal: 11.1s\tremaining: 1m 41s\n",
            "99:\tlearn: 0.0700899\ttotal: 11.2s\tremaining: 1m 40s\n",
            "100:\tlearn: 0.0699920\ttotal: 11.3s\tremaining: 1m 40s\n",
            "101:\tlearn: 0.0698693\ttotal: 11.4s\tremaining: 1m 40s\n",
            "102:\tlearn: 0.0697608\ttotal: 11.5s\tremaining: 1m 40s\n",
            "103:\tlearn: 0.0696159\ttotal: 11.6s\tremaining: 1m 40s\n",
            "104:\tlearn: 0.0695332\ttotal: 11.8s\tremaining: 1m 40s\n",
            "105:\tlearn: 0.0693767\ttotal: 11.9s\tremaining: 1m 40s\n",
            "106:\tlearn: 0.0692751\ttotal: 12s\tremaining: 1m 39s\n",
            "107:\tlearn: 0.0691894\ttotal: 12.1s\tremaining: 1m 39s\n",
            "108:\tlearn: 0.0690024\ttotal: 12.2s\tremaining: 1m 39s\n",
            "109:\tlearn: 0.0687993\ttotal: 12.3s\tremaining: 1m 39s\n",
            "110:\tlearn: 0.0686550\ttotal: 12.4s\tremaining: 1m 39s\n",
            "111:\tlearn: 0.0685172\ttotal: 12.5s\tremaining: 1m 39s\n",
            "112:\tlearn: 0.0683958\ttotal: 12.6s\tremaining: 1m 39s\n",
            "113:\tlearn: 0.0682698\ttotal: 12.8s\tremaining: 1m 39s\n",
            "114:\tlearn: 0.0681934\ttotal: 12.9s\tremaining: 1m 38s\n",
            "115:\tlearn: 0.0681415\ttotal: 13s\tremaining: 1m 38s\n",
            "116:\tlearn: 0.0679925\ttotal: 13.1s\tremaining: 1m 38s\n",
            "117:\tlearn: 0.0678766\ttotal: 13.2s\tremaining: 1m 38s\n",
            "118:\tlearn: 0.0678461\ttotal: 13.3s\tremaining: 1m 38s\n",
            "119:\tlearn: 0.0677828\ttotal: 13.4s\tremaining: 1m 38s\n",
            "120:\tlearn: 0.0675968\ttotal: 13.5s\tremaining: 1m 38s\n",
            "121:\tlearn: 0.0675523\ttotal: 13.6s\tremaining: 1m 38s\n",
            "122:\tlearn: 0.0675051\ttotal: 13.7s\tremaining: 1m 37s\n",
            "123:\tlearn: 0.0674660\ttotal: 13.9s\tremaining: 1m 37s\n",
            "124:\tlearn: 0.0673565\ttotal: 14s\tremaining: 1m 37s\n",
            "125:\tlearn: 0.0673241\ttotal: 14.1s\tremaining: 1m 37s\n",
            "126:\tlearn: 0.0672448\ttotal: 14.2s\tremaining: 1m 37s\n",
            "127:\tlearn: 0.0671398\ttotal: 14.3s\tremaining: 1m 37s\n",
            "128:\tlearn: 0.0670858\ttotal: 14.4s\tremaining: 1m 37s\n",
            "129:\tlearn: 0.0670403\ttotal: 14.5s\tremaining: 1m 37s\n",
            "130:\tlearn: 0.0669666\ttotal: 14.6s\tremaining: 1m 36s\n",
            "131:\tlearn: 0.0669444\ttotal: 14.7s\tremaining: 1m 36s\n",
            "132:\tlearn: 0.0668906\ttotal: 14.8s\tremaining: 1m 36s\n",
            "133:\tlearn: 0.0668462\ttotal: 14.9s\tremaining: 1m 36s\n",
            "134:\tlearn: 0.0667266\ttotal: 15s\tremaining: 1m 36s\n",
            "135:\tlearn: 0.0666737\ttotal: 15.2s\tremaining: 1m 36s\n",
            "136:\tlearn: 0.0666243\ttotal: 15.3s\tremaining: 1m 36s\n",
            "137:\tlearn: 0.0665629\ttotal: 15.4s\tremaining: 1m 35s\n",
            "138:\tlearn: 0.0665171\ttotal: 15.5s\tremaining: 1m 35s\n",
            "139:\tlearn: 0.0664866\ttotal: 15.6s\tremaining: 1m 35s\n",
            "140:\tlearn: 0.0664517\ttotal: 15.7s\tremaining: 1m 35s\n",
            "141:\tlearn: 0.0662318\ttotal: 15.8s\tremaining: 1m 35s\n",
            "142:\tlearn: 0.0662062\ttotal: 15.9s\tremaining: 1m 35s\n",
            "143:\tlearn: 0.0661530\ttotal: 16s\tremaining: 1m 35s\n",
            "144:\tlearn: 0.0660978\ttotal: 16.1s\tremaining: 1m 35s\n",
            "145:\tlearn: 0.0660692\ttotal: 16.2s\tremaining: 1m 34s\n",
            "146:\tlearn: 0.0660086\ttotal: 16.3s\tremaining: 1m 34s\n",
            "147:\tlearn: 0.0659399\ttotal: 16.4s\tremaining: 1m 34s\n",
            "148:\tlearn: 0.0658671\ttotal: 16.5s\tremaining: 1m 34s\n",
            "149:\tlearn: 0.0657484\ttotal: 16.6s\tremaining: 1m 34s\n",
            "150:\tlearn: 0.0656735\ttotal: 16.8s\tremaining: 1m 34s\n",
            "151:\tlearn: 0.0656130\ttotal: 16.9s\tremaining: 1m 34s\n",
            "152:\tlearn: 0.0655678\ttotal: 17s\tremaining: 1m 33s\n",
            "153:\tlearn: 0.0655130\ttotal: 17.1s\tremaining: 1m 33s\n",
            "154:\tlearn: 0.0654664\ttotal: 17.2s\tremaining: 1m 33s\n",
            "155:\tlearn: 0.0653418\ttotal: 17.3s\tremaining: 1m 33s\n",
            "156:\tlearn: 0.0652875\ttotal: 17.4s\tremaining: 1m 33s\n",
            "157:\tlearn: 0.0652012\ttotal: 17.5s\tremaining: 1m 33s\n",
            "158:\tlearn: 0.0651439\ttotal: 17.6s\tremaining: 1m 33s\n",
            "159:\tlearn: 0.0651051\ttotal: 17.7s\tremaining: 1m 33s\n",
            "160:\tlearn: 0.0650711\ttotal: 17.8s\tremaining: 1m 32s\n",
            "161:\tlearn: 0.0650089\ttotal: 17.9s\tremaining: 1m 32s\n",
            "162:\tlearn: 0.0649231\ttotal: 18s\tremaining: 1m 32s\n",
            "163:\tlearn: 0.0648039\ttotal: 18.2s\tremaining: 1m 32s\n",
            "164:\tlearn: 0.0647840\ttotal: 18.3s\tremaining: 1m 32s\n",
            "165:\tlearn: 0.0646876\ttotal: 18.4s\tremaining: 1m 32s\n",
            "166:\tlearn: 0.0646416\ttotal: 18.5s\tremaining: 1m 32s\n",
            "167:\tlearn: 0.0646127\ttotal: 18.6s\tremaining: 1m 32s\n",
            "168:\tlearn: 0.0645581\ttotal: 18.7s\tremaining: 1m 31s\n",
            "169:\tlearn: 0.0644925\ttotal: 18.8s\tremaining: 1m 31s\n",
            "170:\tlearn: 0.0644685\ttotal: 18.9s\tremaining: 1m 31s\n",
            "171:\tlearn: 0.0644096\ttotal: 19s\tremaining: 1m 31s\n",
            "172:\tlearn: 0.0643873\ttotal: 19.1s\tremaining: 1m 31s\n",
            "173:\tlearn: 0.0643234\ttotal: 19.2s\tremaining: 1m 31s\n",
            "174:\tlearn: 0.0643121\ttotal: 19.3s\tremaining: 1m 31s\n",
            "175:\tlearn: 0.0642668\ttotal: 19.4s\tremaining: 1m 30s\n",
            "176:\tlearn: 0.0642078\ttotal: 19.5s\tremaining: 1m 30s\n",
            "177:\tlearn: 0.0641955\ttotal: 19.6s\tremaining: 1m 30s\n",
            "178:\tlearn: 0.0641340\ttotal: 19.8s\tremaining: 1m 30s\n",
            "179:\tlearn: 0.0640933\ttotal: 19.9s\tremaining: 1m 30s\n",
            "180:\tlearn: 0.0640677\ttotal: 20s\tremaining: 1m 30s\n",
            "181:\tlearn: 0.0640551\ttotal: 20.1s\tremaining: 1m 30s\n",
            "182:\tlearn: 0.0639964\ttotal: 20.2s\tremaining: 1m 30s\n",
            "183:\tlearn: 0.0639843\ttotal: 20.3s\tremaining: 1m 29s\n",
            "184:\tlearn: 0.0639330\ttotal: 20.4s\tremaining: 1m 29s\n",
            "185:\tlearn: 0.0638490\ttotal: 20.5s\tremaining: 1m 29s\n",
            "186:\tlearn: 0.0638131\ttotal: 20.6s\tremaining: 1m 29s\n",
            "187:\tlearn: 0.0637219\ttotal: 20.7s\tremaining: 1m 29s\n",
            "188:\tlearn: 0.0636909\ttotal: 20.8s\tremaining: 1m 29s\n",
            "189:\tlearn: 0.0636814\ttotal: 20.9s\tremaining: 1m 29s\n",
            "190:\tlearn: 0.0636409\ttotal: 21s\tremaining: 1m 29s\n",
            "191:\tlearn: 0.0636249\ttotal: 21.1s\tremaining: 1m 28s\n",
            "192:\tlearn: 0.0636050\ttotal: 21.2s\tremaining: 1m 28s\n",
            "193:\tlearn: 0.0635554\ttotal: 21.3s\tremaining: 1m 28s\n",
            "194:\tlearn: 0.0635220\ttotal: 21.5s\tremaining: 1m 28s\n",
            "195:\tlearn: 0.0634847\ttotal: 21.6s\tremaining: 1m 28s\n",
            "196:\tlearn: 0.0633558\ttotal: 21.7s\tremaining: 1m 28s\n",
            "197:\tlearn: 0.0633349\ttotal: 21.8s\tremaining: 1m 28s\n",
            "198:\tlearn: 0.0633116\ttotal: 21.9s\tremaining: 1m 28s\n",
            "199:\tlearn: 0.0632827\ttotal: 22s\tremaining: 1m 27s\n",
            "200:\tlearn: 0.0632572\ttotal: 22.1s\tremaining: 1m 27s\n",
            "201:\tlearn: 0.0632412\ttotal: 22.2s\tremaining: 1m 27s\n",
            "202:\tlearn: 0.0631955\ttotal: 22.3s\tremaining: 1m 27s\n",
            "203:\tlearn: 0.0631758\ttotal: 22.4s\tremaining: 1m 27s\n",
            "204:\tlearn: 0.0631510\ttotal: 22.5s\tremaining: 1m 27s\n",
            "205:\tlearn: 0.0631007\ttotal: 22.6s\tremaining: 1m 27s\n",
            "206:\tlearn: 0.0630836\ttotal: 22.7s\tremaining: 1m 27s\n",
            "207:\tlearn: 0.0630372\ttotal: 22.8s\tremaining: 1m 26s\n",
            "208:\tlearn: 0.0629899\ttotal: 22.9s\tremaining: 1m 26s\n",
            "209:\tlearn: 0.0629642\ttotal: 23s\tremaining: 1m 26s\n",
            "210:\tlearn: 0.0629425\ttotal: 23.1s\tremaining: 1m 26s\n",
            "211:\tlearn: 0.0629220\ttotal: 23.2s\tremaining: 1m 26s\n",
            "212:\tlearn: 0.0629078\ttotal: 23.4s\tremaining: 1m 26s\n",
            "213:\tlearn: 0.0628927\ttotal: 23.5s\tremaining: 1m 26s\n",
            "214:\tlearn: 0.0628745\ttotal: 23.6s\tremaining: 1m 26s\n",
            "215:\tlearn: 0.0628545\ttotal: 23.7s\tremaining: 1m 25s\n",
            "216:\tlearn: 0.0628476\ttotal: 23.8s\tremaining: 1m 25s\n",
            "217:\tlearn: 0.0628306\ttotal: 23.9s\tremaining: 1m 25s\n",
            "218:\tlearn: 0.0627789\ttotal: 24s\tremaining: 1m 25s\n",
            "219:\tlearn: 0.0627564\ttotal: 24.1s\tremaining: 1m 25s\n",
            "220:\tlearn: 0.0626651\ttotal: 24.2s\tremaining: 1m 25s\n",
            "221:\tlearn: 0.0625868\ttotal: 24.3s\tremaining: 1m 25s\n",
            "222:\tlearn: 0.0625553\ttotal: 24.4s\tremaining: 1m 25s\n",
            "223:\tlearn: 0.0625410\ttotal: 24.5s\tremaining: 1m 24s\n",
            "224:\tlearn: 0.0625171\ttotal: 24.6s\tremaining: 1m 24s\n",
            "225:\tlearn: 0.0625069\ttotal: 24.7s\tremaining: 1m 24s\n",
            "226:\tlearn: 0.0624940\ttotal: 24.8s\tremaining: 1m 24s\n",
            "227:\tlearn: 0.0624788\ttotal: 24.9s\tremaining: 1m 24s\n",
            "228:\tlearn: 0.0624302\ttotal: 25.1s\tremaining: 1m 24s\n",
            "229:\tlearn: 0.0623879\ttotal: 25.2s\tremaining: 1m 24s\n",
            "230:\tlearn: 0.0623604\ttotal: 25.3s\tremaining: 1m 24s\n",
            "231:\tlearn: 0.0623449\ttotal: 25.4s\tremaining: 1m 24s\n",
            "232:\tlearn: 0.0623043\ttotal: 25.5s\tremaining: 1m 23s\n",
            "233:\tlearn: 0.0622650\ttotal: 25.6s\tremaining: 1m 23s\n",
            "234:\tlearn: 0.0622404\ttotal: 25.7s\tremaining: 1m 23s\n",
            "235:\tlearn: 0.0622162\ttotal: 25.8s\tremaining: 1m 23s\n",
            "236:\tlearn: 0.0621987\ttotal: 25.9s\tremaining: 1m 23s\n",
            "237:\tlearn: 0.0621546\ttotal: 26s\tremaining: 1m 23s\n",
            "238:\tlearn: 0.0621397\ttotal: 26.1s\tremaining: 1m 23s\n",
            "239:\tlearn: 0.0621211\ttotal: 26.2s\tremaining: 1m 23s\n",
            "240:\tlearn: 0.0620760\ttotal: 26.3s\tremaining: 1m 22s\n",
            "241:\tlearn: 0.0619773\ttotal: 26.4s\tremaining: 1m 22s\n",
            "242:\tlearn: 0.0619386\ttotal: 26.5s\tremaining: 1m 22s\n",
            "243:\tlearn: 0.0618837\ttotal: 26.6s\tremaining: 1m 22s\n",
            "244:\tlearn: 0.0618615\ttotal: 26.8s\tremaining: 1m 22s\n",
            "245:\tlearn: 0.0618337\ttotal: 26.9s\tremaining: 1m 22s\n",
            "246:\tlearn: 0.0618131\ttotal: 27s\tremaining: 1m 22s\n",
            "247:\tlearn: 0.0618037\ttotal: 27.1s\tremaining: 1m 22s\n",
            "248:\tlearn: 0.0617752\ttotal: 27.2s\tremaining: 1m 21s\n",
            "249:\tlearn: 0.0617455\ttotal: 27.3s\tremaining: 1m 21s\n",
            "250:\tlearn: 0.0617131\ttotal: 27.4s\tremaining: 1m 21s\n",
            "251:\tlearn: 0.0616830\ttotal: 27.5s\tremaining: 1m 21s\n",
            "252:\tlearn: 0.0616517\ttotal: 27.6s\tremaining: 1m 21s\n",
            "253:\tlearn: 0.0616422\ttotal: 27.7s\tremaining: 1m 21s\n",
            "254:\tlearn: 0.0616269\ttotal: 27.8s\tremaining: 1m 21s\n",
            "255:\tlearn: 0.0616054\ttotal: 27.9s\tremaining: 1m 21s\n",
            "256:\tlearn: 0.0615766\ttotal: 28s\tremaining: 1m 21s\n",
            "257:\tlearn: 0.0615396\ttotal: 28.1s\tremaining: 1m 20s\n",
            "258:\tlearn: 0.0615311\ttotal: 28.2s\tremaining: 1m 20s\n",
            "259:\tlearn: 0.0615039\ttotal: 28.3s\tremaining: 1m 20s\n",
            "260:\tlearn: 0.0614708\ttotal: 28.4s\tremaining: 1m 20s\n",
            "261:\tlearn: 0.0614352\ttotal: 28.5s\tremaining: 1m 20s\n",
            "262:\tlearn: 0.0614186\ttotal: 28.7s\tremaining: 1m 20s\n",
            "263:\tlearn: 0.0612552\ttotal: 28.8s\tremaining: 1m 20s\n",
            "264:\tlearn: 0.0612387\ttotal: 28.9s\tremaining: 1m 20s\n",
            "265:\tlearn: 0.0611881\ttotal: 29s\tremaining: 1m 19s\n",
            "266:\tlearn: 0.0611318\ttotal: 29.1s\tremaining: 1m 19s\n",
            "267:\tlearn: 0.0611104\ttotal: 29.2s\tremaining: 1m 19s\n",
            "268:\tlearn: 0.0610942\ttotal: 29.3s\tremaining: 1m 19s\n",
            "269:\tlearn: 0.0610703\ttotal: 29.4s\tremaining: 1m 19s\n",
            "270:\tlearn: 0.0610649\ttotal: 29.5s\tremaining: 1m 19s\n",
            "271:\tlearn: 0.0610487\ttotal: 29.6s\tremaining: 1m 19s\n",
            "272:\tlearn: 0.0610360\ttotal: 29.7s\tremaining: 1m 19s\n",
            "273:\tlearn: 0.0609935\ttotal: 29.8s\tremaining: 1m 19s\n",
            "274:\tlearn: 0.0609803\ttotal: 29.9s\tremaining: 1m 18s\n",
            "275:\tlearn: 0.0609319\ttotal: 30s\tremaining: 1m 18s\n",
            "276:\tlearn: 0.0609218\ttotal: 30.1s\tremaining: 1m 18s\n",
            "277:\tlearn: 0.0608981\ttotal: 30.2s\tremaining: 1m 18s\n",
            "278:\tlearn: 0.0608788\ttotal: 30.4s\tremaining: 1m 18s\n",
            "279:\tlearn: 0.0608540\ttotal: 30.5s\tremaining: 1m 18s\n",
            "280:\tlearn: 0.0608212\ttotal: 30.6s\tremaining: 1m 18s\n",
            "281:\tlearn: 0.0607845\ttotal: 30.7s\tremaining: 1m 18s\n",
            "282:\tlearn: 0.0607533\ttotal: 30.8s\tremaining: 1m 17s\n",
            "283:\tlearn: 0.0607300\ttotal: 30.9s\tremaining: 1m 17s\n",
            "284:\tlearn: 0.0606425\ttotal: 31s\tremaining: 1m 17s\n",
            "285:\tlearn: 0.0606255\ttotal: 31.1s\tremaining: 1m 17s\n",
            "286:\tlearn: 0.0605866\ttotal: 31.2s\tremaining: 1m 17s\n",
            "287:\tlearn: 0.0605381\ttotal: 31.3s\tremaining: 1m 17s\n",
            "288:\tlearn: 0.0605018\ttotal: 31.4s\tremaining: 1m 17s\n",
            "289:\tlearn: 0.0604761\ttotal: 31.5s\tremaining: 1m 17s\n",
            "290:\tlearn: 0.0604540\ttotal: 31.6s\tremaining: 1m 17s\n",
            "291:\tlearn: 0.0604354\ttotal: 31.7s\tremaining: 1m 16s\n",
            "292:\tlearn: 0.0604028\ttotal: 31.8s\tremaining: 1m 16s\n",
            "293:\tlearn: 0.0603700\ttotal: 31.9s\tremaining: 1m 16s\n",
            "294:\tlearn: 0.0603608\ttotal: 32s\tremaining: 1m 16s\n",
            "295:\tlearn: 0.0603453\ttotal: 32.1s\tremaining: 1m 16s\n",
            "296:\tlearn: 0.0603066\ttotal: 32.2s\tremaining: 1m 16s\n",
            "297:\tlearn: 0.0602974\ttotal: 32.3s\tremaining: 1m 16s\n",
            "298:\tlearn: 0.0602787\ttotal: 32.5s\tremaining: 1m 16s\n",
            "299:\tlearn: 0.0602209\ttotal: 32.6s\tremaining: 1m 15s\n",
            "300:\tlearn: 0.0601995\ttotal: 32.7s\tremaining: 1m 15s\n",
            "301:\tlearn: 0.0601840\ttotal: 32.8s\tremaining: 1m 15s\n",
            "302:\tlearn: 0.0601746\ttotal: 32.9s\tremaining: 1m 15s\n",
            "303:\tlearn: 0.0601302\ttotal: 33s\tremaining: 1m 15s\n",
            "304:\tlearn: 0.0601044\ttotal: 33.1s\tremaining: 1m 15s\n",
            "305:\tlearn: 0.0600982\ttotal: 33.2s\tremaining: 1m 15s\n",
            "306:\tlearn: 0.0599653\ttotal: 33.3s\tremaining: 1m 15s\n",
            "307:\tlearn: 0.0599261\ttotal: 33.4s\tremaining: 1m 15s\n",
            "308:\tlearn: 0.0599184\ttotal: 33.5s\tremaining: 1m 14s\n",
            "309:\tlearn: 0.0599107\ttotal: 33.6s\tremaining: 1m 14s\n",
            "310:\tlearn: 0.0598925\ttotal: 33.7s\tremaining: 1m 14s\n",
            "311:\tlearn: 0.0598588\ttotal: 33.8s\tremaining: 1m 14s\n",
            "312:\tlearn: 0.0598363\ttotal: 33.9s\tremaining: 1m 14s\n",
            "313:\tlearn: 0.0598291\ttotal: 34s\tremaining: 1m 14s\n",
            "314:\tlearn: 0.0598108\ttotal: 34.1s\tremaining: 1m 14s\n",
            "315:\tlearn: 0.0597878\ttotal: 34.2s\tremaining: 1m 14s\n",
            "316:\tlearn: 0.0597663\ttotal: 34.4s\tremaining: 1m 14s\n",
            "317:\tlearn: 0.0597452\ttotal: 34.5s\tremaining: 1m 13s\n",
            "318:\tlearn: 0.0597301\ttotal: 34.6s\tremaining: 1m 13s\n",
            "319:\tlearn: 0.0596959\ttotal: 34.7s\tremaining: 1m 13s\n",
            "320:\tlearn: 0.0596726\ttotal: 34.8s\tremaining: 1m 13s\n",
            "321:\tlearn: 0.0596522\ttotal: 34.9s\tremaining: 1m 13s\n",
            "322:\tlearn: 0.0596240\ttotal: 35s\tremaining: 1m 13s\n",
            "323:\tlearn: 0.0596088\ttotal: 35.1s\tremaining: 1m 13s\n",
            "324:\tlearn: 0.0595906\ttotal: 35.2s\tremaining: 1m 13s\n",
            "325:\tlearn: 0.0595760\ttotal: 35.3s\tremaining: 1m 12s\n",
            "326:\tlearn: 0.0595646\ttotal: 35.4s\tremaining: 1m 12s\n",
            "327:\tlearn: 0.0595442\ttotal: 35.5s\tremaining: 1m 12s\n",
            "328:\tlearn: 0.0595302\ttotal: 35.6s\tremaining: 1m 12s\n",
            "329:\tlearn: 0.0595137\ttotal: 35.7s\tremaining: 1m 12s\n",
            "330:\tlearn: 0.0594907\ttotal: 35.8s\tremaining: 1m 12s\n",
            "331:\tlearn: 0.0594524\ttotal: 35.9s\tremaining: 1m 12s\n",
            "332:\tlearn: 0.0594323\ttotal: 36.1s\tremaining: 1m 12s\n",
            "333:\tlearn: 0.0594231\ttotal: 36.2s\tremaining: 1m 12s\n",
            "334:\tlearn: 0.0594148\ttotal: 36.3s\tremaining: 1m 12s\n",
            "335:\tlearn: 0.0593720\ttotal: 36.4s\tremaining: 1m 11s\n",
            "336:\tlearn: 0.0593580\ttotal: 36.5s\tremaining: 1m 11s\n",
            "337:\tlearn: 0.0593420\ttotal: 36.6s\tremaining: 1m 11s\n",
            "338:\tlearn: 0.0593110\ttotal: 36.7s\tremaining: 1m 11s\n",
            "339:\tlearn: 0.0592948\ttotal: 36.8s\tremaining: 1m 11s\n",
            "340:\tlearn: 0.0592686\ttotal: 37s\tremaining: 1m 11s\n",
            "341:\tlearn: 0.0592409\ttotal: 37.1s\tremaining: 1m 11s\n",
            "342:\tlearn: 0.0592065\ttotal: 37.2s\tremaining: 1m 11s\n",
            "343:\tlearn: 0.0591859\ttotal: 37.3s\tremaining: 1m 11s\n",
            "344:\tlearn: 0.0591462\ttotal: 37.4s\tremaining: 1m 11s\n",
            "345:\tlearn: 0.0591342\ttotal: 37.5s\tremaining: 1m 10s\n",
            "346:\tlearn: 0.0591146\ttotal: 37.7s\tremaining: 1m 10s\n",
            "347:\tlearn: 0.0590913\ttotal: 37.8s\tremaining: 1m 10s\n",
            "348:\tlearn: 0.0590727\ttotal: 37.9s\tremaining: 1m 10s\n",
            "349:\tlearn: 0.0590640\ttotal: 38s\tremaining: 1m 10s\n",
            "350:\tlearn: 0.0590252\ttotal: 38.1s\tremaining: 1m 10s\n",
            "351:\tlearn: 0.0589955\ttotal: 38.2s\tremaining: 1m 10s\n",
            "352:\tlearn: 0.0589850\ttotal: 38.3s\tremaining: 1m 10s\n",
            "353:\tlearn: 0.0588827\ttotal: 38.4s\tremaining: 1m 10s\n",
            "354:\tlearn: 0.0588754\ttotal: 38.5s\tremaining: 1m 10s\n",
            "355:\tlearn: 0.0588713\ttotal: 38.6s\tremaining: 1m 9s\n",
            "356:\tlearn: 0.0588610\ttotal: 38.8s\tremaining: 1m 9s\n",
            "357:\tlearn: 0.0588528\ttotal: 38.9s\tremaining: 1m 9s\n",
            "358:\tlearn: 0.0588419\ttotal: 39s\tremaining: 1m 9s\n",
            "359:\tlearn: 0.0588259\ttotal: 39.1s\tremaining: 1m 9s\n",
            "360:\tlearn: 0.0588171\ttotal: 39.2s\tremaining: 1m 9s\n",
            "361:\tlearn: 0.0588088\ttotal: 39.3s\tremaining: 1m 9s\n",
            "362:\tlearn: 0.0587771\ttotal: 39.4s\tremaining: 1m 9s\n",
            "363:\tlearn: 0.0587467\ttotal: 39.5s\tremaining: 1m 9s\n",
            "364:\tlearn: 0.0587183\ttotal: 39.6s\tremaining: 1m 8s\n",
            "365:\tlearn: 0.0587026\ttotal: 39.7s\tremaining: 1m 8s\n",
            "366:\tlearn: 0.0586781\ttotal: 39.8s\tremaining: 1m 8s\n",
            "367:\tlearn: 0.0586544\ttotal: 39.9s\tremaining: 1m 8s\n",
            "368:\tlearn: 0.0585932\ttotal: 40.1s\tremaining: 1m 8s\n",
            "369:\tlearn: 0.0585671\ttotal: 40.2s\tremaining: 1m 8s\n",
            "370:\tlearn: 0.0585402\ttotal: 40.3s\tremaining: 1m 8s\n",
            "371:\tlearn: 0.0585212\ttotal: 40.4s\tremaining: 1m 8s\n",
            "372:\tlearn: 0.0584991\ttotal: 40.5s\tremaining: 1m 8s\n",
            "373:\tlearn: 0.0584715\ttotal: 40.6s\tremaining: 1m 7s\n",
            "374:\tlearn: 0.0584634\ttotal: 40.7s\tremaining: 1m 7s\n",
            "375:\tlearn: 0.0584460\ttotal: 40.8s\tremaining: 1m 7s\n",
            "376:\tlearn: 0.0584205\ttotal: 40.9s\tremaining: 1m 7s\n",
            "377:\tlearn: 0.0584033\ttotal: 41s\tremaining: 1m 7s\n",
            "378:\tlearn: 0.0583820\ttotal: 41.1s\tremaining: 1m 7s\n",
            "379:\tlearn: 0.0583660\ttotal: 41.2s\tremaining: 1m 7s\n",
            "380:\tlearn: 0.0583590\ttotal: 41.4s\tremaining: 1m 7s\n",
            "381:\tlearn: 0.0583469\ttotal: 41.5s\tremaining: 1m 7s\n",
            "382:\tlearn: 0.0583242\ttotal: 41.6s\tremaining: 1m 6s\n",
            "383:\tlearn: 0.0582928\ttotal: 41.7s\tremaining: 1m 6s\n",
            "384:\tlearn: 0.0582867\ttotal: 41.8s\tremaining: 1m 6s\n",
            "385:\tlearn: 0.0582698\ttotal: 41.9s\tremaining: 1m 6s\n",
            "386:\tlearn: 0.0582609\ttotal: 42s\tremaining: 1m 6s\n",
            "387:\tlearn: 0.0582430\ttotal: 42.1s\tremaining: 1m 6s\n",
            "388:\tlearn: 0.0582282\ttotal: 42.2s\tremaining: 1m 6s\n",
            "389:\tlearn: 0.0582098\ttotal: 42.3s\tremaining: 1m 6s\n",
            "390:\tlearn: 0.0581959\ttotal: 42.5s\tremaining: 1m 6s\n",
            "391:\tlearn: 0.0581729\ttotal: 42.6s\tremaining: 1m 6s\n",
            "392:\tlearn: 0.0581605\ttotal: 42.7s\tremaining: 1m 5s\n",
            "393:\tlearn: 0.0581498\ttotal: 42.8s\tremaining: 1m 5s\n",
            "394:\tlearn: 0.0581457\ttotal: 42.9s\tremaining: 1m 5s\n",
            "395:\tlearn: 0.0581408\ttotal: 43s\tremaining: 1m 5s\n",
            "396:\tlearn: 0.0581348\ttotal: 43.1s\tremaining: 1m 5s\n",
            "397:\tlearn: 0.0581130\ttotal: 43.2s\tremaining: 1m 5s\n",
            "398:\tlearn: 0.0581030\ttotal: 43.3s\tremaining: 1m 5s\n",
            "399:\tlearn: 0.0580965\ttotal: 43.5s\tremaining: 1m 5s\n",
            "400:\tlearn: 0.0580870\ttotal: 43.6s\tremaining: 1m 5s\n",
            "401:\tlearn: 0.0580733\ttotal: 43.7s\tremaining: 1m 4s\n",
            "402:\tlearn: 0.0580660\ttotal: 43.8s\tremaining: 1m 4s\n",
            "403:\tlearn: 0.0580579\ttotal: 43.9s\tremaining: 1m 4s\n",
            "404:\tlearn: 0.0580509\ttotal: 44s\tremaining: 1m 4s\n",
            "405:\tlearn: 0.0580131\ttotal: 44.1s\tremaining: 1m 4s\n",
            "406:\tlearn: 0.0579914\ttotal: 44.2s\tremaining: 1m 4s\n",
            "407:\tlearn: 0.0579620\ttotal: 44.3s\tremaining: 1m 4s\n",
            "408:\tlearn: 0.0579391\ttotal: 44.4s\tremaining: 1m 4s\n",
            "409:\tlearn: 0.0579250\ttotal: 44.5s\tremaining: 1m 4s\n",
            "410:\tlearn: 0.0579163\ttotal: 44.6s\tremaining: 1m 3s\n",
            "411:\tlearn: 0.0579013\ttotal: 44.7s\tremaining: 1m 3s\n",
            "412:\tlearn: 0.0578743\ttotal: 44.9s\tremaining: 1m 3s\n",
            "413:\tlearn: 0.0578400\ttotal: 45s\tremaining: 1m 3s\n",
            "414:\tlearn: 0.0578272\ttotal: 45.1s\tremaining: 1m 3s\n",
            "415:\tlearn: 0.0578022\ttotal: 45.2s\tremaining: 1m 3s\n",
            "416:\tlearn: 0.0577933\ttotal: 45.3s\tremaining: 1m 3s\n",
            "417:\tlearn: 0.0577821\ttotal: 45.4s\tremaining: 1m 3s\n",
            "418:\tlearn: 0.0577775\ttotal: 45.5s\tremaining: 1m 3s\n",
            "419:\tlearn: 0.0577633\ttotal: 45.6s\tremaining: 1m 2s\n",
            "420:\tlearn: 0.0576993\ttotal: 45.7s\tremaining: 1m 2s\n",
            "421:\tlearn: 0.0576931\ttotal: 45.8s\tremaining: 1m 2s\n",
            "422:\tlearn: 0.0576831\ttotal: 45.9s\tremaining: 1m 2s\n",
            "423:\tlearn: 0.0576735\ttotal: 46s\tremaining: 1m 2s\n",
            "424:\tlearn: 0.0576540\ttotal: 46.2s\tremaining: 1m 2s\n",
            "425:\tlearn: 0.0576387\ttotal: 46.3s\tremaining: 1m 2s\n",
            "426:\tlearn: 0.0576146\ttotal: 46.4s\tremaining: 1m 2s\n",
            "427:\tlearn: 0.0576087\ttotal: 46.5s\tremaining: 1m 2s\n",
            "428:\tlearn: 0.0575992\ttotal: 46.6s\tremaining: 1m 1s\n",
            "429:\tlearn: 0.0575865\ttotal: 46.7s\tremaining: 1m 1s\n",
            "430:\tlearn: 0.0575668\ttotal: 46.8s\tremaining: 1m 1s\n",
            "431:\tlearn: 0.0575512\ttotal: 46.9s\tremaining: 1m 1s\n",
            "432:\tlearn: 0.0575446\ttotal: 47s\tremaining: 1m 1s\n",
            "433:\tlearn: 0.0575350\ttotal: 47.1s\tremaining: 1m 1s\n",
            "434:\tlearn: 0.0574799\ttotal: 47.2s\tremaining: 1m 1s\n",
            "435:\tlearn: 0.0574766\ttotal: 47.3s\tremaining: 1m 1s\n",
            "436:\tlearn: 0.0574662\ttotal: 47.4s\tremaining: 1m 1s\n",
            "437:\tlearn: 0.0574522\ttotal: 47.6s\tremaining: 1m 1s\n",
            "438:\tlearn: 0.0574446\ttotal: 47.7s\tremaining: 1m\n",
            "439:\tlearn: 0.0574319\ttotal: 47.8s\tremaining: 1m\n",
            "440:\tlearn: 0.0574245\ttotal: 47.9s\tremaining: 1m\n",
            "441:\tlearn: 0.0574092\ttotal: 48s\tremaining: 1m\n",
            "442:\tlearn: 0.0573938\ttotal: 48.1s\tremaining: 1m\n",
            "443:\tlearn: 0.0573862\ttotal: 48.2s\tremaining: 1m\n",
            "444:\tlearn: 0.0573744\ttotal: 48.3s\tremaining: 1m\n",
            "445:\tlearn: 0.0573636\ttotal: 48.4s\tremaining: 1m\n",
            "446:\tlearn: 0.0573435\ttotal: 48.5s\tremaining: 1m\n",
            "447:\tlearn: 0.0573384\ttotal: 48.6s\tremaining: 59.9s\n",
            "448:\tlearn: 0.0573327\ttotal: 48.7s\tremaining: 59.8s\n",
            "449:\tlearn: 0.0573295\ttotal: 48.9s\tremaining: 59.7s\n",
            "450:\tlearn: 0.0573256\ttotal: 49s\tremaining: 59.6s\n",
            "451:\tlearn: 0.0573017\ttotal: 49.1s\tremaining: 59.5s\n",
            "452:\tlearn: 0.0572937\ttotal: 49.2s\tremaining: 59.4s\n",
            "453:\tlearn: 0.0572864\ttotal: 49.3s\tremaining: 59.3s\n",
            "454:\tlearn: 0.0572747\ttotal: 49.4s\tremaining: 59.2s\n",
            "455:\tlearn: 0.0572677\ttotal: 49.5s\tremaining: 59.1s\n",
            "456:\tlearn: 0.0572587\ttotal: 49.6s\tremaining: 58.9s\n",
            "457:\tlearn: 0.0572502\ttotal: 49.7s\tremaining: 58.8s\n",
            "458:\tlearn: 0.0572400\ttotal: 49.8s\tremaining: 58.7s\n",
            "459:\tlearn: 0.0572229\ttotal: 49.9s\tremaining: 58.6s\n",
            "460:\tlearn: 0.0572074\ttotal: 50s\tremaining: 58.5s\n",
            "461:\tlearn: 0.0571997\ttotal: 50.2s\tremaining: 58.4s\n",
            "462:\tlearn: 0.0571775\ttotal: 50.3s\tremaining: 58.3s\n",
            "463:\tlearn: 0.0571699\ttotal: 50.4s\tremaining: 58.2s\n",
            "464:\tlearn: 0.0571586\ttotal: 50.5s\tremaining: 58.1s\n",
            "465:\tlearn: 0.0571464\ttotal: 50.6s\tremaining: 58s\n",
            "466:\tlearn: 0.0571372\ttotal: 50.7s\tremaining: 57.9s\n",
            "467:\tlearn: 0.0571221\ttotal: 50.8s\tremaining: 57.8s\n",
            "468:\tlearn: 0.0571189\ttotal: 50.9s\tremaining: 57.7s\n",
            "469:\tlearn: 0.0571071\ttotal: 51s\tremaining: 57.6s\n",
            "470:\tlearn: 0.0571013\ttotal: 51.1s\tremaining: 57.4s\n",
            "471:\tlearn: 0.0570844\ttotal: 51.3s\tremaining: 57.3s\n",
            "472:\tlearn: 0.0570775\ttotal: 51.4s\tremaining: 57.2s\n",
            "473:\tlearn: 0.0570715\ttotal: 51.5s\tremaining: 57.1s\n",
            "474:\tlearn: 0.0570652\ttotal: 51.6s\tremaining: 57s\n",
            "475:\tlearn: 0.0570463\ttotal: 51.7s\tremaining: 56.9s\n",
            "476:\tlearn: 0.0570383\ttotal: 51.8s\tremaining: 56.8s\n",
            "477:\tlearn: 0.0570311\ttotal: 51.9s\tremaining: 56.7s\n",
            "478:\tlearn: 0.0570263\ttotal: 52s\tremaining: 56.6s\n",
            "479:\tlearn: 0.0570185\ttotal: 52.1s\tremaining: 56.5s\n",
            "480:\tlearn: 0.0570144\ttotal: 52.2s\tremaining: 56.4s\n",
            "481:\tlearn: 0.0569995\ttotal: 52.3s\tremaining: 56.3s\n",
            "482:\tlearn: 0.0569758\ttotal: 52.5s\tremaining: 56.1s\n",
            "483:\tlearn: 0.0569645\ttotal: 52.6s\tremaining: 56s\n",
            "484:\tlearn: 0.0569457\ttotal: 52.7s\tremaining: 55.9s\n",
            "485:\tlearn: 0.0569365\ttotal: 52.8s\tremaining: 55.8s\n",
            "486:\tlearn: 0.0569176\ttotal: 52.9s\tremaining: 55.7s\n",
            "487:\tlearn: 0.0568648\ttotal: 53s\tremaining: 55.6s\n",
            "488:\tlearn: 0.0568567\ttotal: 53.1s\tremaining: 55.5s\n",
            "489:\tlearn: 0.0568216\ttotal: 53.2s\tremaining: 55.4s\n",
            "490:\tlearn: 0.0568154\ttotal: 53.3s\tremaining: 55.3s\n",
            "491:\tlearn: 0.0567640\ttotal: 53.4s\tremaining: 55.2s\n",
            "492:\tlearn: 0.0567518\ttotal: 53.5s\tremaining: 55.1s\n",
            "493:\tlearn: 0.0567441\ttotal: 53.7s\tremaining: 55s\n",
            "494:\tlearn: 0.0567306\ttotal: 53.8s\tremaining: 54.9s\n",
            "495:\tlearn: 0.0567166\ttotal: 53.9s\tremaining: 54.8s\n",
            "496:\tlearn: 0.0567131\ttotal: 54s\tremaining: 54.7s\n",
            "497:\tlearn: 0.0567037\ttotal: 54.1s\tremaining: 54.6s\n",
            "498:\tlearn: 0.0566950\ttotal: 54.2s\tremaining: 54.4s\n",
            "499:\tlearn: 0.0566909\ttotal: 54.3s\tremaining: 54.3s\n",
            "500:\tlearn: 0.0566712\ttotal: 54.4s\tremaining: 54.2s\n",
            "501:\tlearn: 0.0566273\ttotal: 54.5s\tremaining: 54.1s\n",
            "502:\tlearn: 0.0566209\ttotal: 54.7s\tremaining: 54s\n",
            "503:\tlearn: 0.0566124\ttotal: 54.8s\tremaining: 53.9s\n",
            "504:\tlearn: 0.0566072\ttotal: 54.9s\tremaining: 53.8s\n",
            "505:\tlearn: 0.0566005\ttotal: 55s\tremaining: 53.7s\n",
            "506:\tlearn: 0.0565288\ttotal: 55.1s\tremaining: 53.6s\n",
            "507:\tlearn: 0.0564925\ttotal: 55.2s\tremaining: 53.5s\n",
            "508:\tlearn: 0.0564374\ttotal: 55.3s\tremaining: 53.4s\n",
            "509:\tlearn: 0.0564164\ttotal: 55.4s\tremaining: 53.2s\n",
            "510:\tlearn: 0.0564127\ttotal: 55.5s\tremaining: 53.2s\n",
            "511:\tlearn: 0.0563492\ttotal: 55.7s\tremaining: 53s\n",
            "512:\tlearn: 0.0563395\ttotal: 55.8s\tremaining: 52.9s\n",
            "513:\tlearn: 0.0563248\ttotal: 55.9s\tremaining: 52.8s\n",
            "514:\tlearn: 0.0563065\ttotal: 56s\tremaining: 52.7s\n",
            "515:\tlearn: 0.0562990\ttotal: 56.1s\tremaining: 52.6s\n",
            "516:\tlearn: 0.0562938\ttotal: 56.2s\tremaining: 52.5s\n",
            "517:\tlearn: 0.0562473\ttotal: 56.3s\tremaining: 52.4s\n",
            "518:\tlearn: 0.0562339\ttotal: 56.4s\tremaining: 52.3s\n",
            "519:\tlearn: 0.0562258\ttotal: 56.5s\tremaining: 52.2s\n",
            "520:\tlearn: 0.0562164\ttotal: 56.7s\tremaining: 52.1s\n",
            "521:\tlearn: 0.0562093\ttotal: 56.8s\tremaining: 52s\n",
            "522:\tlearn: 0.0561985\ttotal: 56.9s\tremaining: 51.9s\n",
            "523:\tlearn: 0.0561842\ttotal: 57s\tremaining: 51.8s\n",
            "524:\tlearn: 0.0561809\ttotal: 57.1s\tremaining: 51.6s\n",
            "525:\tlearn: 0.0561662\ttotal: 57.2s\tremaining: 51.5s\n",
            "526:\tlearn: 0.0561587\ttotal: 57.3s\tremaining: 51.4s\n",
            "527:\tlearn: 0.0561550\ttotal: 57.4s\tremaining: 51.3s\n",
            "528:\tlearn: 0.0561463\ttotal: 57.5s\tremaining: 51.2s\n",
            "529:\tlearn: 0.0561400\ttotal: 57.6s\tremaining: 51.1s\n",
            "530:\tlearn: 0.0561299\ttotal: 57.7s\tremaining: 51s\n",
            "531:\tlearn: 0.0561250\ttotal: 57.8s\tremaining: 50.9s\n",
            "532:\tlearn: 0.0561175\ttotal: 57.9s\tremaining: 50.8s\n",
            "533:\tlearn: 0.0561089\ttotal: 58s\tremaining: 50.7s\n",
            "534:\tlearn: 0.0561052\ttotal: 58.2s\tremaining: 50.5s\n",
            "535:\tlearn: 0.0560973\ttotal: 58.3s\tremaining: 50.4s\n",
            "536:\tlearn: 0.0560801\ttotal: 58.4s\tremaining: 50.3s\n",
            "537:\tlearn: 0.0560742\ttotal: 58.5s\tremaining: 50.2s\n",
            "538:\tlearn: 0.0560400\ttotal: 58.6s\tremaining: 50.1s\n",
            "539:\tlearn: 0.0560256\ttotal: 58.7s\tremaining: 50s\n",
            "540:\tlearn: 0.0560132\ttotal: 58.8s\tremaining: 49.9s\n",
            "541:\tlearn: 0.0560010\ttotal: 58.9s\tremaining: 49.8s\n",
            "542:\tlearn: 0.0559976\ttotal: 59s\tremaining: 49.7s\n",
            "543:\tlearn: 0.0559820\ttotal: 59.1s\tremaining: 49.6s\n",
            "544:\tlearn: 0.0559794\ttotal: 59.2s\tremaining: 49.5s\n",
            "545:\tlearn: 0.0559763\ttotal: 59.4s\tremaining: 49.4s\n",
            "546:\tlearn: 0.0559708\ttotal: 59.5s\tremaining: 49.2s\n",
            "547:\tlearn: 0.0559527\ttotal: 59.6s\tremaining: 49.1s\n",
            "548:\tlearn: 0.0559464\ttotal: 59.7s\tremaining: 49s\n",
            "549:\tlearn: 0.0559406\ttotal: 59.8s\tremaining: 48.9s\n",
            "550:\tlearn: 0.0559104\ttotal: 59.9s\tremaining: 48.8s\n",
            "551:\tlearn: 0.0558873\ttotal: 1m\tremaining: 48.7s\n",
            "552:\tlearn: 0.0558849\ttotal: 1m\tremaining: 48.6s\n",
            "553:\tlearn: 0.0558773\ttotal: 1m\tremaining: 48.5s\n",
            "554:\tlearn: 0.0558729\ttotal: 1m\tremaining: 48.4s\n",
            "555:\tlearn: 0.0558669\ttotal: 1m\tremaining: 48.3s\n",
            "556:\tlearn: 0.0558637\ttotal: 1m\tremaining: 48.2s\n",
            "557:\tlearn: 0.0558416\ttotal: 1m\tremaining: 48.1s\n",
            "558:\tlearn: 0.0558370\ttotal: 1m\tremaining: 47.9s\n",
            "559:\tlearn: 0.0558314\ttotal: 1m\tremaining: 47.8s\n",
            "560:\tlearn: 0.0557888\ttotal: 1m 1s\tremaining: 47.7s\n",
            "561:\tlearn: 0.0557793\ttotal: 1m 1s\tremaining: 47.6s\n",
            "562:\tlearn: 0.0557677\ttotal: 1m 1s\tremaining: 47.5s\n",
            "563:\tlearn: 0.0557417\ttotal: 1m 1s\tremaining: 47.4s\n",
            "564:\tlearn: 0.0557368\ttotal: 1m 1s\tremaining: 47.3s\n",
            "565:\tlearn: 0.0557238\ttotal: 1m 1s\tremaining: 47.2s\n",
            "566:\tlearn: 0.0557161\ttotal: 1m 1s\tremaining: 47.1s\n",
            "567:\tlearn: 0.0557061\ttotal: 1m 1s\tremaining: 47s\n",
            "568:\tlearn: 0.0556921\ttotal: 1m 1s\tremaining: 46.9s\n",
            "569:\tlearn: 0.0556882\ttotal: 1m 2s\tremaining: 46.8s\n",
            "570:\tlearn: 0.0556836\ttotal: 1m 2s\tremaining: 46.7s\n",
            "571:\tlearn: 0.0556656\ttotal: 1m 2s\tremaining: 46.6s\n",
            "572:\tlearn: 0.0556579\ttotal: 1m 2s\tremaining: 46.5s\n",
            "573:\tlearn: 0.0556494\ttotal: 1m 2s\tremaining: 46.3s\n",
            "574:\tlearn: 0.0556419\ttotal: 1m 2s\tremaining: 46.2s\n",
            "575:\tlearn: 0.0556403\ttotal: 1m 2s\tremaining: 46.1s\n",
            "576:\tlearn: 0.0556246\ttotal: 1m 2s\tremaining: 46s\n",
            "577:\tlearn: 0.0556108\ttotal: 1m 2s\tremaining: 45.9s\n",
            "578:\tlearn: 0.0556045\ttotal: 1m 2s\tremaining: 45.8s\n",
            "579:\tlearn: 0.0555972\ttotal: 1m 3s\tremaining: 45.7s\n",
            "580:\tlearn: 0.0555850\ttotal: 1m 3s\tremaining: 45.6s\n",
            "581:\tlearn: 0.0555795\ttotal: 1m 3s\tremaining: 45.5s\n",
            "582:\tlearn: 0.0555691\ttotal: 1m 3s\tremaining: 45.4s\n",
            "583:\tlearn: 0.0555635\ttotal: 1m 3s\tremaining: 45.3s\n",
            "584:\tlearn: 0.0555572\ttotal: 1m 3s\tremaining: 45.2s\n",
            "585:\tlearn: 0.0555533\ttotal: 1m 3s\tremaining: 45s\n",
            "586:\tlearn: 0.0555471\ttotal: 1m 3s\tremaining: 44.9s\n",
            "587:\tlearn: 0.0555398\ttotal: 1m 3s\tremaining: 44.8s\n",
            "588:\tlearn: 0.0555360\ttotal: 1m 4s\tremaining: 44.7s\n",
            "589:\tlearn: 0.0555263\ttotal: 1m 4s\tremaining: 44.6s\n",
            "590:\tlearn: 0.0555165\ttotal: 1m 4s\tremaining: 44.5s\n",
            "591:\tlearn: 0.0555077\ttotal: 1m 4s\tremaining: 44.4s\n",
            "592:\tlearn: 0.0554807\ttotal: 1m 4s\tremaining: 44.3s\n",
            "593:\tlearn: 0.0554760\ttotal: 1m 4s\tremaining: 44.2s\n",
            "594:\tlearn: 0.0554672\ttotal: 1m 4s\tremaining: 44.1s\n",
            "595:\tlearn: 0.0554608\ttotal: 1m 4s\tremaining: 43.9s\n",
            "596:\tlearn: 0.0554403\ttotal: 1m 4s\tremaining: 43.8s\n",
            "597:\tlearn: 0.0554327\ttotal: 1m 5s\tremaining: 43.7s\n",
            "598:\tlearn: 0.0554273\ttotal: 1m 5s\tremaining: 43.6s\n",
            "599:\tlearn: 0.0554225\ttotal: 1m 5s\tremaining: 43.5s\n",
            "600:\tlearn: 0.0554152\ttotal: 1m 5s\tremaining: 43.4s\n",
            "601:\tlearn: 0.0554032\ttotal: 1m 5s\tremaining: 43.3s\n",
            "602:\tlearn: 0.0553845\ttotal: 1m 5s\tremaining: 43.2s\n",
            "603:\tlearn: 0.0553721\ttotal: 1m 5s\tremaining: 43.1s\n",
            "604:\tlearn: 0.0553640\ttotal: 1m 5s\tremaining: 43s\n",
            "605:\tlearn: 0.0553587\ttotal: 1m 5s\tremaining: 42.9s\n",
            "606:\tlearn: 0.0553515\ttotal: 1m 6s\tremaining: 42.8s\n",
            "607:\tlearn: 0.0553405\ttotal: 1m 6s\tremaining: 42.6s\n",
            "608:\tlearn: 0.0553328\ttotal: 1m 6s\tremaining: 42.5s\n",
            "609:\tlearn: 0.0553047\ttotal: 1m 6s\tremaining: 42.4s\n",
            "610:\tlearn: 0.0552902\ttotal: 1m 6s\tremaining: 42.3s\n",
            "611:\tlearn: 0.0552788\ttotal: 1m 6s\tremaining: 42.2s\n",
            "612:\tlearn: 0.0552576\ttotal: 1m 6s\tremaining: 42.1s\n",
            "613:\tlearn: 0.0552532\ttotal: 1m 6s\tremaining: 42s\n",
            "614:\tlearn: 0.0552389\ttotal: 1m 6s\tremaining: 41.9s\n",
            "615:\tlearn: 0.0552359\ttotal: 1m 7s\tremaining: 41.8s\n",
            "616:\tlearn: 0.0552332\ttotal: 1m 7s\tremaining: 41.7s\n",
            "617:\tlearn: 0.0552218\ttotal: 1m 7s\tremaining: 41.5s\n",
            "618:\tlearn: 0.0552075\ttotal: 1m 7s\tremaining: 41.4s\n",
            "619:\tlearn: 0.0552021\ttotal: 1m 7s\tremaining: 41.3s\n",
            "620:\tlearn: 0.0552001\ttotal: 1m 7s\tremaining: 41.2s\n",
            "621:\tlearn: 0.0551940\ttotal: 1m 7s\tremaining: 41.1s\n",
            "622:\tlearn: 0.0551832\ttotal: 1m 7s\tremaining: 41s\n",
            "623:\tlearn: 0.0551728\ttotal: 1m 7s\tremaining: 40.9s\n",
            "624:\tlearn: 0.0551704\ttotal: 1m 7s\tremaining: 40.8s\n",
            "625:\tlearn: 0.0551621\ttotal: 1m 8s\tremaining: 40.7s\n",
            "626:\tlearn: 0.0551558\ttotal: 1m 8s\tremaining: 40.6s\n",
            "627:\tlearn: 0.0551394\ttotal: 1m 8s\tremaining: 40.4s\n",
            "628:\tlearn: 0.0551260\ttotal: 1m 8s\tremaining: 40.3s\n",
            "629:\tlearn: 0.0551202\ttotal: 1m 8s\tremaining: 40.2s\n",
            "630:\tlearn: 0.0551029\ttotal: 1m 8s\tremaining: 40.1s\n",
            "631:\tlearn: 0.0550969\ttotal: 1m 8s\tremaining: 40s\n",
            "632:\tlearn: 0.0550891\ttotal: 1m 8s\tremaining: 39.9s\n",
            "633:\tlearn: 0.0550785\ttotal: 1m 8s\tremaining: 39.8s\n",
            "634:\tlearn: 0.0550720\ttotal: 1m 9s\tremaining: 39.7s\n",
            "635:\tlearn: 0.0550646\ttotal: 1m 9s\tremaining: 39.6s\n",
            "636:\tlearn: 0.0550619\ttotal: 1m 9s\tremaining: 39.5s\n",
            "637:\tlearn: 0.0550555\ttotal: 1m 9s\tremaining: 39.3s\n",
            "638:\tlearn: 0.0550496\ttotal: 1m 9s\tremaining: 39.2s\n",
            "639:\tlearn: 0.0550412\ttotal: 1m 9s\tremaining: 39.1s\n",
            "640:\tlearn: 0.0550374\ttotal: 1m 9s\tremaining: 39s\n",
            "641:\tlearn: 0.0550298\ttotal: 1m 9s\tremaining: 38.9s\n",
            "642:\tlearn: 0.0550257\ttotal: 1m 9s\tremaining: 38.8s\n",
            "643:\tlearn: 0.0549931\ttotal: 1m 9s\tremaining: 38.7s\n",
            "644:\tlearn: 0.0549801\ttotal: 1m 10s\tremaining: 38.6s\n",
            "645:\tlearn: 0.0549687\ttotal: 1m 10s\tremaining: 38.5s\n",
            "646:\tlearn: 0.0549493\ttotal: 1m 10s\tremaining: 38.3s\n",
            "647:\tlearn: 0.0549446\ttotal: 1m 10s\tremaining: 38.2s\n",
            "648:\tlearn: 0.0549378\ttotal: 1m 10s\tremaining: 38.1s\n",
            "649:\tlearn: 0.0549317\ttotal: 1m 10s\tremaining: 38s\n",
            "650:\tlearn: 0.0549293\ttotal: 1m 10s\tremaining: 37.9s\n",
            "651:\tlearn: 0.0549223\ttotal: 1m 10s\tremaining: 37.8s\n",
            "652:\tlearn: 0.0549128\ttotal: 1m 10s\tremaining: 37.7s\n",
            "653:\tlearn: 0.0549096\ttotal: 1m 11s\tremaining: 37.6s\n",
            "654:\tlearn: 0.0548992\ttotal: 1m 11s\tremaining: 37.5s\n",
            "655:\tlearn: 0.0548930\ttotal: 1m 11s\tremaining: 37.4s\n",
            "656:\tlearn: 0.0548812\ttotal: 1m 11s\tremaining: 37.2s\n",
            "657:\tlearn: 0.0548754\ttotal: 1m 11s\tremaining: 37.1s\n",
            "658:\tlearn: 0.0548716\ttotal: 1m 11s\tremaining: 37s\n",
            "659:\tlearn: 0.0548648\ttotal: 1m 11s\tremaining: 36.9s\n",
            "660:\tlearn: 0.0548616\ttotal: 1m 11s\tremaining: 36.8s\n",
            "661:\tlearn: 0.0548535\ttotal: 1m 11s\tremaining: 36.7s\n",
            "662:\tlearn: 0.0548445\ttotal: 1m 11s\tremaining: 36.6s\n",
            "663:\tlearn: 0.0548371\ttotal: 1m 12s\tremaining: 36.5s\n",
            "664:\tlearn: 0.0548323\ttotal: 1m 12s\tremaining: 36.4s\n",
            "665:\tlearn: 0.0548237\ttotal: 1m 12s\tremaining: 36.3s\n",
            "666:\tlearn: 0.0548159\ttotal: 1m 12s\tremaining: 36.1s\n",
            "667:\tlearn: 0.0548141\ttotal: 1m 12s\tremaining: 36s\n",
            "668:\tlearn: 0.0548057\ttotal: 1m 12s\tremaining: 35.9s\n",
            "669:\tlearn: 0.0548016\ttotal: 1m 12s\tremaining: 35.8s\n",
            "670:\tlearn: 0.0547973\ttotal: 1m 12s\tremaining: 35.7s\n",
            "671:\tlearn: 0.0547941\ttotal: 1m 12s\tremaining: 35.6s\n",
            "672:\tlearn: 0.0547870\ttotal: 1m 13s\tremaining: 35.5s\n",
            "673:\tlearn: 0.0547743\ttotal: 1m 13s\tremaining: 35.4s\n",
            "674:\tlearn: 0.0547480\ttotal: 1m 13s\tremaining: 35.3s\n",
            "675:\tlearn: 0.0547379\ttotal: 1m 13s\tremaining: 35.2s\n",
            "676:\tlearn: 0.0547289\ttotal: 1m 13s\tremaining: 35.1s\n",
            "677:\tlearn: 0.0547222\ttotal: 1m 13s\tremaining: 34.9s\n",
            "678:\tlearn: 0.0547097\ttotal: 1m 13s\tremaining: 34.8s\n",
            "679:\tlearn: 0.0547036\ttotal: 1m 13s\tremaining: 34.7s\n",
            "680:\tlearn: 0.0546970\ttotal: 1m 13s\tremaining: 34.6s\n",
            "681:\tlearn: 0.0546853\ttotal: 1m 14s\tremaining: 34.5s\n",
            "682:\tlearn: 0.0546790\ttotal: 1m 14s\tremaining: 34.4s\n",
            "683:\tlearn: 0.0546737\ttotal: 1m 14s\tremaining: 34.3s\n",
            "684:\tlearn: 0.0546709\ttotal: 1m 14s\tremaining: 34.2s\n",
            "685:\tlearn: 0.0546475\ttotal: 1m 14s\tremaining: 34.1s\n",
            "686:\tlearn: 0.0546403\ttotal: 1m 14s\tremaining: 34s\n",
            "687:\tlearn: 0.0546336\ttotal: 1m 14s\tremaining: 33.9s\n",
            "688:\tlearn: 0.0546279\ttotal: 1m 14s\tremaining: 33.7s\n",
            "689:\tlearn: 0.0546157\ttotal: 1m 14s\tremaining: 33.6s\n",
            "690:\tlearn: 0.0545802\ttotal: 1m 14s\tremaining: 33.5s\n",
            "691:\tlearn: 0.0545732\ttotal: 1m 15s\tremaining: 33.4s\n",
            "692:\tlearn: 0.0545691\ttotal: 1m 15s\tremaining: 33.3s\n",
            "693:\tlearn: 0.0545481\ttotal: 1m 15s\tremaining: 33.2s\n",
            "694:\tlearn: 0.0545379\ttotal: 1m 15s\tremaining: 33.1s\n",
            "695:\tlearn: 0.0545324\ttotal: 1m 15s\tremaining: 33s\n",
            "696:\tlearn: 0.0545174\ttotal: 1m 15s\tremaining: 32.9s\n",
            "697:\tlearn: 0.0545101\ttotal: 1m 15s\tremaining: 32.8s\n",
            "698:\tlearn: 0.0545012\ttotal: 1m 15s\tremaining: 32.6s\n",
            "699:\tlearn: 0.0544932\ttotal: 1m 15s\tremaining: 32.5s\n",
            "700:\tlearn: 0.0544888\ttotal: 1m 16s\tremaining: 32.4s\n",
            "701:\tlearn: 0.0544850\ttotal: 1m 16s\tremaining: 32.3s\n",
            "702:\tlearn: 0.0544766\ttotal: 1m 16s\tremaining: 32.2s\n",
            "703:\tlearn: 0.0544609\ttotal: 1m 16s\tremaining: 32.1s\n",
            "704:\tlearn: 0.0544592\ttotal: 1m 16s\tremaining: 32s\n",
            "705:\tlearn: 0.0544493\ttotal: 1m 16s\tremaining: 31.9s\n",
            "706:\tlearn: 0.0544454\ttotal: 1m 16s\tremaining: 31.8s\n",
            "707:\tlearn: 0.0544424\ttotal: 1m 16s\tremaining: 31.7s\n",
            "708:\tlearn: 0.0544333\ttotal: 1m 16s\tremaining: 31.5s\n",
            "709:\tlearn: 0.0544222\ttotal: 1m 16s\tremaining: 31.4s\n",
            "710:\tlearn: 0.0544171\ttotal: 1m 17s\tremaining: 31.3s\n",
            "711:\tlearn: 0.0544133\ttotal: 1m 17s\tremaining: 31.2s\n",
            "712:\tlearn: 0.0544081\ttotal: 1m 17s\tremaining: 31.1s\n",
            "713:\tlearn: 0.0544006\ttotal: 1m 17s\tremaining: 31s\n",
            "714:\tlearn: 0.0543987\ttotal: 1m 17s\tremaining: 30.9s\n",
            "715:\tlearn: 0.0543966\ttotal: 1m 17s\tremaining: 30.8s\n",
            "716:\tlearn: 0.0543880\ttotal: 1m 17s\tremaining: 30.7s\n",
            "717:\tlearn: 0.0543759\ttotal: 1m 17s\tremaining: 30.6s\n",
            "718:\tlearn: 0.0543654\ttotal: 1m 17s\tremaining: 30.5s\n",
            "719:\tlearn: 0.0543309\ttotal: 1m 18s\tremaining: 30.3s\n",
            "720:\tlearn: 0.0543251\ttotal: 1m 18s\tremaining: 30.2s\n",
            "721:\tlearn: 0.0543164\ttotal: 1m 18s\tremaining: 30.1s\n",
            "722:\tlearn: 0.0543126\ttotal: 1m 18s\tremaining: 30s\n",
            "723:\tlearn: 0.0543026\ttotal: 1m 18s\tremaining: 29.9s\n",
            "724:\tlearn: 0.0542975\ttotal: 1m 18s\tremaining: 29.8s\n",
            "725:\tlearn: 0.0542925\ttotal: 1m 18s\tremaining: 29.7s\n",
            "726:\tlearn: 0.0542791\ttotal: 1m 18s\tremaining: 29.6s\n",
            "727:\tlearn: 0.0542770\ttotal: 1m 18s\tremaining: 29.5s\n",
            "728:\tlearn: 0.0542731\ttotal: 1m 18s\tremaining: 29.4s\n",
            "729:\tlearn: 0.0542699\ttotal: 1m 19s\tremaining: 29.2s\n",
            "730:\tlearn: 0.0542601\ttotal: 1m 19s\tremaining: 29.1s\n",
            "731:\tlearn: 0.0542371\ttotal: 1m 19s\tremaining: 29s\n",
            "732:\tlearn: 0.0542350\ttotal: 1m 19s\tremaining: 28.9s\n",
            "733:\tlearn: 0.0542256\ttotal: 1m 19s\tremaining: 28.8s\n",
            "734:\tlearn: 0.0542216\ttotal: 1m 19s\tremaining: 28.7s\n",
            "735:\tlearn: 0.0542090\ttotal: 1m 19s\tremaining: 28.6s\n",
            "736:\tlearn: 0.0542039\ttotal: 1m 19s\tremaining: 28.5s\n",
            "737:\tlearn: 0.0541952\ttotal: 1m 19s\tremaining: 28.4s\n",
            "738:\tlearn: 0.0541887\ttotal: 1m 20s\tremaining: 28.3s\n",
            "739:\tlearn: 0.0541847\ttotal: 1m 20s\tremaining: 28.2s\n",
            "740:\tlearn: 0.0541830\ttotal: 1m 20s\tremaining: 28.1s\n",
            "741:\tlearn: 0.0541737\ttotal: 1m 20s\tremaining: 27.9s\n",
            "742:\tlearn: 0.0541638\ttotal: 1m 20s\tremaining: 27.8s\n",
            "743:\tlearn: 0.0541593\ttotal: 1m 20s\tremaining: 27.7s\n",
            "744:\tlearn: 0.0541497\ttotal: 1m 20s\tremaining: 27.6s\n",
            "745:\tlearn: 0.0541466\ttotal: 1m 20s\tremaining: 27.5s\n",
            "746:\tlearn: 0.0541454\ttotal: 1m 20s\tremaining: 27.4s\n",
            "747:\tlearn: 0.0541367\ttotal: 1m 21s\tremaining: 27.3s\n",
            "748:\tlearn: 0.0541330\ttotal: 1m 21s\tremaining: 27.2s\n",
            "749:\tlearn: 0.0541290\ttotal: 1m 21s\tremaining: 27.1s\n",
            "750:\tlearn: 0.0541209\ttotal: 1m 21s\tremaining: 27s\n",
            "751:\tlearn: 0.0541172\ttotal: 1m 21s\tremaining: 26.9s\n",
            "752:\tlearn: 0.0541070\ttotal: 1m 21s\tremaining: 26.7s\n",
            "753:\tlearn: 0.0541040\ttotal: 1m 21s\tremaining: 26.6s\n",
            "754:\tlearn: 0.0540934\ttotal: 1m 21s\tremaining: 26.5s\n",
            "755:\tlearn: 0.0540909\ttotal: 1m 21s\tremaining: 26.4s\n",
            "756:\tlearn: 0.0540904\ttotal: 1m 21s\tremaining: 26.3s\n",
            "757:\tlearn: 0.0540871\ttotal: 1m 22s\tremaining: 26.2s\n",
            "758:\tlearn: 0.0540807\ttotal: 1m 22s\tremaining: 26.1s\n",
            "759:\tlearn: 0.0540734\ttotal: 1m 22s\tremaining: 26s\n",
            "760:\tlearn: 0.0540712\ttotal: 1m 22s\tremaining: 25.9s\n",
            "761:\tlearn: 0.0540663\ttotal: 1m 22s\tremaining: 25.8s\n",
            "762:\tlearn: 0.0540597\ttotal: 1m 22s\tremaining: 25.7s\n",
            "763:\tlearn: 0.0540530\ttotal: 1m 22s\tremaining: 25.5s\n",
            "764:\tlearn: 0.0540475\ttotal: 1m 22s\tremaining: 25.4s\n",
            "765:\tlearn: 0.0540432\ttotal: 1m 22s\tremaining: 25.3s\n",
            "766:\tlearn: 0.0540301\ttotal: 1m 23s\tremaining: 25.2s\n",
            "767:\tlearn: 0.0540271\ttotal: 1m 23s\tremaining: 25.1s\n",
            "768:\tlearn: 0.0540170\ttotal: 1m 23s\tremaining: 25s\n",
            "769:\tlearn: 0.0540122\ttotal: 1m 23s\tremaining: 24.9s\n",
            "770:\tlearn: 0.0540097\ttotal: 1m 23s\tremaining: 24.8s\n",
            "771:\tlearn: 0.0540056\ttotal: 1m 23s\tremaining: 24.7s\n",
            "772:\tlearn: 0.0540006\ttotal: 1m 23s\tremaining: 24.6s\n",
            "773:\tlearn: 0.0539969\ttotal: 1m 23s\tremaining: 24.5s\n",
            "774:\tlearn: 0.0539921\ttotal: 1m 23s\tremaining: 24.4s\n",
            "775:\tlearn: 0.0539895\ttotal: 1m 23s\tremaining: 24.2s\n",
            "776:\tlearn: 0.0539856\ttotal: 1m 24s\tremaining: 24.1s\n",
            "777:\tlearn: 0.0539682\ttotal: 1m 24s\tremaining: 24s\n",
            "778:\tlearn: 0.0539649\ttotal: 1m 24s\tremaining: 23.9s\n",
            "779:\tlearn: 0.0539603\ttotal: 1m 24s\tremaining: 23.8s\n",
            "780:\tlearn: 0.0539582\ttotal: 1m 24s\tremaining: 23.7s\n",
            "781:\tlearn: 0.0539448\ttotal: 1m 24s\tremaining: 23.6s\n",
            "782:\tlearn: 0.0539354\ttotal: 1m 24s\tremaining: 23.5s\n",
            "783:\tlearn: 0.0539330\ttotal: 1m 24s\tremaining: 23.4s\n",
            "784:\tlearn: 0.0539275\ttotal: 1m 24s\tremaining: 23.3s\n",
            "785:\tlearn: 0.0539107\ttotal: 1m 25s\tremaining: 23.2s\n",
            "786:\tlearn: 0.0539021\ttotal: 1m 25s\tremaining: 23.1s\n",
            "787:\tlearn: 0.0538975\ttotal: 1m 25s\tremaining: 22.9s\n",
            "788:\tlearn: 0.0538941\ttotal: 1m 25s\tremaining: 22.8s\n",
            "789:\tlearn: 0.0538912\ttotal: 1m 25s\tremaining: 22.7s\n",
            "790:\tlearn: 0.0538880\ttotal: 1m 25s\tremaining: 22.6s\n",
            "791:\tlearn: 0.0538865\ttotal: 1m 25s\tremaining: 22.5s\n",
            "792:\tlearn: 0.0538848\ttotal: 1m 25s\tremaining: 22.4s\n",
            "793:\tlearn: 0.0538775\ttotal: 1m 25s\tremaining: 22.3s\n",
            "794:\tlearn: 0.0538734\ttotal: 1m 26s\tremaining: 22.2s\n",
            "795:\tlearn: 0.0538629\ttotal: 1m 26s\tremaining: 22.1s\n",
            "796:\tlearn: 0.0538593\ttotal: 1m 26s\tremaining: 22s\n",
            "797:\tlearn: 0.0538562\ttotal: 1m 26s\tremaining: 21.9s\n",
            "798:\tlearn: 0.0538537\ttotal: 1m 26s\tremaining: 21.7s\n",
            "799:\tlearn: 0.0538474\ttotal: 1m 26s\tremaining: 21.6s\n",
            "800:\tlearn: 0.0538407\ttotal: 1m 26s\tremaining: 21.5s\n",
            "801:\tlearn: 0.0538355\ttotal: 1m 26s\tremaining: 21.4s\n",
            "802:\tlearn: 0.0538321\ttotal: 1m 26s\tremaining: 21.3s\n",
            "803:\tlearn: 0.0538290\ttotal: 1m 26s\tremaining: 21.2s\n",
            "804:\tlearn: 0.0538202\ttotal: 1m 27s\tremaining: 21.1s\n",
            "805:\tlearn: 0.0538181\ttotal: 1m 27s\tremaining: 21s\n",
            "806:\tlearn: 0.0538068\ttotal: 1m 27s\tremaining: 20.9s\n",
            "807:\tlearn: 0.0538033\ttotal: 1m 27s\tremaining: 20.8s\n",
            "808:\tlearn: 0.0537994\ttotal: 1m 27s\tremaining: 20.7s\n",
            "809:\tlearn: 0.0537947\ttotal: 1m 27s\tremaining: 20.5s\n",
            "810:\tlearn: 0.0537869\ttotal: 1m 27s\tremaining: 20.4s\n",
            "811:\tlearn: 0.0537793\ttotal: 1m 27s\tremaining: 20.3s\n",
            "812:\tlearn: 0.0537726\ttotal: 1m 27s\tremaining: 20.2s\n",
            "813:\tlearn: 0.0537647\ttotal: 1m 28s\tremaining: 20.1s\n",
            "814:\tlearn: 0.0537608\ttotal: 1m 28s\tremaining: 20s\n",
            "815:\tlearn: 0.0537562\ttotal: 1m 28s\tremaining: 19.9s\n",
            "816:\tlearn: 0.0537526\ttotal: 1m 28s\tremaining: 19.8s\n",
            "817:\tlearn: 0.0537461\ttotal: 1m 28s\tremaining: 19.7s\n",
            "818:\tlearn: 0.0537393\ttotal: 1m 28s\tremaining: 19.6s\n",
            "819:\tlearn: 0.0537332\ttotal: 1m 28s\tremaining: 19.5s\n",
            "820:\tlearn: 0.0537271\ttotal: 1m 28s\tremaining: 19.4s\n",
            "821:\tlearn: 0.0537231\ttotal: 1m 28s\tremaining: 19.2s\n",
            "822:\tlearn: 0.0537134\ttotal: 1m 28s\tremaining: 19.1s\n",
            "823:\tlearn: 0.0537115\ttotal: 1m 29s\tremaining: 19s\n",
            "824:\tlearn: 0.0537099\ttotal: 1m 29s\tremaining: 18.9s\n",
            "825:\tlearn: 0.0536893\ttotal: 1m 29s\tremaining: 18.8s\n",
            "826:\tlearn: 0.0536806\ttotal: 1m 29s\tremaining: 18.7s\n",
            "827:\tlearn: 0.0536765\ttotal: 1m 29s\tremaining: 18.6s\n",
            "828:\tlearn: 0.0536741\ttotal: 1m 29s\tremaining: 18.5s\n",
            "829:\tlearn: 0.0536648\ttotal: 1m 29s\tremaining: 18.4s\n",
            "830:\tlearn: 0.0536627\ttotal: 1m 29s\tremaining: 18.3s\n",
            "831:\tlearn: 0.0536611\ttotal: 1m 29s\tremaining: 18.2s\n",
            "832:\tlearn: 0.0536595\ttotal: 1m 30s\tremaining: 18.1s\n",
            "833:\tlearn: 0.0536480\ttotal: 1m 30s\tremaining: 17.9s\n",
            "834:\tlearn: 0.0536395\ttotal: 1m 30s\tremaining: 17.8s\n",
            "835:\tlearn: 0.0536338\ttotal: 1m 30s\tremaining: 17.7s\n",
            "836:\tlearn: 0.0536314\ttotal: 1m 30s\tremaining: 17.6s\n",
            "837:\tlearn: 0.0536095\ttotal: 1m 30s\tremaining: 17.5s\n",
            "838:\tlearn: 0.0535929\ttotal: 1m 30s\tremaining: 17.4s\n",
            "839:\tlearn: 0.0535878\ttotal: 1m 30s\tremaining: 17.3s\n",
            "840:\tlearn: 0.0535820\ttotal: 1m 30s\tremaining: 17.2s\n",
            "841:\tlearn: 0.0535788\ttotal: 1m 30s\tremaining: 17.1s\n",
            "842:\tlearn: 0.0535695\ttotal: 1m 31s\tremaining: 17s\n",
            "843:\tlearn: 0.0535427\ttotal: 1m 31s\tremaining: 16.9s\n",
            "844:\tlearn: 0.0535372\ttotal: 1m 31s\tremaining: 16.8s\n",
            "845:\tlearn: 0.0535334\ttotal: 1m 31s\tremaining: 16.6s\n",
            "846:\tlearn: 0.0535310\ttotal: 1m 31s\tremaining: 16.5s\n",
            "847:\tlearn: 0.0535216\ttotal: 1m 31s\tremaining: 16.4s\n",
            "848:\tlearn: 0.0535144\ttotal: 1m 31s\tremaining: 16.3s\n",
            "849:\tlearn: 0.0535124\ttotal: 1m 31s\tremaining: 16.2s\n",
            "850:\tlearn: 0.0535085\ttotal: 1m 31s\tremaining: 16.1s\n",
            "851:\tlearn: 0.0535013\ttotal: 1m 32s\tremaining: 16s\n",
            "852:\tlearn: 0.0534953\ttotal: 1m 32s\tremaining: 15.9s\n",
            "853:\tlearn: 0.0534852\ttotal: 1m 32s\tremaining: 15.8s\n",
            "854:\tlearn: 0.0534812\ttotal: 1m 32s\tremaining: 15.7s\n",
            "855:\tlearn: 0.0534790\ttotal: 1m 32s\tremaining: 15.6s\n",
            "856:\tlearn: 0.0534746\ttotal: 1m 32s\tremaining: 15.5s\n",
            "857:\tlearn: 0.0534708\ttotal: 1m 32s\tremaining: 15.3s\n",
            "858:\tlearn: 0.0534689\ttotal: 1m 32s\tremaining: 15.2s\n",
            "859:\tlearn: 0.0534628\ttotal: 1m 32s\tremaining: 15.1s\n",
            "860:\tlearn: 0.0534590\ttotal: 1m 33s\tremaining: 15s\n",
            "861:\tlearn: 0.0534553\ttotal: 1m 33s\tremaining: 14.9s\n",
            "862:\tlearn: 0.0534509\ttotal: 1m 33s\tremaining: 14.8s\n",
            "863:\tlearn: 0.0534426\ttotal: 1m 33s\tremaining: 14.7s\n",
            "864:\tlearn: 0.0534406\ttotal: 1m 33s\tremaining: 14.6s\n",
            "865:\tlearn: 0.0534273\ttotal: 1m 33s\tremaining: 14.5s\n",
            "866:\tlearn: 0.0534220\ttotal: 1m 33s\tremaining: 14.4s\n",
            "867:\tlearn: 0.0534176\ttotal: 1m 33s\tremaining: 14.3s\n",
            "868:\tlearn: 0.0534133\ttotal: 1m 33s\tremaining: 14.2s\n",
            "869:\tlearn: 0.0534053\ttotal: 1m 33s\tremaining: 14s\n",
            "870:\tlearn: 0.0534028\ttotal: 1m 34s\tremaining: 13.9s\n",
            "871:\tlearn: 0.0534004\ttotal: 1m 34s\tremaining: 13.8s\n",
            "872:\tlearn: 0.0533967\ttotal: 1m 34s\tremaining: 13.7s\n",
            "873:\tlearn: 0.0533874\ttotal: 1m 34s\tremaining: 13.6s\n",
            "874:\tlearn: 0.0533814\ttotal: 1m 34s\tremaining: 13.5s\n",
            "875:\tlearn: 0.0533693\ttotal: 1m 34s\tremaining: 13.4s\n",
            "876:\tlearn: 0.0533654\ttotal: 1m 34s\tremaining: 13.3s\n",
            "877:\tlearn: 0.0533637\ttotal: 1m 34s\tremaining: 13.2s\n",
            "878:\tlearn: 0.0533621\ttotal: 1m 34s\tremaining: 13.1s\n",
            "879:\tlearn: 0.0533600\ttotal: 1m 35s\tremaining: 13s\n",
            "880:\tlearn: 0.0533570\ttotal: 1m 35s\tremaining: 12.8s\n",
            "881:\tlearn: 0.0533420\ttotal: 1m 35s\tremaining: 12.7s\n",
            "882:\tlearn: 0.0533372\ttotal: 1m 35s\tremaining: 12.6s\n",
            "883:\tlearn: 0.0533335\ttotal: 1m 35s\tremaining: 12.5s\n",
            "884:\tlearn: 0.0533278\ttotal: 1m 35s\tremaining: 12.4s\n",
            "885:\tlearn: 0.0533240\ttotal: 1m 35s\tremaining: 12.3s\n",
            "886:\tlearn: 0.0533109\ttotal: 1m 35s\tremaining: 12.2s\n",
            "887:\tlearn: 0.0533012\ttotal: 1m 35s\tremaining: 12.1s\n",
            "888:\tlearn: 0.0532980\ttotal: 1m 35s\tremaining: 12s\n",
            "889:\tlearn: 0.0532890\ttotal: 1m 36s\tremaining: 11.9s\n",
            "890:\tlearn: 0.0532825\ttotal: 1m 36s\tremaining: 11.8s\n",
            "891:\tlearn: 0.0532689\ttotal: 1m 36s\tremaining: 11.7s\n",
            "892:\tlearn: 0.0532655\ttotal: 1m 36s\tremaining: 11.6s\n",
            "893:\tlearn: 0.0532631\ttotal: 1m 36s\tremaining: 11.4s\n",
            "894:\tlearn: 0.0532533\ttotal: 1m 36s\tremaining: 11.3s\n",
            "895:\tlearn: 0.0532515\ttotal: 1m 36s\tremaining: 11.2s\n",
            "896:\tlearn: 0.0532485\ttotal: 1m 36s\tremaining: 11.1s\n",
            "897:\tlearn: 0.0532426\ttotal: 1m 36s\tremaining: 11s\n",
            "898:\tlearn: 0.0532373\ttotal: 1m 37s\tremaining: 10.9s\n",
            "899:\tlearn: 0.0532149\ttotal: 1m 37s\tremaining: 10.8s\n",
            "900:\tlearn: 0.0532130\ttotal: 1m 37s\tremaining: 10.7s\n",
            "901:\tlearn: 0.0532105\ttotal: 1m 37s\tremaining: 10.6s\n",
            "902:\tlearn: 0.0532084\ttotal: 1m 37s\tremaining: 10.5s\n",
            "903:\tlearn: 0.0532019\ttotal: 1m 37s\tremaining: 10.4s\n",
            "904:\tlearn: 0.0531931\ttotal: 1m 37s\tremaining: 10.3s\n",
            "905:\tlearn: 0.0531922\ttotal: 1m 37s\tremaining: 10.1s\n",
            "906:\tlearn: 0.0531879\ttotal: 1m 37s\tremaining: 10s\n",
            "907:\tlearn: 0.0531776\ttotal: 1m 37s\tremaining: 9.93s\n",
            "908:\tlearn: 0.0531752\ttotal: 1m 38s\tremaining: 9.82s\n",
            "909:\tlearn: 0.0531665\ttotal: 1m 38s\tremaining: 9.71s\n",
            "910:\tlearn: 0.0531624\ttotal: 1m 38s\tremaining: 9.6s\n",
            "911:\tlearn: 0.0531585\ttotal: 1m 38s\tremaining: 9.49s\n",
            "912:\tlearn: 0.0531535\ttotal: 1m 38s\tremaining: 9.38s\n",
            "913:\tlearn: 0.0531475\ttotal: 1m 38s\tremaining: 9.28s\n",
            "914:\tlearn: 0.0531428\ttotal: 1m 38s\tremaining: 9.17s\n",
            "915:\tlearn: 0.0531360\ttotal: 1m 38s\tremaining: 9.06s\n",
            "916:\tlearn: 0.0531311\ttotal: 1m 38s\tremaining: 8.95s\n",
            "917:\tlearn: 0.0531277\ttotal: 1m 39s\tremaining: 8.85s\n",
            "918:\tlearn: 0.0531174\ttotal: 1m 39s\tremaining: 8.74s\n",
            "919:\tlearn: 0.0531123\ttotal: 1m 39s\tremaining: 8.63s\n",
            "920:\tlearn: 0.0530991\ttotal: 1m 39s\tremaining: 8.52s\n",
            "921:\tlearn: 0.0530932\ttotal: 1m 39s\tremaining: 8.41s\n",
            "922:\tlearn: 0.0530858\ttotal: 1m 39s\tremaining: 8.3s\n",
            "923:\tlearn: 0.0530753\ttotal: 1m 39s\tremaining: 8.2s\n",
            "924:\tlearn: 0.0530729\ttotal: 1m 39s\tremaining: 8.09s\n",
            "925:\tlearn: 0.0530666\ttotal: 1m 39s\tremaining: 7.98s\n",
            "926:\tlearn: 0.0530613\ttotal: 1m 39s\tremaining: 7.87s\n",
            "927:\tlearn: 0.0530603\ttotal: 1m 40s\tremaining: 7.76s\n",
            "928:\tlearn: 0.0530578\ttotal: 1m 40s\tremaining: 7.66s\n",
            "929:\tlearn: 0.0530539\ttotal: 1m 40s\tremaining: 7.55s\n",
            "930:\tlearn: 0.0530479\ttotal: 1m 40s\tremaining: 7.44s\n",
            "931:\tlearn: 0.0530427\ttotal: 1m 40s\tremaining: 7.33s\n",
            "932:\tlearn: 0.0530329\ttotal: 1m 40s\tremaining: 7.22s\n",
            "933:\tlearn: 0.0530254\ttotal: 1m 40s\tremaining: 7.12s\n",
            "934:\tlearn: 0.0530190\ttotal: 1m 40s\tremaining: 7.01s\n",
            "935:\tlearn: 0.0530097\ttotal: 1m 40s\tremaining: 6.9s\n",
            "936:\tlearn: 0.0529756\ttotal: 1m 41s\tremaining: 6.79s\n",
            "937:\tlearn: 0.0529716\ttotal: 1m 41s\tremaining: 6.68s\n",
            "938:\tlearn: 0.0529605\ttotal: 1m 41s\tremaining: 6.58s\n",
            "939:\tlearn: 0.0529586\ttotal: 1m 41s\tremaining: 6.47s\n",
            "940:\tlearn: 0.0529573\ttotal: 1m 41s\tremaining: 6.36s\n",
            "941:\tlearn: 0.0529493\ttotal: 1m 41s\tremaining: 6.25s\n",
            "942:\tlearn: 0.0529466\ttotal: 1m 41s\tremaining: 6.14s\n",
            "943:\tlearn: 0.0529459\ttotal: 1m 41s\tremaining: 6.04s\n",
            "944:\tlearn: 0.0529403\ttotal: 1m 41s\tremaining: 5.93s\n",
            "945:\tlearn: 0.0529372\ttotal: 1m 41s\tremaining: 5.82s\n",
            "946:\tlearn: 0.0529337\ttotal: 1m 42s\tremaining: 5.71s\n",
            "947:\tlearn: 0.0529309\ttotal: 1m 42s\tremaining: 5.61s\n",
            "948:\tlearn: 0.0529253\ttotal: 1m 42s\tremaining: 5.5s\n",
            "949:\tlearn: 0.0529229\ttotal: 1m 42s\tremaining: 5.39s\n",
            "950:\tlearn: 0.0529187\ttotal: 1m 42s\tremaining: 5.28s\n",
            "951:\tlearn: 0.0529142\ttotal: 1m 42s\tremaining: 5.17s\n",
            "952:\tlearn: 0.0529115\ttotal: 1m 42s\tremaining: 5.07s\n",
            "953:\tlearn: 0.0529085\ttotal: 1m 42s\tremaining: 4.96s\n",
            "954:\tlearn: 0.0529022\ttotal: 1m 42s\tremaining: 4.85s\n",
            "955:\tlearn: 0.0528941\ttotal: 1m 43s\tremaining: 4.74s\n",
            "956:\tlearn: 0.0528894\ttotal: 1m 43s\tremaining: 4.63s\n",
            "957:\tlearn: 0.0528753\ttotal: 1m 43s\tremaining: 4.53s\n",
            "958:\tlearn: 0.0528744\ttotal: 1m 43s\tremaining: 4.42s\n",
            "959:\tlearn: 0.0528731\ttotal: 1m 43s\tremaining: 4.31s\n",
            "960:\tlearn: 0.0528673\ttotal: 1m 43s\tremaining: 4.2s\n",
            "961:\tlearn: 0.0528616\ttotal: 1m 43s\tremaining: 4.09s\n",
            "962:\tlearn: 0.0528607\ttotal: 1m 43s\tremaining: 3.99s\n",
            "963:\tlearn: 0.0528586\ttotal: 1m 43s\tremaining: 3.88s\n",
            "964:\tlearn: 0.0528535\ttotal: 1m 43s\tremaining: 3.77s\n",
            "965:\tlearn: 0.0528524\ttotal: 1m 44s\tremaining: 3.66s\n",
            "966:\tlearn: 0.0528507\ttotal: 1m 44s\tremaining: 3.56s\n",
            "967:\tlearn: 0.0528495\ttotal: 1m 44s\tremaining: 3.45s\n",
            "968:\tlearn: 0.0528428\ttotal: 1m 44s\tremaining: 3.34s\n",
            "969:\tlearn: 0.0528369\ttotal: 1m 44s\tremaining: 3.23s\n",
            "970:\tlearn: 0.0528338\ttotal: 1m 44s\tremaining: 3.12s\n",
            "971:\tlearn: 0.0528150\ttotal: 1m 44s\tremaining: 3.02s\n",
            "972:\tlearn: 0.0528134\ttotal: 1m 44s\tremaining: 2.91s\n",
            "973:\tlearn: 0.0528099\ttotal: 1m 44s\tremaining: 2.8s\n",
            "974:\tlearn: 0.0528088\ttotal: 1m 45s\tremaining: 2.69s\n",
            "975:\tlearn: 0.0528078\ttotal: 1m 45s\tremaining: 2.58s\n",
            "976:\tlearn: 0.0528051\ttotal: 1m 45s\tremaining: 2.48s\n",
            "977:\tlearn: 0.0528029\ttotal: 1m 45s\tremaining: 2.37s\n",
            "978:\tlearn: 0.0528004\ttotal: 1m 45s\tremaining: 2.26s\n",
            "979:\tlearn: 0.0527931\ttotal: 1m 45s\tremaining: 2.15s\n",
            "980:\tlearn: 0.0527869\ttotal: 1m 45s\tremaining: 2.05s\n",
            "981:\tlearn: 0.0527842\ttotal: 1m 45s\tremaining: 1.94s\n",
            "982:\tlearn: 0.0527708\ttotal: 1m 45s\tremaining: 1.83s\n",
            "983:\tlearn: 0.0527658\ttotal: 1m 45s\tremaining: 1.72s\n",
            "984:\tlearn: 0.0527635\ttotal: 1m 46s\tremaining: 1.61s\n",
            "985:\tlearn: 0.0527462\ttotal: 1m 46s\tremaining: 1.51s\n",
            "986:\tlearn: 0.0527429\ttotal: 1m 46s\tremaining: 1.4s\n",
            "987:\tlearn: 0.0527397\ttotal: 1m 46s\tremaining: 1.29s\n",
            "988:\tlearn: 0.0527383\ttotal: 1m 46s\tremaining: 1.18s\n",
            "989:\tlearn: 0.0527353\ttotal: 1m 46s\tremaining: 1.08s\n",
            "990:\tlearn: 0.0527307\ttotal: 1m 46s\tremaining: 969ms\n",
            "991:\tlearn: 0.0527224\ttotal: 1m 46s\tremaining: 862ms\n",
            "992:\tlearn: 0.0527176\ttotal: 1m 46s\tremaining: 754ms\n",
            "993:\tlearn: 0.0527161\ttotal: 1m 47s\tremaining: 646ms\n",
            "994:\tlearn: 0.0527095\ttotal: 1m 47s\tremaining: 538ms\n",
            "995:\tlearn: 0.0527054\ttotal: 1m 47s\tremaining: 431ms\n",
            "996:\tlearn: 0.0527042\ttotal: 1m 47s\tremaining: 323ms\n",
            "997:\tlearn: 0.0527012\ttotal: 1m 47s\tremaining: 215ms\n",
            "998:\tlearn: 0.0526979\ttotal: 1m 47s\tremaining: 108ms\n",
            "999:\tlearn: 0.0526851\ttotal: 1m 47s\tremaining: 0us\n",
            "Accuracy of Model: 0.9837523582181497\n",
            "Precision of Model: 0.9799815512719356\n",
            "Recall of Model: 0.9837523582181497\n",
            "F1-score of Model: 0.9798182222387176\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     21576\n",
            "           1       0.99      1.00      1.00       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           4       1.00      1.00      1.00      1098\n",
            "           5       0.75      1.00      0.86         3\n",
            "           6       1.00      1.00      1.00       922\n",
            "           7       1.00      0.98      0.99        66\n",
            "           8       1.00      1.00      1.00       739\n",
            "           9       0.78      0.48      0.59       224\n",
            "          10       1.00      0.89      0.94        18\n",
            "          11       0.70      0.90      0.79       309\n",
            "          12       0.51      0.08      0.14       259\n",
            "          14       1.00      1.00      1.00       300\n",
            "\n",
            "    accuracy                           0.98     25973\n",
            "   macro avg       0.82      0.79      0.79     25973\n",
            "weighted avg       0.98      0.98      0.98     25973\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  cat_model.sav\n",
            "Accuracy Model:  0.9822507989065568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Kw1VrfyUhN1",
        "outputId": "1ea51989-fb6f-456f-9e79-4a925a2e93f4"
      },
      "source": [
        "build_model(light)\n",
        "light_train , light_test = save_model(light,'light_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
            "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
            "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
            "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
            "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
            "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
            "Accuracy of Model: 0.8622030570207523\n",
            "Precision of Model: 0.8234053988782165\n",
            "Recall of Model: 0.8622030570207523\n",
            "F1-score of Model: 0.8407571631800697\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94     21576\n",
            "           1       0.47      0.87      0.61       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           3       0.00      0.00      0.00         0\n",
            "           4       0.02      0.00      0.00      1098\n",
            "           5       0.00      0.00      0.00         3\n",
            "           6       0.73      0.62      0.67       922\n",
            "           7       0.00      0.00      0.00        66\n",
            "           8       0.80      0.91      0.85       739\n",
            "           9       0.00      0.00      0.00       224\n",
            "          10       0.00      0.00      0.00        18\n",
            "          11       0.00      0.00      0.00       309\n",
            "          12       0.01      0.01      0.01       259\n",
            "          14       0.00      0.00      0.00       300\n",
            "\n",
            "    accuracy                           0.86     25973\n",
            "   macro avg       0.21      0.24      0.22     25973\n",
            "weighted avg       0.82      0.86      0.84     25973\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  light_model.sav\n",
            "Accuracy Model:  0.9822507989065568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV36zGtQUhH6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBlRuIuq2wfS"
      },
      "source": [
        "### 4. Features Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsIb05PR7WWz"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "feature_names = [f'feature {i}' for i in range(X.shape[1])]\n",
        "forest = RandomForestClassifier(n_estimators=250,random_state=0)\n",
        "forest.fit(X_train3, y_train3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zg0bQdhwwjY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPGmcIp3mEQ8"
      },
      "source": [
        "importances = forest.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "refclasscol=list(df.columns.values)\n",
        "impocol = pd.DataFrame({'Features':refclasscol[0:69],'importance':importances[0:69]})\n",
        "impocol = impocol.sort_values('importance',ascending=False).set_index('Features')\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (15, 7)\n",
        "impocol.plot.bar();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZXAORac3AjE"
      },
      "source": [
        "impocol.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byBi31GvV9Kz"
      },
      "source": [
        "impocol.tail(14) # 69 - 14 = 55  ## features < 0.001000 you will delete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcF5kbLegao9"
      },
      "source": [
        "impocol = pd.DataFrame({'Features':refclasscol[0:55],'importance':importances[0:55]}) # we will selected 55 features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0jW5WapVAae"
      },
      "source": [
        "implist = impocol['Features']\n",
        "implist.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InHhZSqgY_Y0"
      },
      "source": [
        "implist = ['dst_port',  'protocol',  'timestamp',  'flow_duration',  'tot_fwd_pkts',  'tot_bwd_pkts',  'totlen_fwd_pkts',  \n",
        "           'totlen_bwd_pkts',  'fwd_pkt_len_max',  'fwd_pkt_len_min',  'fwd_pkt_len_mean',  'fwd_pkt_len_std',  'bwd_pkt_len_max',  \n",
        "           'bwd_pkt_len_min',  'bwd_pkt_len_mean',  'bwd_pkt_len_std',  'flow_byts_s',  'flow_pkts_s',  'flow_iat_mean',  'flow_iat_std',  \n",
        "           'flow_iat_max',  'flow_iat_min',  'fwd_iat_tot',  'fwd_iat_mean',  'fwd_iat_std',  'fwd_iat_max',  'fwd_iat_min',  'bwd_iat_tot',  \n",
        "           'bwd_iat_mean',  'bwd_iat_std',  'bwd_iat_max',  'bwd_iat_min',  'fwd_psh_flags',  'bwd_psh_flags',  'fwd_urg_flags',  'bwd_urg_flags',  \n",
        "           'fwd_header_len',  'bwd_header_len',  'fwd_pkts_s',  'bwd_pkts_s',  'pkt_len_min',  'pkt_len_max',  'pkt_len_mean',  'pkt_len_std',  \n",
        "           'pkt_len_var',  'fin_flag_cnt',  'syn_flag_cnt',  'rst_flag_cnt',  'psh_flag_cnt',  'ack_flag_cnt',  'urg_flag_cnt',  'cwe_flag_count', \n",
        "           'ece_flag_cnt',  'down_up_ratio',  'pkt_size_avg']  ## 55 Features for impocol "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmETJB6YmzLZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKPvxCURi5jc",
        "outputId": "315a80d0-cb32-4c73-c2b9-62f4c3cc7af2"
      },
      "source": [
        "impocol = implist\n",
        "print (impocol)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dst_port', 'protocol', 'timestamp', 'flow_duration', 'tot_fwd_pkts', 'tot_bwd_pkts', 'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max', 'fwd_pkt_len_min', 'fwd_pkt_len_mean', 'fwd_pkt_len_std', 'bwd_pkt_len_max', 'bwd_pkt_len_min', 'bwd_pkt_len_mean', 'bwd_pkt_len_std', 'flow_byts_s', 'flow_pkts_s', 'flow_iat_mean', 'flow_iat_std', 'flow_iat_max', 'flow_iat_min', 'fwd_iat_tot', 'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max', 'fwd_iat_min', 'bwd_iat_tot', 'bwd_iat_mean', 'bwd_iat_std', 'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_len', 'bwd_header_len', 'fwd_pkts_s', 'bwd_pkts_s', 'pkt_len_min', 'pkt_len_max', 'pkt_len_mean', 'pkt_len_std', 'pkt_len_var', 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt', 'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt', 'cwe_flag_count', 'ece_flag_cnt', 'down_up_ratio', 'pkt_size_avg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU-ylLsEja56"
      },
      "source": [
        "impocol.remove('bwd_urg_flags') # Not in index \n",
        "impocol.remove('dst_port')      # Not in index \n",
        "impocol.remove('timestamp')     # Not in index \n",
        "impocol.remove('bwd_psh_flags') # Not in index "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZz_zxIN8vXq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJhlhE60RCHy"
      },
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2, random_state = 0, stratify = y)\n",
        "\n",
        "# X_train1, X_test1, y_train1, y_test1 = train_test_split(X_test,y_test, train_size = 0.8, test_size = 0.2, random_state = 0, stratify = y_test)\n",
        "# X_train2, X_test2, y_train2, y_test2 = train_test_split(X_test1,y_test1, train_size = 0.8, test_size = 0.2, random_state = 0, stratify = y_test1)\n",
        "# X_train3, X_test3, y_train3, y_test3 = train_test_split(X_test2,y_test2, train_size = 0.8, test_size = 0.2, random_state = 0, stratify = y_test2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6lydQuURDgs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHWvMzvC3AXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf63c9a-5a6d-4af8-eaba-c30c9954a40a"
      },
      "source": [
        "X_new = X_test2[impocol]\n",
        "print(\"Remaining amount of features:\" , {len(X_new.columns)})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Remaining amount of features: {51}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7gSVzyxRP2o"
      },
      "source": [
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X_new,y_test2, train_size = 0.8, test_size = 0.2, random_state = 0, stratify = y_test2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRpdetZoipHq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "5a2c940b-4685-464e-8afd-c3c5d6f380c3"
      },
      "source": [
        "X_train4.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>protocol</th>\n",
              "      <th>flow_duration</th>\n",
              "      <th>tot_fwd_pkts</th>\n",
              "      <th>tot_bwd_pkts</th>\n",
              "      <th>totlen_fwd_pkts</th>\n",
              "      <th>totlen_bwd_pkts</th>\n",
              "      <th>fwd_pkt_len_max</th>\n",
              "      <th>fwd_pkt_len_min</th>\n",
              "      <th>fwd_pkt_len_mean</th>\n",
              "      <th>fwd_pkt_len_std</th>\n",
              "      <th>bwd_pkt_len_max</th>\n",
              "      <th>bwd_pkt_len_min</th>\n",
              "      <th>bwd_pkt_len_mean</th>\n",
              "      <th>bwd_pkt_len_std</th>\n",
              "      <th>flow_byts_s</th>\n",
              "      <th>flow_pkts_s</th>\n",
              "      <th>flow_iat_mean</th>\n",
              "      <th>flow_iat_std</th>\n",
              "      <th>flow_iat_max</th>\n",
              "      <th>flow_iat_min</th>\n",
              "      <th>fwd_iat_tot</th>\n",
              "      <th>fwd_iat_mean</th>\n",
              "      <th>fwd_iat_std</th>\n",
              "      <th>fwd_iat_max</th>\n",
              "      <th>fwd_iat_min</th>\n",
              "      <th>bwd_iat_tot</th>\n",
              "      <th>bwd_iat_mean</th>\n",
              "      <th>bwd_iat_std</th>\n",
              "      <th>bwd_iat_max</th>\n",
              "      <th>bwd_iat_min</th>\n",
              "      <th>fwd_psh_flags</th>\n",
              "      <th>fwd_urg_flags</th>\n",
              "      <th>fwd_header_len</th>\n",
              "      <th>bwd_header_len</th>\n",
              "      <th>fwd_pkts_s</th>\n",
              "      <th>bwd_pkts_s</th>\n",
              "      <th>pkt_len_min</th>\n",
              "      <th>pkt_len_max</th>\n",
              "      <th>pkt_len_mean</th>\n",
              "      <th>pkt_len_std</th>\n",
              "      <th>pkt_len_var</th>\n",
              "      <th>fin_flag_cnt</th>\n",
              "      <th>syn_flag_cnt</th>\n",
              "      <th>rst_flag_cnt</th>\n",
              "      <th>psh_flag_cnt</th>\n",
              "      <th>ack_flag_cnt</th>\n",
              "      <th>urg_flag_cnt</th>\n",
              "      <th>cwe_flag_count</th>\n",
              "      <th>ece_flag_cnt</th>\n",
              "      <th>down_up_ratio</th>\n",
              "      <th>pkt_size_avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10486085</th>\n",
              "      <td>6</td>\n",
              "      <td>2281890.0</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>1052</td>\n",
              "      <td>1429</td>\n",
              "      <td>581</td>\n",
              "      <td>0</td>\n",
              "      <td>131.50000</td>\n",
              "      <td>196.37210</td>\n",
              "      <td>1149</td>\n",
              "      <td>0</td>\n",
              "      <td>204.14285</td>\n",
              "      <td>420.49908</td>\n",
              "      <td>1087.256616</td>\n",
              "      <td>6.573498</td>\n",
              "      <td>162992.14</td>\n",
              "      <td>249620.94</td>\n",
              "      <td>953401.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2281890.0</td>\n",
              "      <td>325984.28</td>\n",
              "      <td>403106.62</td>\n",
              "      <td>1222281.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2062411</td>\n",
              "      <td>343735.16</td>\n",
              "      <td>299566.9</td>\n",
              "      <td>953401</td>\n",
              "      <td>199828</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>152</td>\n",
              "      <td>3.505866</td>\n",
              "      <td>3.067632</td>\n",
              "      <td>0</td>\n",
              "      <td>1149</td>\n",
              "      <td>155.06250</td>\n",
              "      <td>302.89700</td>\n",
              "      <td>91746.59</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>165.40000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5222695</th>\n",
              "      <td>6</td>\n",
              "      <td>9441907.0</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>1148</td>\n",
              "      <td>1581</td>\n",
              "      <td>677</td>\n",
              "      <td>0</td>\n",
              "      <td>104.36364</td>\n",
              "      <td>202.29448</td>\n",
              "      <td>1173</td>\n",
              "      <td>0</td>\n",
              "      <td>225.85715</td>\n",
              "      <td>430.09860</td>\n",
              "      <td>289.030595</td>\n",
              "      <td>1.906395</td>\n",
              "      <td>555406.30</td>\n",
              "      <td>941207.30</td>\n",
              "      <td>3513961.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>9441907.0</td>\n",
              "      <td>944190.70</td>\n",
              "      <td>1139554.20</td>\n",
              "      <td>3513961.0</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>6669606</td>\n",
              "      <td>1111601.00</td>\n",
              "      <td>1482133.4</td>\n",
              "      <td>4081124</td>\n",
              "      <td>261029</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>232</td>\n",
              "      <td>152</td>\n",
              "      <td>1.165019</td>\n",
              "      <td>0.741376</td>\n",
              "      <td>0</td>\n",
              "      <td>1173</td>\n",
              "      <td>143.63158</td>\n",
              "      <td>298.52026</td>\n",
              "      <td>89114.36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>151.61111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          protocol  flow_duration  ...  down_up_ratio  pkt_size_avg\n",
              "10486085         6      2281890.0  ...              0     165.40000\n",
              "5222695          6      9441907.0  ...              0     151.61111\n",
              "\n",
              "[2 rows x 51 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niWT4lgmnYfc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXtR1Qw13diC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZtMlHXER0tO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6nLbynH4wIf"
      },
      "source": [
        "### 5. SMOTE ENN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va2fUjxD4OuI",
        "outputId": "09007c74-f538-4a0c-d4a6-75f7a8c2c979"
      },
      "source": [
        "pd.Series(y_new).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     21576\n",
              "4      1098\n",
              "6       922\n",
              "8       739\n",
              "1       458\n",
              "11      309\n",
              "14      300\n",
              "12      259\n",
              "9       224\n",
              "7        66\n",
              "10       18\n",
              "5         3\n",
              "2         1\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1J27pAb7O1w"
      },
      "source": [
        "# SMOTE + ENN\n",
        "#from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTEENN \n",
        "smenn = SMOTEENN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoUjFgLq4ddk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny-JMXB7PZp5"
      },
      "source": [
        "smote = SMOTEENN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azwPKZi54tkh",
        "outputId": "3ac32051-93d3-4ab2-f1b6-118a2bef7650"
      },
      "source": [
        "#smote = SMOTE(n_jobs = -1,sampling_strategy = {7:15000}) <<\n",
        "smote = SMOTEENN(sampling_strategy = {7:15000}) \n",
        "X_new, y_new = smote.fit_sample(X_new, y_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st8uzFmK4tfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0610ca85-ece0-4ed4-e7eb-620999162a16"
      },
      "source": [
        "#smote = SMOTE(n_jobs = -1,sampling_strategy = {10:15000}) \n",
        "smote = SMOTEENN(sampling_strategy = {10:15000}) \n",
        "X_new, y_new = smote.fit_sample(X_new, y_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHri5Za4Tijb",
        "outputId": "783f43a4-e24c-4c57-8c2b-d208f66142eb"
      },
      "source": [
        "#smote = SMOTE(n_jobs = -1,sampling_strategy = {5:15000})\n",
        "smote = SMOTEENN(sampling_strategy = {9:15000}) \n",
        "X_new, y_new = smote.fit_sample(X_new, y_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrnI5oMAM0Ys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5995798c-78e5-4ec1-e5d4-78c38f249672"
      },
      "source": [
        "#smote = SMOTE(n_jobs = -1,sampling_strategy = {5:15000})\n",
        "smote = SMOTEENN(sampling_strategy = {12:15000}) \n",
        "X_new, y_new = smote.fit_sample(X_new, y_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ncockmTUA78",
        "outputId": "a8c2b9fe-1c56-4c67-efe2-6dc4af94e23e"
      },
      "source": [
        "#smote = SMOTE(n_jobs = -1,sampling_strategy = {5:15000})\n",
        "smote = SMOTEENN(sampling_strategy = {6:15000}) \n",
        "X_new, y_new = smote.fit_sample(X_new, y_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0A4UkquUJxz",
        "outputId": "4648dea6-5e8c-4780-82db-b5e37c101164"
      },
      "source": [
        "#smote = SMOTE(n_jobs = -1,sampling_strategy = {5:15000})\n",
        "smote = SMOTEENN(sampling_strategy = {4:15000}) \n",
        "X_new, y_new = smote.fit_sample(X_new, y_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj46EyKoUOc8",
        "outputId": "4cf15535-777c-4e4d-967a-6cbe9c407ad1"
      },
      "source": [
        "#smote = SMOTE(n_jobs = -1,sampling_strategy = {5:15000})\n",
        "smote = SMOTEENN(sampling_strategy = {1:15000}) \n",
        "X_new, y_new = smote.fit_sample(X_new, y_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yEWHlztUUQs",
        "outputId": "ddab6ea4-9a32-418a-a44a-e2552417cede"
      },
      "source": [
        "#smote = SMOTE(n_jobs = -1,sampling_strategy = {5:15000})\n",
        "smote = SMOTEENN(sampling_strategy = {14:15000}) \n",
        "X_new, y_new = smote.fit_sample(X_new, y_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKYjul5hUc6b",
        "outputId": "e312a0e0-6cf1-49bd-fdfd-4e981289c860"
      },
      "source": [
        "#smote = SMOTE(n_jobs = -1,sampling_strategy = {5:15000})\n",
        "smote = SMOTEENN(sampling_strategy = {8:15000}) \n",
        "X_new, y_new = smote.fit_sample(X_new, y_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2BSvg0IUpRc",
        "outputId": "8298c080-dd6d-4daf-c4c4-3f4cf0876cf3"
      },
      "source": [
        "#smote = SMOTE(n_jobs = -1,sampling_strategy = {5:15000})\n",
        "smote = SMOTEENN(sampling_strategy = {11:15000}) \n",
        "X_new, y_new = smote.fit_sample(X_new, y_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IItoYcqkM0Lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cab3484-fc36-4325-9134-503200022680"
      },
      "source": [
        "pd.Series(y_new).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     20534\n",
              "14    15000\n",
              "10    15000\n",
              "9     15000\n",
              "8     15000\n",
              "4     15000\n",
              "1     15000\n",
              "12    14998\n",
              "11    14972\n",
              "6     14966\n",
              "7     14405\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GyU2ikCR0lm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txKefY_YUYhd"
      },
      "source": [
        "### 6. Pre Model New"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Jvt8R6TWOb"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt_uOJ2VR0dc"
      },
      "source": [
        "rf = RandomForestClassifier(random_state = 0)\n",
        "xg = xgb.XGBClassifier(n_estimators = 10)\n",
        "cat = CatBoostClassifier()\n",
        "light = lgb.LGBMClassifier()\n",
        "\n",
        "def build_model_new (model,X_train4,y_train4, X_test4,y_test4):\n",
        "    print(model)\n",
        "    model.fit(X_train4, y_train4)\n",
        "\n",
        "    model_score = model.score(X_test4,y_test4)\n",
        "    y_predict = model.predict(X_test4)\n",
        "    y_true = y_test4\n",
        "\n",
        "    report_acc (model_score, y_true, y_predict)\n",
        "\n",
        "    model_train=model.predict(X_train4)\n",
        "    model_test=model.predict(X_test4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlsjTRurYWCa"
      },
      "source": [
        "def save_model_new (model,name_model,X_train4,y_train4, X_test4,y_test4):\n",
        "    # Save the model to disk \n",
        "    %cd /content/drive/My Drive/Colab Notebooks/Newi/2021Project3/\n",
        "\n",
        "    filename = name_model + '.sav' ; \n",
        "    pickle.dump(model, open('./result_model/' + filename, 'wb'))\n",
        "    print(\"Done save model in drive: \" , filename)\n",
        "    #---------------------------------------------------------------------------\n",
        "    # Load the model from disk\n",
        "    loaded_model = pickle.load(open('./result_model/' + filename , 'rb'))\n",
        "    result = loaded_model.score(X_test4, y_test4)\n",
        "    print(\"Accuracy Model: \" , result)\n",
        "    #---------------------------------------------------------------------------\n",
        "    train = name_model + 'train'\n",
        "    train = loaded_model.predict(X_train4)\n",
        "    test = loaded_model.predict(X_test4)\n",
        "\n",
        "    return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4u0s8na7Mj6"
      },
      "source": [
        "def report_acc (model_score, y_true, y_predict):\n",
        "  print('Accuracy of Model: ' + str(model_score))\n",
        "  precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
        "  print('Precision of Model: ' + (str(precision)))\n",
        "  print('Recall of Model: ' + (str(recall)))\n",
        "  print('F1-score of Model: ' + (str(fscore)))\n",
        "  print(classification_report(y_true,y_predict))\n",
        "\n",
        "  #cm = confusion_matrix(y_true,y_predict)\n",
        "  #f,ax = plt.subplots(figsize=(8,8))\n",
        "  #sns.heatmap(cm,annot=True,linewidth = 0.5,linecolor=\"blue\",fmt=\".0f\",ax=ax)\n",
        "  #plt.xlabel(\"y_pred\")\n",
        "  #plt.ylabel(\"y_true\")\n",
        "  #plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VZD2kAnVPvp"
      },
      "source": [
        "### 7. Apply Model New"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eqWLZ9BVPUa",
        "outputId": "dba8650b-e4db-4cad-b99a-4c29affb18ed"
      },
      "source": [
        "build_model_new (rf , X_train4 , y_train4 , X_test4 , y_test4)\n",
        "rf_train_new , rf_test_new = save_model_new(rf,'rf_model_new', X_train4 , y_train4 , X_test4 , y_test4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
            "                       warm_start=False)\n",
            "Accuracy of Model: 0.9733184460786201\n",
            "Precision of Model: 0.968698323661959\n",
            "Recall of Model: 0.9733184460786201\n",
            "F1-score of Model: 0.9696937480917386\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99     21576\n",
            "           1       0.92      0.96      0.94       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           4       0.92      0.91      0.91      1098\n",
            "           5       0.75      1.00      0.86         3\n",
            "           6       0.98      0.97      0.97       922\n",
            "           7       0.98      0.98      0.98        66\n",
            "           8       0.99      0.99      0.99       739\n",
            "           9       0.78      0.48      0.59       224\n",
            "          10       1.00      0.94      0.97        18\n",
            "          11       0.70      0.90      0.79       309\n",
            "          12       0.35      0.08      0.13       259\n",
            "          14       1.00      1.00      1.00       300\n",
            "\n",
            "    accuracy                           0.97     25973\n",
            "   macro avg       0.80      0.79      0.78     25973\n",
            "weighted avg       0.97      0.97      0.97     25973\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  rf_model_new.sav\n",
            "Accuracy Model:  0.9733184460786201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYOf1qBIVPMz",
        "outputId": "a98560bd-2baa-46ce-f21d-ad0fd78fd952"
      },
      "source": [
        "build_model_new (xg, X_train4 , y_train4 , X_test4 , y_test4)\n",
        "xg_train_new , xg_test_new = save_model_new(xg,'xg_model_new', X_train4 , y_train4 , X_test4 , y_test4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
            "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
            "              nthread=None, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n",
            "Accuracy of Model: 0.9595734031494244\n",
            "Precision of Model: 0.9517398530579302\n",
            "Recall of Model: 0.9595734031494244\n",
            "F1-score of Model: 0.9520262752108889\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     21576\n",
            "           1       1.00      0.49      0.66       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           4       0.89      0.95      0.92      1098\n",
            "           5       0.75      1.00      0.86         3\n",
            "           6       0.94      0.98      0.96       922\n",
            "           7       1.00      0.14      0.24        66\n",
            "           8       0.98      0.94      0.96       739\n",
            "           9       0.76      0.43      0.55       224\n",
            "          10       1.00      0.67      0.80        18\n",
            "          11       0.71      0.80      0.75       309\n",
            "          12       0.14      0.00      0.01       259\n",
            "          14       1.00      0.98      0.99       300\n",
            "\n",
            "    accuracy                           0.96     25973\n",
            "   macro avg       0.78      0.64      0.67     25973\n",
            "weighted avg       0.95      0.96      0.95     25973\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  xg_model_new.sav\n",
            "Accuracy Model:  0.9595734031494244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2S4XeE1VPEF",
        "outputId": "47bd8053-b90b-4376-d220-b62771f71412"
      },
      "source": [
        "build_model_new (cat, X_train4 , y_train4 , X_test4 , y_test4)\n",
        "cat_train_new , cat_test_new = save_model_new(cat,'cat_model_new', X_train4 , y_train4 , X_test4 , y_test4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<catboost.core.CatBoostClassifier object at 0x7fec82b00350>\n",
            "Learning rate set to 0.100269\n",
            "0:\tlearn: 1.6569563\ttotal: 205ms\tremaining: 3m 24s\n",
            "1:\tlearn: 1.3538572\ttotal: 302ms\tremaining: 2m 30s\n",
            "2:\tlearn: 1.1477707\ttotal: 379ms\tremaining: 2m 6s\n",
            "3:\tlearn: 1.0034998\ttotal: 459ms\tremaining: 1m 54s\n",
            "4:\tlearn: 0.8909845\ttotal: 540ms\tremaining: 1m 47s\n",
            "5:\tlearn: 0.7971506\ttotal: 622ms\tremaining: 1m 43s\n",
            "6:\tlearn: 0.7134586\ttotal: 703ms\tremaining: 1m 39s\n",
            "7:\tlearn: 0.6456923\ttotal: 782ms\tremaining: 1m 36s\n",
            "8:\tlearn: 0.5855636\ttotal: 862ms\tremaining: 1m 34s\n",
            "9:\tlearn: 0.5365903\ttotal: 943ms\tremaining: 1m 33s\n",
            "10:\tlearn: 0.4945138\ttotal: 1.02s\tremaining: 1m 31s\n",
            "11:\tlearn: 0.4567998\ttotal: 1.1s\tremaining: 1m 30s\n",
            "12:\tlearn: 0.4242365\ttotal: 1.18s\tremaining: 1m 29s\n",
            "13:\tlearn: 0.3924315\ttotal: 1.26s\tremaining: 1m 28s\n",
            "14:\tlearn: 0.3687923\ttotal: 1.34s\tremaining: 1m 28s\n",
            "15:\tlearn: 0.3451738\ttotal: 1.42s\tremaining: 1m 27s\n",
            "16:\tlearn: 0.3233541\ttotal: 1.5s\tremaining: 1m 26s\n",
            "17:\tlearn: 0.3060498\ttotal: 1.58s\tremaining: 1m 26s\n",
            "18:\tlearn: 0.2894281\ttotal: 1.66s\tremaining: 1m 25s\n",
            "19:\tlearn: 0.2746597\ttotal: 1.74s\tremaining: 1m 25s\n",
            "20:\tlearn: 0.2611763\ttotal: 1.82s\tremaining: 1m 24s\n",
            "21:\tlearn: 0.2491221\ttotal: 1.9s\tremaining: 1m 24s\n",
            "22:\tlearn: 0.2391636\ttotal: 1.98s\tremaining: 1m 24s\n",
            "23:\tlearn: 0.2314104\ttotal: 2.06s\tremaining: 1m 23s\n",
            "24:\tlearn: 0.2235663\ttotal: 2.14s\tremaining: 1m 23s\n",
            "25:\tlearn: 0.2141991\ttotal: 2.22s\tremaining: 1m 23s\n",
            "26:\tlearn: 0.2062325\ttotal: 2.3s\tremaining: 1m 22s\n",
            "27:\tlearn: 0.1991621\ttotal: 2.38s\tremaining: 1m 22s\n",
            "28:\tlearn: 0.1932214\ttotal: 2.46s\tremaining: 1m 22s\n",
            "29:\tlearn: 0.1881845\ttotal: 2.54s\tremaining: 1m 22s\n",
            "30:\tlearn: 0.1820568\ttotal: 2.62s\tremaining: 1m 21s\n",
            "31:\tlearn: 0.1766746\ttotal: 2.7s\tremaining: 1m 21s\n",
            "32:\tlearn: 0.1715580\ttotal: 2.78s\tremaining: 1m 21s\n",
            "33:\tlearn: 0.1667971\ttotal: 2.86s\tremaining: 1m 21s\n",
            "34:\tlearn: 0.1628175\ttotal: 2.94s\tremaining: 1m 21s\n",
            "35:\tlearn: 0.1588250\ttotal: 3.02s\tremaining: 1m 20s\n",
            "36:\tlearn: 0.1553061\ttotal: 3.1s\tremaining: 1m 20s\n",
            "37:\tlearn: 0.1521757\ttotal: 3.18s\tremaining: 1m 20s\n",
            "38:\tlearn: 0.1484803\ttotal: 3.26s\tremaining: 1m 20s\n",
            "39:\tlearn: 0.1456259\ttotal: 3.34s\tremaining: 1m 20s\n",
            "40:\tlearn: 0.1434866\ttotal: 3.42s\tremaining: 1m 20s\n",
            "41:\tlearn: 0.1412347\ttotal: 3.5s\tremaining: 1m 19s\n",
            "42:\tlearn: 0.1387814\ttotal: 3.58s\tremaining: 1m 19s\n",
            "43:\tlearn: 0.1370542\ttotal: 3.67s\tremaining: 1m 19s\n",
            "44:\tlearn: 0.1350855\ttotal: 3.75s\tremaining: 1m 19s\n",
            "45:\tlearn: 0.1334952\ttotal: 3.83s\tremaining: 1m 19s\n",
            "46:\tlearn: 0.1317635\ttotal: 3.91s\tremaining: 1m 19s\n",
            "47:\tlearn: 0.1295616\ttotal: 3.99s\tremaining: 1m 19s\n",
            "48:\tlearn: 0.1276194\ttotal: 4.07s\tremaining: 1m 18s\n",
            "49:\tlearn: 0.1261854\ttotal: 4.15s\tremaining: 1m 18s\n",
            "50:\tlearn: 0.1249692\ttotal: 4.23s\tremaining: 1m 18s\n",
            "51:\tlearn: 0.1236727\ttotal: 4.31s\tremaining: 1m 18s\n",
            "52:\tlearn: 0.1221300\ttotal: 4.39s\tremaining: 1m 18s\n",
            "53:\tlearn: 0.1211549\ttotal: 4.46s\tremaining: 1m 18s\n",
            "54:\tlearn: 0.1199199\ttotal: 4.54s\tremaining: 1m 18s\n",
            "55:\tlearn: 0.1190289\ttotal: 4.62s\tremaining: 1m 17s\n",
            "56:\tlearn: 0.1180361\ttotal: 4.7s\tremaining: 1m 17s\n",
            "57:\tlearn: 0.1169218\ttotal: 4.79s\tremaining: 1m 17s\n",
            "58:\tlearn: 0.1161450\ttotal: 4.86s\tremaining: 1m 17s\n",
            "59:\tlearn: 0.1149738\ttotal: 4.94s\tremaining: 1m 17s\n",
            "60:\tlearn: 0.1140573\ttotal: 5.03s\tremaining: 1m 17s\n",
            "61:\tlearn: 0.1131488\ttotal: 5.11s\tremaining: 1m 17s\n",
            "62:\tlearn: 0.1125147\ttotal: 5.18s\tremaining: 1m 17s\n",
            "63:\tlearn: 0.1114781\ttotal: 5.26s\tremaining: 1m 16s\n",
            "64:\tlearn: 0.1103844\ttotal: 5.34s\tremaining: 1m 16s\n",
            "65:\tlearn: 0.1097380\ttotal: 5.42s\tremaining: 1m 16s\n",
            "66:\tlearn: 0.1090908\ttotal: 5.5s\tremaining: 1m 16s\n",
            "67:\tlearn: 0.1084342\ttotal: 5.59s\tremaining: 1m 16s\n",
            "68:\tlearn: 0.1079509\ttotal: 5.67s\tremaining: 1m 16s\n",
            "69:\tlearn: 0.1076208\ttotal: 5.75s\tremaining: 1m 16s\n",
            "70:\tlearn: 0.1070684\ttotal: 5.83s\tremaining: 1m 16s\n",
            "71:\tlearn: 0.1065187\ttotal: 5.92s\tremaining: 1m 16s\n",
            "72:\tlearn: 0.1058259\ttotal: 5.99s\tremaining: 1m 16s\n",
            "73:\tlearn: 0.1051866\ttotal: 6.07s\tremaining: 1m 15s\n",
            "74:\tlearn: 0.1046613\ttotal: 6.15s\tremaining: 1m 15s\n",
            "75:\tlearn: 0.1042335\ttotal: 6.23s\tremaining: 1m 15s\n",
            "76:\tlearn: 0.1038574\ttotal: 6.3s\tremaining: 1m 15s\n",
            "77:\tlearn: 0.1034267\ttotal: 6.39s\tremaining: 1m 15s\n",
            "78:\tlearn: 0.1031124\ttotal: 6.47s\tremaining: 1m 15s\n",
            "79:\tlearn: 0.1028212\ttotal: 6.55s\tremaining: 1m 15s\n",
            "80:\tlearn: 0.1024212\ttotal: 6.63s\tremaining: 1m 15s\n",
            "81:\tlearn: 0.1018440\ttotal: 6.71s\tremaining: 1m 15s\n",
            "82:\tlearn: 0.1015619\ttotal: 6.79s\tremaining: 1m 15s\n",
            "83:\tlearn: 0.1013323\ttotal: 6.88s\tremaining: 1m 14s\n",
            "84:\tlearn: 0.1009169\ttotal: 6.96s\tremaining: 1m 14s\n",
            "85:\tlearn: 0.1006503\ttotal: 7.04s\tremaining: 1m 14s\n",
            "86:\tlearn: 0.1004036\ttotal: 7.11s\tremaining: 1m 14s\n",
            "87:\tlearn: 0.1000197\ttotal: 7.19s\tremaining: 1m 14s\n",
            "88:\tlearn: 0.0998634\ttotal: 7.28s\tremaining: 1m 14s\n",
            "89:\tlearn: 0.0995712\ttotal: 7.35s\tremaining: 1m 14s\n",
            "90:\tlearn: 0.0994823\ttotal: 7.43s\tremaining: 1m 14s\n",
            "91:\tlearn: 0.0993288\ttotal: 7.52s\tremaining: 1m 14s\n",
            "92:\tlearn: 0.0990850\ttotal: 7.6s\tremaining: 1m 14s\n",
            "93:\tlearn: 0.0988168\ttotal: 7.67s\tremaining: 1m 13s\n",
            "94:\tlearn: 0.0986099\ttotal: 7.75s\tremaining: 1m 13s\n",
            "95:\tlearn: 0.0984676\ttotal: 7.84s\tremaining: 1m 13s\n",
            "96:\tlearn: 0.0983131\ttotal: 7.92s\tremaining: 1m 13s\n",
            "97:\tlearn: 0.0978039\ttotal: 8s\tremaining: 1m 13s\n",
            "98:\tlearn: 0.0976924\ttotal: 8.07s\tremaining: 1m 13s\n",
            "99:\tlearn: 0.0974526\ttotal: 8.15s\tremaining: 1m 13s\n",
            "100:\tlearn: 0.0973494\ttotal: 8.24s\tremaining: 1m 13s\n",
            "101:\tlearn: 0.0971397\ttotal: 8.32s\tremaining: 1m 13s\n",
            "102:\tlearn: 0.0967751\ttotal: 8.4s\tremaining: 1m 13s\n",
            "103:\tlearn: 0.0966887\ttotal: 8.48s\tremaining: 1m 13s\n",
            "104:\tlearn: 0.0965485\ttotal: 8.56s\tremaining: 1m 12s\n",
            "105:\tlearn: 0.0963884\ttotal: 8.64s\tremaining: 1m 12s\n",
            "106:\tlearn: 0.0962319\ttotal: 8.71s\tremaining: 1m 12s\n",
            "107:\tlearn: 0.0959549\ttotal: 8.8s\tremaining: 1m 12s\n",
            "108:\tlearn: 0.0957997\ttotal: 8.88s\tremaining: 1m 12s\n",
            "109:\tlearn: 0.0956790\ttotal: 8.95s\tremaining: 1m 12s\n",
            "110:\tlearn: 0.0955232\ttotal: 9.04s\tremaining: 1m 12s\n",
            "111:\tlearn: 0.0951023\ttotal: 9.12s\tremaining: 1m 12s\n",
            "112:\tlearn: 0.0949993\ttotal: 9.2s\tremaining: 1m 12s\n",
            "113:\tlearn: 0.0948359\ttotal: 9.29s\tremaining: 1m 12s\n",
            "114:\tlearn: 0.0947152\ttotal: 9.37s\tremaining: 1m 12s\n",
            "115:\tlearn: 0.0943989\ttotal: 9.45s\tremaining: 1m 12s\n",
            "116:\tlearn: 0.0942444\ttotal: 9.53s\tremaining: 1m 11s\n",
            "117:\tlearn: 0.0941102\ttotal: 9.61s\tremaining: 1m 11s\n",
            "118:\tlearn: 0.0940587\ttotal: 9.69s\tremaining: 1m 11s\n",
            "119:\tlearn: 0.0938452\ttotal: 9.77s\tremaining: 1m 11s\n",
            "120:\tlearn: 0.0937111\ttotal: 9.85s\tremaining: 1m 11s\n",
            "121:\tlearn: 0.0935404\ttotal: 9.93s\tremaining: 1m 11s\n",
            "122:\tlearn: 0.0934083\ttotal: 10s\tremaining: 1m 11s\n",
            "123:\tlearn: 0.0932708\ttotal: 10.1s\tremaining: 1m 11s\n",
            "124:\tlearn: 0.0931775\ttotal: 10.2s\tremaining: 1m 11s\n",
            "125:\tlearn: 0.0930686\ttotal: 10.2s\tremaining: 1m 11s\n",
            "126:\tlearn: 0.0929995\ttotal: 10.3s\tremaining: 1m 10s\n",
            "127:\tlearn: 0.0929382\ttotal: 10.4s\tremaining: 1m 10s\n",
            "128:\tlearn: 0.0928476\ttotal: 10.5s\tremaining: 1m 10s\n",
            "129:\tlearn: 0.0927259\ttotal: 10.6s\tremaining: 1m 10s\n",
            "130:\tlearn: 0.0926391\ttotal: 10.6s\tremaining: 1m 10s\n",
            "131:\tlearn: 0.0924233\ttotal: 10.7s\tremaining: 1m 10s\n",
            "132:\tlearn: 0.0922084\ttotal: 10.8s\tremaining: 1m 10s\n",
            "133:\tlearn: 0.0919044\ttotal: 10.9s\tremaining: 1m 10s\n",
            "134:\tlearn: 0.0917926\ttotal: 11s\tremaining: 1m 10s\n",
            "135:\tlearn: 0.0916839\ttotal: 11s\tremaining: 1m 10s\n",
            "136:\tlearn: 0.0916124\ttotal: 11.1s\tremaining: 1m 10s\n",
            "137:\tlearn: 0.0914817\ttotal: 11.2s\tremaining: 1m 9s\n",
            "138:\tlearn: 0.0913331\ttotal: 11.3s\tremaining: 1m 9s\n",
            "139:\tlearn: 0.0911032\ttotal: 11.4s\tremaining: 1m 9s\n",
            "140:\tlearn: 0.0910145\ttotal: 11.4s\tremaining: 1m 9s\n",
            "141:\tlearn: 0.0909765\ttotal: 11.5s\tremaining: 1m 9s\n",
            "142:\tlearn: 0.0908786\ttotal: 11.6s\tremaining: 1m 9s\n",
            "143:\tlearn: 0.0906446\ttotal: 11.7s\tremaining: 1m 9s\n",
            "144:\tlearn: 0.0905723\ttotal: 11.8s\tremaining: 1m 9s\n",
            "145:\tlearn: 0.0904136\ttotal: 11.8s\tremaining: 1m 9s\n",
            "146:\tlearn: 0.0903667\ttotal: 11.9s\tremaining: 1m 9s\n",
            "147:\tlearn: 0.0902591\ttotal: 12s\tremaining: 1m 9s\n",
            "148:\tlearn: 0.0902185\ttotal: 12.1s\tremaining: 1m 8s\n",
            "149:\tlearn: 0.0901351\ttotal: 12.2s\tremaining: 1m 8s\n",
            "150:\tlearn: 0.0899359\ttotal: 12.2s\tremaining: 1m 8s\n",
            "151:\tlearn: 0.0898967\ttotal: 12.3s\tremaining: 1m 8s\n",
            "152:\tlearn: 0.0898320\ttotal: 12.4s\tremaining: 1m 8s\n",
            "153:\tlearn: 0.0897958\ttotal: 12.5s\tremaining: 1m 8s\n",
            "154:\tlearn: 0.0895845\ttotal: 12.6s\tremaining: 1m 8s\n",
            "155:\tlearn: 0.0895628\ttotal: 12.6s\tremaining: 1m 8s\n",
            "156:\tlearn: 0.0894945\ttotal: 12.7s\tremaining: 1m 8s\n",
            "157:\tlearn: 0.0893659\ttotal: 12.8s\tremaining: 1m 8s\n",
            "158:\tlearn: 0.0892583\ttotal: 12.9s\tremaining: 1m 8s\n",
            "159:\tlearn: 0.0891823\ttotal: 13s\tremaining: 1m 8s\n",
            "160:\tlearn: 0.0891239\ttotal: 13s\tremaining: 1m 7s\n",
            "161:\tlearn: 0.0890561\ttotal: 13.1s\tremaining: 1m 7s\n",
            "162:\tlearn: 0.0889894\ttotal: 13.2s\tremaining: 1m 7s\n",
            "163:\tlearn: 0.0889011\ttotal: 13.3s\tremaining: 1m 7s\n",
            "164:\tlearn: 0.0888126\ttotal: 13.4s\tremaining: 1m 7s\n",
            "165:\tlearn: 0.0887061\ttotal: 13.4s\tremaining: 1m 7s\n",
            "166:\tlearn: 0.0886552\ttotal: 13.5s\tremaining: 1m 7s\n",
            "167:\tlearn: 0.0885916\ttotal: 13.6s\tremaining: 1m 7s\n",
            "168:\tlearn: 0.0884519\ttotal: 13.7s\tremaining: 1m 7s\n",
            "169:\tlearn: 0.0884005\ttotal: 13.8s\tremaining: 1m 7s\n",
            "170:\tlearn: 0.0883248\ttotal: 13.8s\tremaining: 1m 7s\n",
            "171:\tlearn: 0.0882371\ttotal: 13.9s\tremaining: 1m 6s\n",
            "172:\tlearn: 0.0881472\ttotal: 14s\tremaining: 1m 6s\n",
            "173:\tlearn: 0.0881136\ttotal: 14.1s\tremaining: 1m 6s\n",
            "174:\tlearn: 0.0880667\ttotal: 14.2s\tremaining: 1m 6s\n",
            "175:\tlearn: 0.0879871\ttotal: 14.2s\tremaining: 1m 6s\n",
            "176:\tlearn: 0.0878342\ttotal: 14.3s\tremaining: 1m 6s\n",
            "177:\tlearn: 0.0877583\ttotal: 14.4s\tremaining: 1m 6s\n",
            "178:\tlearn: 0.0877214\ttotal: 14.5s\tremaining: 1m 6s\n",
            "179:\tlearn: 0.0875709\ttotal: 14.6s\tremaining: 1m 6s\n",
            "180:\tlearn: 0.0875114\ttotal: 14.6s\tremaining: 1m 6s\n",
            "181:\tlearn: 0.0875003\ttotal: 14.7s\tremaining: 1m 6s\n",
            "182:\tlearn: 0.0874524\ttotal: 14.8s\tremaining: 1m 6s\n",
            "183:\tlearn: 0.0874182\ttotal: 14.9s\tremaining: 1m 5s\n",
            "184:\tlearn: 0.0873511\ttotal: 15s\tremaining: 1m 5s\n",
            "185:\tlearn: 0.0873236\ttotal: 15s\tremaining: 1m 5s\n",
            "186:\tlearn: 0.0872496\ttotal: 15.1s\tremaining: 1m 5s\n",
            "187:\tlearn: 0.0872042\ttotal: 15.2s\tremaining: 1m 5s\n",
            "188:\tlearn: 0.0871451\ttotal: 15.3s\tremaining: 1m 5s\n",
            "189:\tlearn: 0.0870465\ttotal: 15.4s\tremaining: 1m 5s\n",
            "190:\tlearn: 0.0870258\ttotal: 15.4s\tremaining: 1m 5s\n",
            "191:\tlearn: 0.0869475\ttotal: 15.5s\tremaining: 1m 5s\n",
            "192:\tlearn: 0.0869255\ttotal: 15.6s\tremaining: 1m 5s\n",
            "193:\tlearn: 0.0869082\ttotal: 15.7s\tremaining: 1m 5s\n",
            "194:\tlearn: 0.0868722\ttotal: 15.8s\tremaining: 1m 5s\n",
            "195:\tlearn: 0.0867199\ttotal: 15.9s\tremaining: 1m 5s\n",
            "196:\tlearn: 0.0866096\ttotal: 15.9s\tremaining: 1m 4s\n",
            "197:\tlearn: 0.0864315\ttotal: 16s\tremaining: 1m 4s\n",
            "198:\tlearn: 0.0863652\ttotal: 16.1s\tremaining: 1m 4s\n",
            "199:\tlearn: 0.0862904\ttotal: 16.2s\tremaining: 1m 4s\n",
            "200:\tlearn: 0.0862732\ttotal: 16.3s\tremaining: 1m 4s\n",
            "201:\tlearn: 0.0862268\ttotal: 16.3s\tremaining: 1m 4s\n",
            "202:\tlearn: 0.0862083\ttotal: 16.4s\tremaining: 1m 4s\n",
            "203:\tlearn: 0.0861072\ttotal: 16.5s\tremaining: 1m 4s\n",
            "204:\tlearn: 0.0860615\ttotal: 16.6s\tremaining: 1m 4s\n",
            "205:\tlearn: 0.0860166\ttotal: 16.7s\tremaining: 1m 4s\n",
            "206:\tlearn: 0.0859584\ttotal: 16.8s\tremaining: 1m 4s\n",
            "207:\tlearn: 0.0859309\ttotal: 16.8s\tremaining: 1m 4s\n",
            "208:\tlearn: 0.0858318\ttotal: 16.9s\tremaining: 1m 4s\n",
            "209:\tlearn: 0.0857974\ttotal: 17s\tremaining: 1m 3s\n",
            "210:\tlearn: 0.0857369\ttotal: 17.1s\tremaining: 1m 3s\n",
            "211:\tlearn: 0.0856995\ttotal: 17.2s\tremaining: 1m 3s\n",
            "212:\tlearn: 0.0856484\ttotal: 17.2s\tremaining: 1m 3s\n",
            "213:\tlearn: 0.0855943\ttotal: 17.3s\tremaining: 1m 3s\n",
            "214:\tlearn: 0.0855528\ttotal: 17.4s\tremaining: 1m 3s\n",
            "215:\tlearn: 0.0855253\ttotal: 17.5s\tremaining: 1m 3s\n",
            "216:\tlearn: 0.0854917\ttotal: 17.6s\tremaining: 1m 3s\n",
            "217:\tlearn: 0.0854357\ttotal: 17.6s\tremaining: 1m 3s\n",
            "218:\tlearn: 0.0854161\ttotal: 17.7s\tremaining: 1m 3s\n",
            "219:\tlearn: 0.0853745\ttotal: 17.8s\tremaining: 1m 3s\n",
            "220:\tlearn: 0.0853615\ttotal: 17.9s\tremaining: 1m 3s\n",
            "221:\tlearn: 0.0853171\ttotal: 18s\tremaining: 1m 2s\n",
            "222:\tlearn: 0.0852859\ttotal: 18.1s\tremaining: 1m 2s\n",
            "223:\tlearn: 0.0852350\ttotal: 18.1s\tremaining: 1m 2s\n",
            "224:\tlearn: 0.0852029\ttotal: 18.2s\tremaining: 1m 2s\n",
            "225:\tlearn: 0.0851491\ttotal: 18.3s\tremaining: 1m 2s\n",
            "226:\tlearn: 0.0851362\ttotal: 18.4s\tremaining: 1m 2s\n",
            "227:\tlearn: 0.0850330\ttotal: 18.4s\tremaining: 1m 2s\n",
            "228:\tlearn: 0.0850083\ttotal: 18.5s\tremaining: 1m 2s\n",
            "229:\tlearn: 0.0849635\ttotal: 18.6s\tremaining: 1m 2s\n",
            "230:\tlearn: 0.0848536\ttotal: 18.7s\tremaining: 1m 2s\n",
            "231:\tlearn: 0.0848009\ttotal: 18.8s\tremaining: 1m 2s\n",
            "232:\tlearn: 0.0847549\ttotal: 18.9s\tremaining: 1m 2s\n",
            "233:\tlearn: 0.0847169\ttotal: 18.9s\tremaining: 1m 2s\n",
            "234:\tlearn: 0.0846930\ttotal: 19s\tremaining: 1m 1s\n",
            "235:\tlearn: 0.0846609\ttotal: 19.1s\tremaining: 1m 1s\n",
            "236:\tlearn: 0.0846264\ttotal: 19.2s\tremaining: 1m 1s\n",
            "237:\tlearn: 0.0845819\ttotal: 19.3s\tremaining: 1m 1s\n",
            "238:\tlearn: 0.0845584\ttotal: 19.3s\tremaining: 1m 1s\n",
            "239:\tlearn: 0.0845234\ttotal: 19.4s\tremaining: 1m 1s\n",
            "240:\tlearn: 0.0843934\ttotal: 19.5s\tremaining: 1m 1s\n",
            "241:\tlearn: 0.0843533\ttotal: 19.6s\tremaining: 1m 1s\n",
            "242:\tlearn: 0.0842709\ttotal: 19.7s\tremaining: 1m 1s\n",
            "243:\tlearn: 0.0842235\ttotal: 19.7s\tremaining: 1m 1s\n",
            "244:\tlearn: 0.0841645\ttotal: 19.8s\tremaining: 1m 1s\n",
            "245:\tlearn: 0.0841234\ttotal: 19.9s\tremaining: 1m 1s\n",
            "246:\tlearn: 0.0840829\ttotal: 20s\tremaining: 1m\n",
            "247:\tlearn: 0.0840592\ttotal: 20.1s\tremaining: 1m\n",
            "248:\tlearn: 0.0840256\ttotal: 20.2s\tremaining: 1m\n",
            "249:\tlearn: 0.0839507\ttotal: 20.2s\tremaining: 1m\n",
            "250:\tlearn: 0.0838994\ttotal: 20.3s\tremaining: 1m\n",
            "251:\tlearn: 0.0838765\ttotal: 20.4s\tremaining: 1m\n",
            "252:\tlearn: 0.0838524\ttotal: 20.5s\tremaining: 1m\n",
            "253:\tlearn: 0.0838293\ttotal: 20.6s\tremaining: 1m\n",
            "254:\tlearn: 0.0838089\ttotal: 20.6s\tremaining: 1m\n",
            "255:\tlearn: 0.0837820\ttotal: 20.7s\tremaining: 1m\n",
            "256:\tlearn: 0.0837671\ttotal: 20.8s\tremaining: 1m\n",
            "257:\tlearn: 0.0837486\ttotal: 20.9s\tremaining: 1m\n",
            "258:\tlearn: 0.0836987\ttotal: 21s\tremaining: 60s\n",
            "259:\tlearn: 0.0836760\ttotal: 21s\tremaining: 59.9s\n",
            "260:\tlearn: 0.0836380\ttotal: 21.1s\tremaining: 59.8s\n",
            "261:\tlearn: 0.0836126\ttotal: 21.2s\tremaining: 59.7s\n",
            "262:\tlearn: 0.0836031\ttotal: 21.3s\tremaining: 59.7s\n",
            "263:\tlearn: 0.0835617\ttotal: 21.4s\tremaining: 59.6s\n",
            "264:\tlearn: 0.0835463\ttotal: 21.4s\tremaining: 59.5s\n",
            "265:\tlearn: 0.0834970\ttotal: 21.5s\tremaining: 59.4s\n",
            "266:\tlearn: 0.0834889\ttotal: 21.6s\tremaining: 59.3s\n",
            "267:\tlearn: 0.0834494\ttotal: 21.7s\tremaining: 59.2s\n",
            "268:\tlearn: 0.0834310\ttotal: 21.8s\tremaining: 59.2s\n",
            "269:\tlearn: 0.0834176\ttotal: 21.8s\tremaining: 59.1s\n",
            "270:\tlearn: 0.0833864\ttotal: 21.9s\tremaining: 59s\n",
            "271:\tlearn: 0.0833442\ttotal: 22s\tremaining: 58.9s\n",
            "272:\tlearn: 0.0833060\ttotal: 22.1s\tremaining: 58.8s\n",
            "273:\tlearn: 0.0832832\ttotal: 22.2s\tremaining: 58.7s\n",
            "274:\tlearn: 0.0832783\ttotal: 22.2s\tremaining: 58.6s\n",
            "275:\tlearn: 0.0832413\ttotal: 22.3s\tremaining: 58.6s\n",
            "276:\tlearn: 0.0832127\ttotal: 22.4s\tremaining: 58.5s\n",
            "277:\tlearn: 0.0831966\ttotal: 22.5s\tremaining: 58.4s\n",
            "278:\tlearn: 0.0831716\ttotal: 22.6s\tremaining: 58.3s\n",
            "279:\tlearn: 0.0831497\ttotal: 22.6s\tremaining: 58.2s\n",
            "280:\tlearn: 0.0831218\ttotal: 22.7s\tremaining: 58.1s\n",
            "281:\tlearn: 0.0830853\ttotal: 22.8s\tremaining: 58.1s\n",
            "282:\tlearn: 0.0830655\ttotal: 22.9s\tremaining: 58s\n",
            "283:\tlearn: 0.0830409\ttotal: 23s\tremaining: 57.9s\n",
            "284:\tlearn: 0.0830112\ttotal: 23s\tremaining: 57.8s\n",
            "285:\tlearn: 0.0829951\ttotal: 23.1s\tremaining: 57.7s\n",
            "286:\tlearn: 0.0829497\ttotal: 23.2s\tremaining: 57.7s\n",
            "287:\tlearn: 0.0829237\ttotal: 23.3s\tremaining: 57.6s\n",
            "288:\tlearn: 0.0829110\ttotal: 23.4s\tremaining: 57.5s\n",
            "289:\tlearn: 0.0828779\ttotal: 23.4s\tremaining: 57.4s\n",
            "290:\tlearn: 0.0828526\ttotal: 23.5s\tremaining: 57.3s\n",
            "291:\tlearn: 0.0828220\ttotal: 23.6s\tremaining: 57.2s\n",
            "292:\tlearn: 0.0828002\ttotal: 23.7s\tremaining: 57.2s\n",
            "293:\tlearn: 0.0827747\ttotal: 23.8s\tremaining: 57.1s\n",
            "294:\tlearn: 0.0827354\ttotal: 23.8s\tremaining: 57s\n",
            "295:\tlearn: 0.0827063\ttotal: 23.9s\tremaining: 56.9s\n",
            "296:\tlearn: 0.0826861\ttotal: 24s\tremaining: 56.8s\n",
            "297:\tlearn: 0.0826264\ttotal: 24.1s\tremaining: 56.8s\n",
            "298:\tlearn: 0.0826088\ttotal: 24.2s\tremaining: 56.7s\n",
            "299:\tlearn: 0.0825991\ttotal: 24.3s\tremaining: 56.6s\n",
            "300:\tlearn: 0.0825606\ttotal: 24.3s\tremaining: 56.5s\n",
            "301:\tlearn: 0.0825492\ttotal: 24.4s\tremaining: 56.4s\n",
            "302:\tlearn: 0.0825302\ttotal: 24.5s\tremaining: 56.4s\n",
            "303:\tlearn: 0.0824632\ttotal: 24.6s\tremaining: 56.3s\n",
            "304:\tlearn: 0.0824407\ttotal: 24.7s\tremaining: 56.2s\n",
            "305:\tlearn: 0.0824101\ttotal: 24.7s\tremaining: 56.1s\n",
            "306:\tlearn: 0.0823792\ttotal: 24.8s\tremaining: 56s\n",
            "307:\tlearn: 0.0823661\ttotal: 24.9s\tremaining: 55.9s\n",
            "308:\tlearn: 0.0823085\ttotal: 25s\tremaining: 55.9s\n",
            "309:\tlearn: 0.0822796\ttotal: 25.1s\tremaining: 55.8s\n",
            "310:\tlearn: 0.0822557\ttotal: 25.1s\tremaining: 55.7s\n",
            "311:\tlearn: 0.0822280\ttotal: 25.2s\tremaining: 55.6s\n",
            "312:\tlearn: 0.0821648\ttotal: 25.3s\tremaining: 55.5s\n",
            "313:\tlearn: 0.0821516\ttotal: 25.4s\tremaining: 55.5s\n",
            "314:\tlearn: 0.0821344\ttotal: 25.5s\tremaining: 55.4s\n",
            "315:\tlearn: 0.0821176\ttotal: 25.5s\tremaining: 55.3s\n",
            "316:\tlearn: 0.0821028\ttotal: 25.6s\tremaining: 55.2s\n",
            "317:\tlearn: 0.0820836\ttotal: 25.7s\tremaining: 55.1s\n",
            "318:\tlearn: 0.0820732\ttotal: 25.8s\tremaining: 55.1s\n",
            "319:\tlearn: 0.0820384\ttotal: 25.9s\tremaining: 55s\n",
            "320:\tlearn: 0.0819556\ttotal: 25.9s\tremaining: 54.9s\n",
            "321:\tlearn: 0.0818741\ttotal: 26s\tremaining: 54.8s\n",
            "322:\tlearn: 0.0818230\ttotal: 26.1s\tremaining: 54.7s\n",
            "323:\tlearn: 0.0818051\ttotal: 26.2s\tremaining: 54.6s\n",
            "324:\tlearn: 0.0817364\ttotal: 26.3s\tremaining: 54.5s\n",
            "325:\tlearn: 0.0817050\ttotal: 26.3s\tremaining: 54.5s\n",
            "326:\tlearn: 0.0816833\ttotal: 26.4s\tremaining: 54.4s\n",
            "327:\tlearn: 0.0816633\ttotal: 26.5s\tremaining: 54.3s\n",
            "328:\tlearn: 0.0816411\ttotal: 26.6s\tremaining: 54.2s\n",
            "329:\tlearn: 0.0816131\ttotal: 26.7s\tremaining: 54.1s\n",
            "330:\tlearn: 0.0815961\ttotal: 26.7s\tremaining: 54.1s\n",
            "331:\tlearn: 0.0815547\ttotal: 26.8s\tremaining: 54s\n",
            "332:\tlearn: 0.0815239\ttotal: 26.9s\tremaining: 53.9s\n",
            "333:\tlearn: 0.0814509\ttotal: 27s\tremaining: 53.8s\n",
            "334:\tlearn: 0.0814252\ttotal: 27.1s\tremaining: 53.7s\n",
            "335:\tlearn: 0.0814076\ttotal: 27.1s\tremaining: 53.6s\n",
            "336:\tlearn: 0.0813778\ttotal: 27.2s\tremaining: 53.6s\n",
            "337:\tlearn: 0.0813640\ttotal: 27.3s\tremaining: 53.5s\n",
            "338:\tlearn: 0.0813340\ttotal: 27.4s\tremaining: 53.4s\n",
            "339:\tlearn: 0.0813186\ttotal: 27.5s\tremaining: 53.3s\n",
            "340:\tlearn: 0.0813059\ttotal: 27.5s\tremaining: 53.2s\n",
            "341:\tlearn: 0.0812925\ttotal: 27.6s\tremaining: 53.2s\n",
            "342:\tlearn: 0.0812763\ttotal: 27.7s\tremaining: 53.1s\n",
            "343:\tlearn: 0.0812631\ttotal: 27.8s\tremaining: 53s\n",
            "344:\tlearn: 0.0812412\ttotal: 27.9s\tremaining: 52.9s\n",
            "345:\tlearn: 0.0812267\ttotal: 27.9s\tremaining: 52.8s\n",
            "346:\tlearn: 0.0811965\ttotal: 28s\tremaining: 52.7s\n",
            "347:\tlearn: 0.0811792\ttotal: 28.1s\tremaining: 52.6s\n",
            "348:\tlearn: 0.0811634\ttotal: 28.2s\tremaining: 52.6s\n",
            "349:\tlearn: 0.0811469\ttotal: 28.3s\tremaining: 52.5s\n",
            "350:\tlearn: 0.0811256\ttotal: 28.3s\tremaining: 52.4s\n",
            "351:\tlearn: 0.0811136\ttotal: 28.4s\tremaining: 52.3s\n",
            "352:\tlearn: 0.0810982\ttotal: 28.5s\tremaining: 52.2s\n",
            "353:\tlearn: 0.0810774\ttotal: 28.6s\tremaining: 52.1s\n",
            "354:\tlearn: 0.0810434\ttotal: 28.7s\tremaining: 52.1s\n",
            "355:\tlearn: 0.0810186\ttotal: 28.7s\tremaining: 52s\n",
            "356:\tlearn: 0.0809877\ttotal: 28.8s\tremaining: 51.9s\n",
            "357:\tlearn: 0.0809653\ttotal: 28.9s\tremaining: 51.8s\n",
            "358:\tlearn: 0.0809341\ttotal: 29s\tremaining: 51.7s\n",
            "359:\tlearn: 0.0809292\ttotal: 29s\tremaining: 51.6s\n",
            "360:\tlearn: 0.0809127\ttotal: 29.1s\tremaining: 51.6s\n",
            "361:\tlearn: 0.0808925\ttotal: 29.2s\tremaining: 51.5s\n",
            "362:\tlearn: 0.0808723\ttotal: 29.3s\tremaining: 51.4s\n",
            "363:\tlearn: 0.0808483\ttotal: 29.4s\tremaining: 51.3s\n",
            "364:\tlearn: 0.0808387\ttotal: 29.4s\tremaining: 51.2s\n",
            "365:\tlearn: 0.0808298\ttotal: 29.5s\tremaining: 51.1s\n",
            "366:\tlearn: 0.0807884\ttotal: 29.6s\tremaining: 51.1s\n",
            "367:\tlearn: 0.0807705\ttotal: 29.7s\tremaining: 51s\n",
            "368:\tlearn: 0.0807621\ttotal: 29.8s\tremaining: 50.9s\n",
            "369:\tlearn: 0.0807344\ttotal: 29.8s\tremaining: 50.8s\n",
            "370:\tlearn: 0.0807213\ttotal: 29.9s\tremaining: 50.7s\n",
            "371:\tlearn: 0.0807051\ttotal: 30s\tremaining: 50.7s\n",
            "372:\tlearn: 0.0806960\ttotal: 30.1s\tremaining: 50.6s\n",
            "373:\tlearn: 0.0806849\ttotal: 30.2s\tremaining: 50.5s\n",
            "374:\tlearn: 0.0806603\ttotal: 30.2s\tremaining: 50.4s\n",
            "375:\tlearn: 0.0806508\ttotal: 30.3s\tremaining: 50.3s\n",
            "376:\tlearn: 0.0806385\ttotal: 30.4s\tremaining: 50.2s\n",
            "377:\tlearn: 0.0806309\ttotal: 30.5s\tremaining: 50.2s\n",
            "378:\tlearn: 0.0806239\ttotal: 30.6s\tremaining: 50.1s\n",
            "379:\tlearn: 0.0805987\ttotal: 30.6s\tremaining: 50s\n",
            "380:\tlearn: 0.0805824\ttotal: 30.7s\tremaining: 49.9s\n",
            "381:\tlearn: 0.0805252\ttotal: 30.8s\tremaining: 49.8s\n",
            "382:\tlearn: 0.0805116\ttotal: 30.9s\tremaining: 49.8s\n",
            "383:\tlearn: 0.0804983\ttotal: 31s\tremaining: 49.7s\n",
            "384:\tlearn: 0.0804452\ttotal: 31s\tremaining: 49.6s\n",
            "385:\tlearn: 0.0804374\ttotal: 31.1s\tremaining: 49.5s\n",
            "386:\tlearn: 0.0804208\ttotal: 31.2s\tremaining: 49.4s\n",
            "387:\tlearn: 0.0803742\ttotal: 31.3s\tremaining: 49.4s\n",
            "388:\tlearn: 0.0803105\ttotal: 31.4s\tremaining: 49.3s\n",
            "389:\tlearn: 0.0802879\ttotal: 31.4s\tremaining: 49.2s\n",
            "390:\tlearn: 0.0802690\ttotal: 31.5s\tremaining: 49.1s\n",
            "391:\tlearn: 0.0802454\ttotal: 31.6s\tremaining: 49s\n",
            "392:\tlearn: 0.0802217\ttotal: 31.7s\tremaining: 48.9s\n",
            "393:\tlearn: 0.0801582\ttotal: 31.8s\tremaining: 48.9s\n",
            "394:\tlearn: 0.0801328\ttotal: 31.8s\tremaining: 48.8s\n",
            "395:\tlearn: 0.0801175\ttotal: 31.9s\tremaining: 48.7s\n",
            "396:\tlearn: 0.0801075\ttotal: 32s\tremaining: 48.6s\n",
            "397:\tlearn: 0.0800840\ttotal: 32.1s\tremaining: 48.5s\n",
            "398:\tlearn: 0.0800716\ttotal: 32.2s\tremaining: 48.5s\n",
            "399:\tlearn: 0.0800566\ttotal: 32.3s\tremaining: 48.4s\n",
            "400:\tlearn: 0.0800168\ttotal: 32.3s\tremaining: 48.3s\n",
            "401:\tlearn: 0.0799747\ttotal: 32.4s\tremaining: 48.2s\n",
            "402:\tlearn: 0.0799213\ttotal: 32.5s\tremaining: 48.1s\n",
            "403:\tlearn: 0.0799183\ttotal: 32.6s\tremaining: 48.1s\n",
            "404:\tlearn: 0.0799038\ttotal: 32.7s\tremaining: 48s\n",
            "405:\tlearn: 0.0798794\ttotal: 32.7s\tremaining: 47.9s\n",
            "406:\tlearn: 0.0798672\ttotal: 32.8s\tremaining: 47.8s\n",
            "407:\tlearn: 0.0798483\ttotal: 32.9s\tremaining: 47.7s\n",
            "408:\tlearn: 0.0798377\ttotal: 33s\tremaining: 47.7s\n",
            "409:\tlearn: 0.0798182\ttotal: 33.1s\tremaining: 47.6s\n",
            "410:\tlearn: 0.0798148\ttotal: 33.1s\tremaining: 47.5s\n",
            "411:\tlearn: 0.0797695\ttotal: 33.2s\tremaining: 47.4s\n",
            "412:\tlearn: 0.0797545\ttotal: 33.3s\tremaining: 47.3s\n",
            "413:\tlearn: 0.0797439\ttotal: 33.4s\tremaining: 47.3s\n",
            "414:\tlearn: 0.0797024\ttotal: 33.5s\tremaining: 47.2s\n",
            "415:\tlearn: 0.0796938\ttotal: 33.5s\tremaining: 47.1s\n",
            "416:\tlearn: 0.0796796\ttotal: 33.6s\tremaining: 47s\n",
            "417:\tlearn: 0.0796692\ttotal: 33.7s\tremaining: 46.9s\n",
            "418:\tlearn: 0.0796622\ttotal: 33.8s\tremaining: 46.9s\n",
            "419:\tlearn: 0.0796483\ttotal: 33.9s\tremaining: 46.8s\n",
            "420:\tlearn: 0.0796297\ttotal: 34s\tremaining: 46.7s\n",
            "421:\tlearn: 0.0796145\ttotal: 34s\tremaining: 46.6s\n",
            "422:\tlearn: 0.0796084\ttotal: 34.1s\tremaining: 46.5s\n",
            "423:\tlearn: 0.0795994\ttotal: 34.2s\tremaining: 46.5s\n",
            "424:\tlearn: 0.0795821\ttotal: 34.3s\tremaining: 46.4s\n",
            "425:\tlearn: 0.0795408\ttotal: 34.4s\tremaining: 46.3s\n",
            "426:\tlearn: 0.0795300\ttotal: 34.4s\tremaining: 46.2s\n",
            "427:\tlearn: 0.0795078\ttotal: 34.5s\tremaining: 46.1s\n",
            "428:\tlearn: 0.0795006\ttotal: 34.6s\tremaining: 46.1s\n",
            "429:\tlearn: 0.0794847\ttotal: 34.7s\tremaining: 46s\n",
            "430:\tlearn: 0.0794670\ttotal: 34.8s\tremaining: 45.9s\n",
            "431:\tlearn: 0.0794594\ttotal: 34.8s\tremaining: 45.8s\n",
            "432:\tlearn: 0.0794503\ttotal: 34.9s\tremaining: 45.7s\n",
            "433:\tlearn: 0.0794434\ttotal: 35s\tremaining: 45.6s\n",
            "434:\tlearn: 0.0794259\ttotal: 35.1s\tremaining: 45.6s\n",
            "435:\tlearn: 0.0794108\ttotal: 35.2s\tremaining: 45.5s\n",
            "436:\tlearn: 0.0794005\ttotal: 35.2s\tremaining: 45.4s\n",
            "437:\tlearn: 0.0793901\ttotal: 35.3s\tremaining: 45.3s\n",
            "438:\tlearn: 0.0793642\ttotal: 35.4s\tremaining: 45.2s\n",
            "439:\tlearn: 0.0793551\ttotal: 35.5s\tremaining: 45.1s\n",
            "440:\tlearn: 0.0793243\ttotal: 35.6s\tremaining: 45.1s\n",
            "441:\tlearn: 0.0793168\ttotal: 35.6s\tremaining: 45s\n",
            "442:\tlearn: 0.0792944\ttotal: 35.7s\tremaining: 44.9s\n",
            "443:\tlearn: 0.0792873\ttotal: 35.8s\tremaining: 44.8s\n",
            "444:\tlearn: 0.0792425\ttotal: 35.9s\tremaining: 44.7s\n",
            "445:\tlearn: 0.0792214\ttotal: 35.9s\tremaining: 44.7s\n",
            "446:\tlearn: 0.0792008\ttotal: 36s\tremaining: 44.6s\n",
            "447:\tlearn: 0.0791969\ttotal: 36.1s\tremaining: 44.5s\n",
            "448:\tlearn: 0.0791848\ttotal: 36.2s\tremaining: 44.4s\n",
            "449:\tlearn: 0.0791509\ttotal: 36.3s\tremaining: 44.3s\n",
            "450:\tlearn: 0.0791438\ttotal: 36.3s\tremaining: 44.2s\n",
            "451:\tlearn: 0.0791220\ttotal: 36.4s\tremaining: 44.2s\n",
            "452:\tlearn: 0.0791138\ttotal: 36.5s\tremaining: 44.1s\n",
            "453:\tlearn: 0.0790972\ttotal: 36.6s\tremaining: 44s\n",
            "454:\tlearn: 0.0790919\ttotal: 36.7s\tremaining: 43.9s\n",
            "455:\tlearn: 0.0790852\ttotal: 36.7s\tremaining: 43.8s\n",
            "456:\tlearn: 0.0790779\ttotal: 36.8s\tremaining: 43.8s\n",
            "457:\tlearn: 0.0790700\ttotal: 36.9s\tremaining: 43.7s\n",
            "458:\tlearn: 0.0790641\ttotal: 37s\tremaining: 43.6s\n",
            "459:\tlearn: 0.0790543\ttotal: 37.1s\tremaining: 43.5s\n",
            "460:\tlearn: 0.0790476\ttotal: 37.1s\tremaining: 43.4s\n",
            "461:\tlearn: 0.0790290\ttotal: 37.2s\tremaining: 43.3s\n",
            "462:\tlearn: 0.0790202\ttotal: 37.3s\tremaining: 43.3s\n",
            "463:\tlearn: 0.0790046\ttotal: 37.4s\tremaining: 43.2s\n",
            "464:\tlearn: 0.0789943\ttotal: 37.5s\tremaining: 43.1s\n",
            "465:\tlearn: 0.0789640\ttotal: 37.6s\tremaining: 43s\n",
            "466:\tlearn: 0.0789433\ttotal: 37.6s\tremaining: 43s\n",
            "467:\tlearn: 0.0789393\ttotal: 37.7s\tremaining: 42.9s\n",
            "468:\tlearn: 0.0789265\ttotal: 37.8s\tremaining: 42.8s\n",
            "469:\tlearn: 0.0789186\ttotal: 37.9s\tremaining: 42.7s\n",
            "470:\tlearn: 0.0788913\ttotal: 37.9s\tremaining: 42.6s\n",
            "471:\tlearn: 0.0788878\ttotal: 38s\tremaining: 42.5s\n",
            "472:\tlearn: 0.0788401\ttotal: 38.1s\tremaining: 42.5s\n",
            "473:\tlearn: 0.0788107\ttotal: 38.2s\tremaining: 42.4s\n",
            "474:\tlearn: 0.0787936\ttotal: 38.3s\tremaining: 42.3s\n",
            "475:\tlearn: 0.0787883\ttotal: 38.3s\tremaining: 42.2s\n",
            "476:\tlearn: 0.0787818\ttotal: 38.4s\tremaining: 42.1s\n",
            "477:\tlearn: 0.0787778\ttotal: 38.5s\tremaining: 42s\n",
            "478:\tlearn: 0.0787645\ttotal: 38.6s\tremaining: 42s\n",
            "479:\tlearn: 0.0787494\ttotal: 38.6s\tremaining: 41.9s\n",
            "480:\tlearn: 0.0786911\ttotal: 38.7s\tremaining: 41.8s\n",
            "481:\tlearn: 0.0786762\ttotal: 38.8s\tremaining: 41.7s\n",
            "482:\tlearn: 0.0786519\ttotal: 38.9s\tremaining: 41.6s\n",
            "483:\tlearn: 0.0786329\ttotal: 39s\tremaining: 41.5s\n",
            "484:\tlearn: 0.0786133\ttotal: 39s\tremaining: 41.5s\n",
            "485:\tlearn: 0.0785926\ttotal: 39.1s\tremaining: 41.4s\n",
            "486:\tlearn: 0.0785821\ttotal: 39.2s\tremaining: 41.3s\n",
            "487:\tlearn: 0.0785777\ttotal: 39.3s\tremaining: 41.2s\n",
            "488:\tlearn: 0.0785727\ttotal: 39.4s\tremaining: 41.1s\n",
            "489:\tlearn: 0.0785703\ttotal: 39.4s\tremaining: 41.1s\n",
            "490:\tlearn: 0.0785565\ttotal: 39.5s\tremaining: 41s\n",
            "491:\tlearn: 0.0785115\ttotal: 39.6s\tremaining: 40.9s\n",
            "492:\tlearn: 0.0785073\ttotal: 39.7s\tremaining: 40.8s\n",
            "493:\tlearn: 0.0785002\ttotal: 39.8s\tremaining: 40.7s\n",
            "494:\tlearn: 0.0784818\ttotal: 39.9s\tremaining: 40.7s\n",
            "495:\tlearn: 0.0784556\ttotal: 39.9s\tremaining: 40.6s\n",
            "496:\tlearn: 0.0784511\ttotal: 40s\tremaining: 40.5s\n",
            "497:\tlearn: 0.0784418\ttotal: 40.1s\tremaining: 40.4s\n",
            "498:\tlearn: 0.0784285\ttotal: 40.2s\tremaining: 40.3s\n",
            "499:\tlearn: 0.0784230\ttotal: 40.3s\tremaining: 40.3s\n",
            "500:\tlearn: 0.0783828\ttotal: 40.3s\tremaining: 40.2s\n",
            "501:\tlearn: 0.0783720\ttotal: 40.4s\tremaining: 40.1s\n",
            "502:\tlearn: 0.0783498\ttotal: 40.5s\tremaining: 40s\n",
            "503:\tlearn: 0.0783459\ttotal: 40.6s\tremaining: 39.9s\n",
            "504:\tlearn: 0.0783282\ttotal: 40.7s\tremaining: 39.9s\n",
            "505:\tlearn: 0.0783194\ttotal: 40.8s\tremaining: 39.8s\n",
            "506:\tlearn: 0.0783133\ttotal: 40.8s\tremaining: 39.7s\n",
            "507:\tlearn: 0.0782746\ttotal: 40.9s\tremaining: 39.6s\n",
            "508:\tlearn: 0.0782636\ttotal: 41s\tremaining: 39.5s\n",
            "509:\tlearn: 0.0782537\ttotal: 41.1s\tremaining: 39.5s\n",
            "510:\tlearn: 0.0782305\ttotal: 41.1s\tremaining: 39.4s\n",
            "511:\tlearn: 0.0782039\ttotal: 41.2s\tremaining: 39.3s\n",
            "512:\tlearn: 0.0781965\ttotal: 41.3s\tremaining: 39.2s\n",
            "513:\tlearn: 0.0781490\ttotal: 41.4s\tremaining: 39.1s\n",
            "514:\tlearn: 0.0781330\ttotal: 41.5s\tremaining: 39s\n",
            "515:\tlearn: 0.0781154\ttotal: 41.5s\tremaining: 39s\n",
            "516:\tlearn: 0.0781043\ttotal: 41.6s\tremaining: 38.9s\n",
            "517:\tlearn: 0.0780849\ttotal: 41.7s\tremaining: 38.8s\n",
            "518:\tlearn: 0.0780717\ttotal: 41.8s\tremaining: 38.7s\n",
            "519:\tlearn: 0.0780633\ttotal: 41.9s\tremaining: 38.6s\n",
            "520:\tlearn: 0.0780549\ttotal: 41.9s\tremaining: 38.6s\n",
            "521:\tlearn: 0.0780435\ttotal: 42s\tremaining: 38.5s\n",
            "522:\tlearn: 0.0780409\ttotal: 42.1s\tremaining: 38.4s\n",
            "523:\tlearn: 0.0780276\ttotal: 42.2s\tremaining: 38.3s\n",
            "524:\tlearn: 0.0780221\ttotal: 42.3s\tremaining: 38.2s\n",
            "525:\tlearn: 0.0780019\ttotal: 42.3s\tremaining: 38.2s\n",
            "526:\tlearn: 0.0779949\ttotal: 42.4s\tremaining: 38.1s\n",
            "527:\tlearn: 0.0779897\ttotal: 42.5s\tremaining: 38s\n",
            "528:\tlearn: 0.0779870\ttotal: 42.6s\tremaining: 37.9s\n",
            "529:\tlearn: 0.0779565\ttotal: 42.7s\tremaining: 37.8s\n",
            "530:\tlearn: 0.0779416\ttotal: 42.7s\tremaining: 37.7s\n",
            "531:\tlearn: 0.0779302\ttotal: 42.8s\tremaining: 37.7s\n",
            "532:\tlearn: 0.0779231\ttotal: 42.9s\tremaining: 37.6s\n",
            "533:\tlearn: 0.0779076\ttotal: 43s\tremaining: 37.5s\n",
            "534:\tlearn: 0.0779036\ttotal: 43.1s\tremaining: 37.4s\n",
            "535:\tlearn: 0.0778835\ttotal: 43.1s\tremaining: 37.3s\n",
            "536:\tlearn: 0.0778730\ttotal: 43.2s\tremaining: 37.3s\n",
            "537:\tlearn: 0.0778662\ttotal: 43.3s\tremaining: 37.2s\n",
            "538:\tlearn: 0.0778624\ttotal: 43.4s\tremaining: 37.1s\n",
            "539:\tlearn: 0.0778491\ttotal: 43.5s\tremaining: 37s\n",
            "540:\tlearn: 0.0778386\ttotal: 43.5s\tremaining: 36.9s\n",
            "541:\tlearn: 0.0778260\ttotal: 43.6s\tremaining: 36.9s\n",
            "542:\tlearn: 0.0778185\ttotal: 43.7s\tremaining: 36.8s\n",
            "543:\tlearn: 0.0777974\ttotal: 43.8s\tremaining: 36.7s\n",
            "544:\tlearn: 0.0777554\ttotal: 43.9s\tremaining: 36.6s\n",
            "545:\tlearn: 0.0777484\ttotal: 44s\tremaining: 36.5s\n",
            "546:\tlearn: 0.0777417\ttotal: 44s\tremaining: 36.5s\n",
            "547:\tlearn: 0.0777133\ttotal: 44.1s\tremaining: 36.4s\n",
            "548:\tlearn: 0.0776823\ttotal: 44.2s\tremaining: 36.3s\n",
            "549:\tlearn: 0.0776741\ttotal: 44.3s\tremaining: 36.2s\n",
            "550:\tlearn: 0.0776686\ttotal: 44.4s\tremaining: 36.2s\n",
            "551:\tlearn: 0.0776501\ttotal: 44.4s\tremaining: 36.1s\n",
            "552:\tlearn: 0.0776129\ttotal: 44.5s\tremaining: 36s\n",
            "553:\tlearn: 0.0776029\ttotal: 44.6s\tremaining: 35.9s\n",
            "554:\tlearn: 0.0775886\ttotal: 44.7s\tremaining: 35.8s\n",
            "555:\tlearn: 0.0775720\ttotal: 44.8s\tremaining: 35.8s\n",
            "556:\tlearn: 0.0775527\ttotal: 44.9s\tremaining: 35.7s\n",
            "557:\tlearn: 0.0775475\ttotal: 44.9s\tremaining: 35.6s\n",
            "558:\tlearn: 0.0775415\ttotal: 45s\tremaining: 35.5s\n",
            "559:\tlearn: 0.0775225\ttotal: 45.1s\tremaining: 35.4s\n",
            "560:\tlearn: 0.0775113\ttotal: 45.2s\tremaining: 35.4s\n",
            "561:\tlearn: 0.0775073\ttotal: 45.3s\tremaining: 35.3s\n",
            "562:\tlearn: 0.0775004\ttotal: 45.4s\tremaining: 35.2s\n",
            "563:\tlearn: 0.0774833\ttotal: 45.5s\tremaining: 35.1s\n",
            "564:\tlearn: 0.0774712\ttotal: 45.5s\tremaining: 35.1s\n",
            "565:\tlearn: 0.0774545\ttotal: 45.6s\tremaining: 35s\n",
            "566:\tlearn: 0.0774494\ttotal: 45.7s\tremaining: 34.9s\n",
            "567:\tlearn: 0.0774314\ttotal: 45.8s\tremaining: 34.8s\n",
            "568:\tlearn: 0.0774251\ttotal: 45.9s\tremaining: 34.7s\n",
            "569:\tlearn: 0.0774172\ttotal: 45.9s\tremaining: 34.7s\n",
            "570:\tlearn: 0.0773873\ttotal: 46s\tremaining: 34.6s\n",
            "571:\tlearn: 0.0773829\ttotal: 46.1s\tremaining: 34.5s\n",
            "572:\tlearn: 0.0773729\ttotal: 46.2s\tremaining: 34.4s\n",
            "573:\tlearn: 0.0773670\ttotal: 46.3s\tremaining: 34.3s\n",
            "574:\tlearn: 0.0773596\ttotal: 46.4s\tremaining: 34.3s\n",
            "575:\tlearn: 0.0773399\ttotal: 46.4s\tremaining: 34.2s\n",
            "576:\tlearn: 0.0773305\ttotal: 46.5s\tremaining: 34.1s\n",
            "577:\tlearn: 0.0773251\ttotal: 46.6s\tremaining: 34s\n",
            "578:\tlearn: 0.0773168\ttotal: 46.7s\tremaining: 33.9s\n",
            "579:\tlearn: 0.0773049\ttotal: 46.8s\tremaining: 33.9s\n",
            "580:\tlearn: 0.0772918\ttotal: 46.9s\tremaining: 33.8s\n",
            "581:\tlearn: 0.0772658\ttotal: 46.9s\tremaining: 33.7s\n",
            "582:\tlearn: 0.0772559\ttotal: 47s\tremaining: 33.6s\n",
            "583:\tlearn: 0.0772482\ttotal: 47.1s\tremaining: 33.6s\n",
            "584:\tlearn: 0.0772366\ttotal: 47.2s\tremaining: 33.5s\n",
            "585:\tlearn: 0.0772226\ttotal: 47.3s\tremaining: 33.4s\n",
            "586:\tlearn: 0.0772111\ttotal: 47.4s\tremaining: 33.3s\n",
            "587:\tlearn: 0.0772078\ttotal: 47.4s\tremaining: 33.2s\n",
            "588:\tlearn: 0.0771998\ttotal: 47.5s\tremaining: 33.2s\n",
            "589:\tlearn: 0.0771936\ttotal: 47.6s\tremaining: 33.1s\n",
            "590:\tlearn: 0.0771832\ttotal: 47.7s\tremaining: 33s\n",
            "591:\tlearn: 0.0771706\ttotal: 47.8s\tremaining: 32.9s\n",
            "592:\tlearn: 0.0771506\ttotal: 47.9s\tremaining: 32.8s\n",
            "593:\tlearn: 0.0771319\ttotal: 47.9s\tremaining: 32.8s\n",
            "594:\tlearn: 0.0771221\ttotal: 48s\tremaining: 32.7s\n",
            "595:\tlearn: 0.0771132\ttotal: 48.1s\tremaining: 32.6s\n",
            "596:\tlearn: 0.0771018\ttotal: 48.2s\tremaining: 32.5s\n",
            "597:\tlearn: 0.0770905\ttotal: 48.3s\tremaining: 32.4s\n",
            "598:\tlearn: 0.0770856\ttotal: 48.3s\tremaining: 32.4s\n",
            "599:\tlearn: 0.0770840\ttotal: 48.4s\tremaining: 32.3s\n",
            "600:\tlearn: 0.0770692\ttotal: 48.5s\tremaining: 32.2s\n",
            "601:\tlearn: 0.0770605\ttotal: 48.6s\tremaining: 32.1s\n",
            "602:\tlearn: 0.0770573\ttotal: 48.7s\tremaining: 32s\n",
            "603:\tlearn: 0.0770489\ttotal: 48.7s\tremaining: 32s\n",
            "604:\tlearn: 0.0770314\ttotal: 48.8s\tremaining: 31.9s\n",
            "605:\tlearn: 0.0770251\ttotal: 48.9s\tremaining: 31.8s\n",
            "606:\tlearn: 0.0769907\ttotal: 49s\tremaining: 31.7s\n",
            "607:\tlearn: 0.0769894\ttotal: 49.1s\tremaining: 31.6s\n",
            "608:\tlearn: 0.0769753\ttotal: 49.1s\tremaining: 31.6s\n",
            "609:\tlearn: 0.0769528\ttotal: 49.2s\tremaining: 31.5s\n",
            "610:\tlearn: 0.0769456\ttotal: 49.3s\tremaining: 31.4s\n",
            "611:\tlearn: 0.0769083\ttotal: 49.4s\tremaining: 31.3s\n",
            "612:\tlearn: 0.0769035\ttotal: 49.5s\tremaining: 31.2s\n",
            "613:\tlearn: 0.0768920\ttotal: 49.6s\tremaining: 31.2s\n",
            "614:\tlearn: 0.0768901\ttotal: 49.6s\tremaining: 31.1s\n",
            "615:\tlearn: 0.0768800\ttotal: 49.7s\tremaining: 31s\n",
            "616:\tlearn: 0.0768758\ttotal: 49.8s\tremaining: 30.9s\n",
            "617:\tlearn: 0.0768519\ttotal: 49.9s\tremaining: 30.8s\n",
            "618:\tlearn: 0.0768424\ttotal: 50s\tremaining: 30.8s\n",
            "619:\tlearn: 0.0768293\ttotal: 50s\tremaining: 30.7s\n",
            "620:\tlearn: 0.0768207\ttotal: 50.1s\tremaining: 30.6s\n",
            "621:\tlearn: 0.0768058\ttotal: 50.2s\tremaining: 30.5s\n",
            "622:\tlearn: 0.0768022\ttotal: 50.3s\tremaining: 30.4s\n",
            "623:\tlearn: 0.0767913\ttotal: 50.4s\tremaining: 30.4s\n",
            "624:\tlearn: 0.0767875\ttotal: 50.5s\tremaining: 30.3s\n",
            "625:\tlearn: 0.0767819\ttotal: 50.5s\tremaining: 30.2s\n",
            "626:\tlearn: 0.0767789\ttotal: 50.6s\tremaining: 30.1s\n",
            "627:\tlearn: 0.0767765\ttotal: 50.7s\tremaining: 30s\n",
            "628:\tlearn: 0.0767730\ttotal: 50.8s\tremaining: 29.9s\n",
            "629:\tlearn: 0.0767624\ttotal: 50.9s\tremaining: 29.9s\n",
            "630:\tlearn: 0.0767570\ttotal: 50.9s\tremaining: 29.8s\n",
            "631:\tlearn: 0.0767505\ttotal: 51s\tremaining: 29.7s\n",
            "632:\tlearn: 0.0767447\ttotal: 51.1s\tremaining: 29.6s\n",
            "633:\tlearn: 0.0767238\ttotal: 51.2s\tremaining: 29.5s\n",
            "634:\tlearn: 0.0767208\ttotal: 51.3s\tremaining: 29.5s\n",
            "635:\tlearn: 0.0767088\ttotal: 51.3s\tremaining: 29.4s\n",
            "636:\tlearn: 0.0767006\ttotal: 51.4s\tremaining: 29.3s\n",
            "637:\tlearn: 0.0766913\ttotal: 51.5s\tremaining: 29.2s\n",
            "638:\tlearn: 0.0766815\ttotal: 51.6s\tremaining: 29.1s\n",
            "639:\tlearn: 0.0766741\ttotal: 51.7s\tremaining: 29.1s\n",
            "640:\tlearn: 0.0766682\ttotal: 51.7s\tremaining: 29s\n",
            "641:\tlearn: 0.0766642\ttotal: 51.8s\tremaining: 28.9s\n",
            "642:\tlearn: 0.0766542\ttotal: 51.9s\tremaining: 28.8s\n",
            "643:\tlearn: 0.0766355\ttotal: 52s\tremaining: 28.7s\n",
            "644:\tlearn: 0.0766285\ttotal: 52.1s\tremaining: 28.7s\n",
            "645:\tlearn: 0.0766111\ttotal: 52.1s\tremaining: 28.6s\n",
            "646:\tlearn: 0.0766061\ttotal: 52.2s\tremaining: 28.5s\n",
            "647:\tlearn: 0.0765999\ttotal: 52.3s\tremaining: 28.4s\n",
            "648:\tlearn: 0.0765710\ttotal: 52.4s\tremaining: 28.3s\n",
            "649:\tlearn: 0.0765660\ttotal: 52.5s\tremaining: 28.3s\n",
            "650:\tlearn: 0.0765607\ttotal: 52.5s\tremaining: 28.2s\n",
            "651:\tlearn: 0.0765543\ttotal: 52.6s\tremaining: 28.1s\n",
            "652:\tlearn: 0.0765465\ttotal: 52.7s\tremaining: 28s\n",
            "653:\tlearn: 0.0765439\ttotal: 52.8s\tremaining: 27.9s\n",
            "654:\tlearn: 0.0765416\ttotal: 52.9s\tremaining: 27.8s\n",
            "655:\tlearn: 0.0765350\ttotal: 53s\tremaining: 27.8s\n",
            "656:\tlearn: 0.0765191\ttotal: 53s\tremaining: 27.7s\n",
            "657:\tlearn: 0.0765157\ttotal: 53.1s\tremaining: 27.6s\n",
            "658:\tlearn: 0.0765105\ttotal: 53.2s\tremaining: 27.5s\n",
            "659:\tlearn: 0.0765078\ttotal: 53.3s\tremaining: 27.4s\n",
            "660:\tlearn: 0.0764969\ttotal: 53.4s\tremaining: 27.4s\n",
            "661:\tlearn: 0.0764842\ttotal: 53.4s\tremaining: 27.3s\n",
            "662:\tlearn: 0.0764764\ttotal: 53.5s\tremaining: 27.2s\n",
            "663:\tlearn: 0.0764658\ttotal: 53.6s\tremaining: 27.1s\n",
            "664:\tlearn: 0.0764626\ttotal: 53.7s\tremaining: 27s\n",
            "665:\tlearn: 0.0764557\ttotal: 53.8s\tremaining: 27s\n",
            "666:\tlearn: 0.0764350\ttotal: 53.8s\tremaining: 26.9s\n",
            "667:\tlearn: 0.0764176\ttotal: 53.9s\tremaining: 26.8s\n",
            "668:\tlearn: 0.0764018\ttotal: 54s\tremaining: 26.7s\n",
            "669:\tlearn: 0.0763891\ttotal: 54.1s\tremaining: 26.6s\n",
            "670:\tlearn: 0.0763770\ttotal: 54.2s\tremaining: 26.6s\n",
            "671:\tlearn: 0.0763696\ttotal: 54.2s\tremaining: 26.5s\n",
            "672:\tlearn: 0.0763614\ttotal: 54.3s\tremaining: 26.4s\n",
            "673:\tlearn: 0.0763594\ttotal: 54.4s\tremaining: 26.3s\n",
            "674:\tlearn: 0.0763539\ttotal: 54.5s\tremaining: 26.2s\n",
            "675:\tlearn: 0.0763459\ttotal: 54.6s\tremaining: 26.1s\n",
            "676:\tlearn: 0.0763406\ttotal: 54.6s\tremaining: 26.1s\n",
            "677:\tlearn: 0.0763365\ttotal: 54.7s\tremaining: 26s\n",
            "678:\tlearn: 0.0763295\ttotal: 54.8s\tremaining: 25.9s\n",
            "679:\tlearn: 0.0763260\ttotal: 54.9s\tremaining: 25.8s\n",
            "680:\tlearn: 0.0763235\ttotal: 55s\tremaining: 25.7s\n",
            "681:\tlearn: 0.0763170\ttotal: 55s\tremaining: 25.7s\n",
            "682:\tlearn: 0.0763111\ttotal: 55.1s\tremaining: 25.6s\n",
            "683:\tlearn: 0.0763012\ttotal: 55.2s\tremaining: 25.5s\n",
            "684:\tlearn: 0.0762953\ttotal: 55.3s\tremaining: 25.4s\n",
            "685:\tlearn: 0.0762908\ttotal: 55.4s\tremaining: 25.3s\n",
            "686:\tlearn: 0.0762808\ttotal: 55.4s\tremaining: 25.3s\n",
            "687:\tlearn: 0.0762723\ttotal: 55.5s\tremaining: 25.2s\n",
            "688:\tlearn: 0.0762653\ttotal: 55.6s\tremaining: 25.1s\n",
            "689:\tlearn: 0.0762604\ttotal: 55.7s\tremaining: 25s\n",
            "690:\tlearn: 0.0762563\ttotal: 55.8s\tremaining: 24.9s\n",
            "691:\tlearn: 0.0762519\ttotal: 55.8s\tremaining: 24.8s\n",
            "692:\tlearn: 0.0762447\ttotal: 55.9s\tremaining: 24.8s\n",
            "693:\tlearn: 0.0762379\ttotal: 56s\tremaining: 24.7s\n",
            "694:\tlearn: 0.0762345\ttotal: 56.1s\tremaining: 24.6s\n",
            "695:\tlearn: 0.0762236\ttotal: 56.1s\tremaining: 24.5s\n",
            "696:\tlearn: 0.0762181\ttotal: 56.2s\tremaining: 24.4s\n",
            "697:\tlearn: 0.0761811\ttotal: 56.3s\tremaining: 24.4s\n",
            "698:\tlearn: 0.0761751\ttotal: 56.4s\tremaining: 24.3s\n",
            "699:\tlearn: 0.0761694\ttotal: 56.5s\tremaining: 24.2s\n",
            "700:\tlearn: 0.0761671\ttotal: 56.5s\tremaining: 24.1s\n",
            "701:\tlearn: 0.0761629\ttotal: 56.6s\tremaining: 24s\n",
            "702:\tlearn: 0.0761612\ttotal: 56.7s\tremaining: 24s\n",
            "703:\tlearn: 0.0761598\ttotal: 56.8s\tremaining: 23.9s\n",
            "704:\tlearn: 0.0761439\ttotal: 56.9s\tremaining: 23.8s\n",
            "705:\tlearn: 0.0761404\ttotal: 56.9s\tremaining: 23.7s\n",
            "706:\tlearn: 0.0761320\ttotal: 57s\tremaining: 23.6s\n",
            "707:\tlearn: 0.0761236\ttotal: 57.1s\tremaining: 23.6s\n",
            "708:\tlearn: 0.0761187\ttotal: 57.2s\tremaining: 23.5s\n",
            "709:\tlearn: 0.0761084\ttotal: 57.3s\tremaining: 23.4s\n",
            "710:\tlearn: 0.0761024\ttotal: 57.4s\tremaining: 23.3s\n",
            "711:\tlearn: 0.0760979\ttotal: 57.4s\tremaining: 23.2s\n",
            "712:\tlearn: 0.0760959\ttotal: 57.5s\tremaining: 23.1s\n",
            "713:\tlearn: 0.0760924\ttotal: 57.6s\tremaining: 23.1s\n",
            "714:\tlearn: 0.0760824\ttotal: 57.7s\tremaining: 23s\n",
            "715:\tlearn: 0.0760770\ttotal: 57.8s\tremaining: 22.9s\n",
            "716:\tlearn: 0.0760696\ttotal: 57.8s\tremaining: 22.8s\n",
            "717:\tlearn: 0.0760679\ttotal: 57.9s\tremaining: 22.7s\n",
            "718:\tlearn: 0.0760614\ttotal: 58s\tremaining: 22.7s\n",
            "719:\tlearn: 0.0760409\ttotal: 58.1s\tremaining: 22.6s\n",
            "720:\tlearn: 0.0760355\ttotal: 58.2s\tremaining: 22.5s\n",
            "721:\tlearn: 0.0760282\ttotal: 58.2s\tremaining: 22.4s\n",
            "722:\tlearn: 0.0760271\ttotal: 58.3s\tremaining: 22.3s\n",
            "723:\tlearn: 0.0760160\ttotal: 58.4s\tremaining: 22.3s\n",
            "724:\tlearn: 0.0760121\ttotal: 58.5s\tremaining: 22.2s\n",
            "725:\tlearn: 0.0759991\ttotal: 58.6s\tremaining: 22.1s\n",
            "726:\tlearn: 0.0759962\ttotal: 58.6s\tremaining: 22s\n",
            "727:\tlearn: 0.0759842\ttotal: 58.7s\tremaining: 21.9s\n",
            "728:\tlearn: 0.0759824\ttotal: 58.8s\tremaining: 21.9s\n",
            "729:\tlearn: 0.0759701\ttotal: 58.9s\tremaining: 21.8s\n",
            "730:\tlearn: 0.0759588\ttotal: 59s\tremaining: 21.7s\n",
            "731:\tlearn: 0.0759543\ttotal: 59s\tremaining: 21.6s\n",
            "732:\tlearn: 0.0759480\ttotal: 59.1s\tremaining: 21.5s\n",
            "733:\tlearn: 0.0759356\ttotal: 59.2s\tremaining: 21.5s\n",
            "734:\tlearn: 0.0759265\ttotal: 59.3s\tremaining: 21.4s\n",
            "735:\tlearn: 0.0759193\ttotal: 59.4s\tremaining: 21.3s\n",
            "736:\tlearn: 0.0759138\ttotal: 59.4s\tremaining: 21.2s\n",
            "737:\tlearn: 0.0759068\ttotal: 59.5s\tremaining: 21.1s\n",
            "738:\tlearn: 0.0758980\ttotal: 59.6s\tremaining: 21.1s\n",
            "739:\tlearn: 0.0758948\ttotal: 59.7s\tremaining: 21s\n",
            "740:\tlearn: 0.0758919\ttotal: 59.8s\tremaining: 20.9s\n",
            "741:\tlearn: 0.0758769\ttotal: 59.8s\tremaining: 20.8s\n",
            "742:\tlearn: 0.0758690\ttotal: 59.9s\tremaining: 20.7s\n",
            "743:\tlearn: 0.0758548\ttotal: 60s\tremaining: 20.6s\n",
            "744:\tlearn: 0.0758481\ttotal: 1m\tremaining: 20.6s\n",
            "745:\tlearn: 0.0758435\ttotal: 1m\tremaining: 20.5s\n",
            "746:\tlearn: 0.0758360\ttotal: 1m\tremaining: 20.4s\n",
            "747:\tlearn: 0.0758283\ttotal: 1m\tremaining: 20.3s\n",
            "748:\tlearn: 0.0758118\ttotal: 1m\tremaining: 20.2s\n",
            "749:\tlearn: 0.0758079\ttotal: 1m\tremaining: 20.2s\n",
            "750:\tlearn: 0.0758064\ttotal: 1m\tremaining: 20.1s\n",
            "751:\tlearn: 0.0758054\ttotal: 1m\tremaining: 20s\n",
            "752:\tlearn: 0.0757971\ttotal: 1m\tremaining: 19.9s\n",
            "753:\tlearn: 0.0757921\ttotal: 1m\tremaining: 19.8s\n",
            "754:\tlearn: 0.0757886\ttotal: 1m\tremaining: 19.7s\n",
            "755:\tlearn: 0.0757761\ttotal: 1m\tremaining: 19.7s\n",
            "756:\tlearn: 0.0757600\ttotal: 1m 1s\tremaining: 19.6s\n",
            "757:\tlearn: 0.0757533\ttotal: 1m 1s\tremaining: 19.5s\n",
            "758:\tlearn: 0.0757296\ttotal: 1m 1s\tremaining: 19.4s\n",
            "759:\tlearn: 0.0757254\ttotal: 1m 1s\tremaining: 19.3s\n",
            "760:\tlearn: 0.0757157\ttotal: 1m 1s\tremaining: 19.3s\n",
            "761:\tlearn: 0.0757135\ttotal: 1m 1s\tremaining: 19.2s\n",
            "762:\tlearn: 0.0757098\ttotal: 1m 1s\tremaining: 19.1s\n",
            "763:\tlearn: 0.0757040\ttotal: 1m 1s\tremaining: 19s\n",
            "764:\tlearn: 0.0756994\ttotal: 1m 1s\tremaining: 18.9s\n",
            "765:\tlearn: 0.0756782\ttotal: 1m 1s\tremaining: 18.9s\n",
            "766:\tlearn: 0.0756738\ttotal: 1m 1s\tremaining: 18.8s\n",
            "767:\tlearn: 0.0756636\ttotal: 1m 1s\tremaining: 18.7s\n",
            "768:\tlearn: 0.0756518\ttotal: 1m 1s\tremaining: 18.6s\n",
            "769:\tlearn: 0.0756452\ttotal: 1m 2s\tremaining: 18.5s\n",
            "770:\tlearn: 0.0756350\ttotal: 1m 2s\tremaining: 18.5s\n",
            "771:\tlearn: 0.0756291\ttotal: 1m 2s\tremaining: 18.4s\n",
            "772:\tlearn: 0.0756253\ttotal: 1m 2s\tremaining: 18.3s\n",
            "773:\tlearn: 0.0756117\ttotal: 1m 2s\tremaining: 18.2s\n",
            "774:\tlearn: 0.0756062\ttotal: 1m 2s\tremaining: 18.1s\n",
            "775:\tlearn: 0.0755976\ttotal: 1m 2s\tremaining: 18s\n",
            "776:\tlearn: 0.0755951\ttotal: 1m 2s\tremaining: 18s\n",
            "777:\tlearn: 0.0755855\ttotal: 1m 2s\tremaining: 17.9s\n",
            "778:\tlearn: 0.0755847\ttotal: 1m 2s\tremaining: 17.8s\n",
            "779:\tlearn: 0.0755812\ttotal: 1m 2s\tremaining: 17.7s\n",
            "780:\tlearn: 0.0755734\ttotal: 1m 2s\tremaining: 17.6s\n",
            "781:\tlearn: 0.0755691\ttotal: 1m 2s\tremaining: 17.6s\n",
            "782:\tlearn: 0.0755647\ttotal: 1m 3s\tremaining: 17.5s\n",
            "783:\tlearn: 0.0755562\ttotal: 1m 3s\tremaining: 17.4s\n",
            "784:\tlearn: 0.0755536\ttotal: 1m 3s\tremaining: 17.3s\n",
            "785:\tlearn: 0.0755489\ttotal: 1m 3s\tremaining: 17.2s\n",
            "786:\tlearn: 0.0755405\ttotal: 1m 3s\tremaining: 17.2s\n",
            "787:\tlearn: 0.0755375\ttotal: 1m 3s\tremaining: 17.1s\n",
            "788:\tlearn: 0.0755343\ttotal: 1m 3s\tremaining: 17s\n",
            "789:\tlearn: 0.0755261\ttotal: 1m 3s\tremaining: 16.9s\n",
            "790:\tlearn: 0.0755216\ttotal: 1m 3s\tremaining: 16.8s\n",
            "791:\tlearn: 0.0755190\ttotal: 1m 3s\tremaining: 16.8s\n",
            "792:\tlearn: 0.0755116\ttotal: 1m 3s\tremaining: 16.7s\n",
            "793:\tlearn: 0.0755064\ttotal: 1m 3s\tremaining: 16.6s\n",
            "794:\tlearn: 0.0755002\ttotal: 1m 4s\tremaining: 16.5s\n",
            "795:\tlearn: 0.0754949\ttotal: 1m 4s\tremaining: 16.4s\n",
            "796:\tlearn: 0.0754866\ttotal: 1m 4s\tremaining: 16.3s\n",
            "797:\tlearn: 0.0754803\ttotal: 1m 4s\tremaining: 16.3s\n",
            "798:\tlearn: 0.0754731\ttotal: 1m 4s\tremaining: 16.2s\n",
            "799:\tlearn: 0.0754723\ttotal: 1m 4s\tremaining: 16.1s\n",
            "800:\tlearn: 0.0754692\ttotal: 1m 4s\tremaining: 16s\n",
            "801:\tlearn: 0.0754610\ttotal: 1m 4s\tremaining: 15.9s\n",
            "802:\tlearn: 0.0754468\ttotal: 1m 4s\tremaining: 15.9s\n",
            "803:\tlearn: 0.0754312\ttotal: 1m 4s\tremaining: 15.8s\n",
            "804:\tlearn: 0.0754239\ttotal: 1m 4s\tremaining: 15.7s\n",
            "805:\tlearn: 0.0754175\ttotal: 1m 4s\tremaining: 15.6s\n",
            "806:\tlearn: 0.0754158\ttotal: 1m 4s\tremaining: 15.5s\n",
            "807:\tlearn: 0.0754098\ttotal: 1m 5s\tremaining: 15.5s\n",
            "808:\tlearn: 0.0754024\ttotal: 1m 5s\tremaining: 15.4s\n",
            "809:\tlearn: 0.0753963\ttotal: 1m 5s\tremaining: 15.3s\n",
            "810:\tlearn: 0.0753723\ttotal: 1m 5s\tremaining: 15.2s\n",
            "811:\tlearn: 0.0753587\ttotal: 1m 5s\tremaining: 15.1s\n",
            "812:\tlearn: 0.0753558\ttotal: 1m 5s\tremaining: 15.1s\n",
            "813:\tlearn: 0.0753365\ttotal: 1m 5s\tremaining: 15s\n",
            "814:\tlearn: 0.0753353\ttotal: 1m 5s\tremaining: 14.9s\n",
            "815:\tlearn: 0.0753256\ttotal: 1m 5s\tremaining: 14.8s\n",
            "816:\tlearn: 0.0753223\ttotal: 1m 5s\tremaining: 14.7s\n",
            "817:\tlearn: 0.0753195\ttotal: 1m 5s\tremaining: 14.7s\n",
            "818:\tlearn: 0.0753106\ttotal: 1m 5s\tremaining: 14.6s\n",
            "819:\tlearn: 0.0753055\ttotal: 1m 6s\tremaining: 14.5s\n",
            "820:\tlearn: 0.0753022\ttotal: 1m 6s\tremaining: 14.4s\n",
            "821:\tlearn: 0.0753005\ttotal: 1m 6s\tremaining: 14.3s\n",
            "822:\tlearn: 0.0752922\ttotal: 1m 6s\tremaining: 14.3s\n",
            "823:\tlearn: 0.0752903\ttotal: 1m 6s\tremaining: 14.2s\n",
            "824:\tlearn: 0.0752788\ttotal: 1m 6s\tremaining: 14.1s\n",
            "825:\tlearn: 0.0752663\ttotal: 1m 6s\tremaining: 14s\n",
            "826:\tlearn: 0.0752578\ttotal: 1m 6s\tremaining: 13.9s\n",
            "827:\tlearn: 0.0752284\ttotal: 1m 6s\tremaining: 13.9s\n",
            "828:\tlearn: 0.0752252\ttotal: 1m 6s\tremaining: 13.8s\n",
            "829:\tlearn: 0.0752198\ttotal: 1m 6s\tremaining: 13.7s\n",
            "830:\tlearn: 0.0752123\ttotal: 1m 6s\tremaining: 13.6s\n",
            "831:\tlearn: 0.0752111\ttotal: 1m 7s\tremaining: 13.5s\n",
            "832:\tlearn: 0.0752051\ttotal: 1m 7s\tremaining: 13.4s\n",
            "833:\tlearn: 0.0751993\ttotal: 1m 7s\tremaining: 13.4s\n",
            "834:\tlearn: 0.0751917\ttotal: 1m 7s\tremaining: 13.3s\n",
            "835:\tlearn: 0.0751845\ttotal: 1m 7s\tremaining: 13.2s\n",
            "836:\tlearn: 0.0751690\ttotal: 1m 7s\tremaining: 13.1s\n",
            "837:\tlearn: 0.0751628\ttotal: 1m 7s\tremaining: 13s\n",
            "838:\tlearn: 0.0751563\ttotal: 1m 7s\tremaining: 13s\n",
            "839:\tlearn: 0.0751461\ttotal: 1m 7s\tremaining: 12.9s\n",
            "840:\tlearn: 0.0751436\ttotal: 1m 7s\tremaining: 12.8s\n",
            "841:\tlearn: 0.0751413\ttotal: 1m 7s\tremaining: 12.7s\n",
            "842:\tlearn: 0.0751357\ttotal: 1m 7s\tremaining: 12.6s\n",
            "843:\tlearn: 0.0751280\ttotal: 1m 7s\tremaining: 12.6s\n",
            "844:\tlearn: 0.0751260\ttotal: 1m 8s\tremaining: 12.5s\n",
            "845:\tlearn: 0.0751217\ttotal: 1m 8s\tremaining: 12.4s\n",
            "846:\tlearn: 0.0751092\ttotal: 1m 8s\tremaining: 12.3s\n",
            "847:\tlearn: 0.0751003\ttotal: 1m 8s\tremaining: 12.2s\n",
            "848:\tlearn: 0.0750965\ttotal: 1m 8s\tremaining: 12.2s\n",
            "849:\tlearn: 0.0750929\ttotal: 1m 8s\tremaining: 12.1s\n",
            "850:\tlearn: 0.0750867\ttotal: 1m 8s\tremaining: 12s\n",
            "851:\tlearn: 0.0750825\ttotal: 1m 8s\tremaining: 11.9s\n",
            "852:\tlearn: 0.0750806\ttotal: 1m 8s\tremaining: 11.8s\n",
            "853:\tlearn: 0.0750663\ttotal: 1m 8s\tremaining: 11.8s\n",
            "854:\tlearn: 0.0750647\ttotal: 1m 8s\tremaining: 11.7s\n",
            "855:\tlearn: 0.0750552\ttotal: 1m 8s\tremaining: 11.6s\n",
            "856:\tlearn: 0.0750468\ttotal: 1m 8s\tremaining: 11.5s\n",
            "857:\tlearn: 0.0750302\ttotal: 1m 9s\tremaining: 11.4s\n",
            "858:\tlearn: 0.0750189\ttotal: 1m 9s\tremaining: 11.4s\n",
            "859:\tlearn: 0.0750155\ttotal: 1m 9s\tremaining: 11.3s\n",
            "860:\tlearn: 0.0750086\ttotal: 1m 9s\tremaining: 11.2s\n",
            "861:\tlearn: 0.0750073\ttotal: 1m 9s\tremaining: 11.1s\n",
            "862:\tlearn: 0.0750005\ttotal: 1m 9s\tremaining: 11s\n",
            "863:\tlearn: 0.0749979\ttotal: 1m 9s\tremaining: 10.9s\n",
            "864:\tlearn: 0.0749606\ttotal: 1m 9s\tremaining: 10.9s\n",
            "865:\tlearn: 0.0749584\ttotal: 1m 9s\tremaining: 10.8s\n",
            "866:\tlearn: 0.0749471\ttotal: 1m 9s\tremaining: 10.7s\n",
            "867:\tlearn: 0.0749417\ttotal: 1m 9s\tremaining: 10.6s\n",
            "868:\tlearn: 0.0749391\ttotal: 1m 9s\tremaining: 10.5s\n",
            "869:\tlearn: 0.0749345\ttotal: 1m 10s\tremaining: 10.5s\n",
            "870:\tlearn: 0.0749291\ttotal: 1m 10s\tremaining: 10.4s\n",
            "871:\tlearn: 0.0749253\ttotal: 1m 10s\tremaining: 10.3s\n",
            "872:\tlearn: 0.0749108\ttotal: 1m 10s\tremaining: 10.2s\n",
            "873:\tlearn: 0.0749074\ttotal: 1m 10s\tremaining: 10.1s\n",
            "874:\tlearn: 0.0749059\ttotal: 1m 10s\tremaining: 10.1s\n",
            "875:\tlearn: 0.0749010\ttotal: 1m 10s\tremaining: 9.98s\n",
            "876:\tlearn: 0.0748982\ttotal: 1m 10s\tremaining: 9.9s\n",
            "877:\tlearn: 0.0748906\ttotal: 1m 10s\tremaining: 9.82s\n",
            "878:\tlearn: 0.0748801\ttotal: 1m 10s\tremaining: 9.74s\n",
            "879:\tlearn: 0.0748751\ttotal: 1m 10s\tremaining: 9.66s\n",
            "880:\tlearn: 0.0748578\ttotal: 1m 10s\tremaining: 9.58s\n",
            "881:\tlearn: 0.0748569\ttotal: 1m 11s\tremaining: 9.5s\n",
            "882:\tlearn: 0.0748357\ttotal: 1m 11s\tremaining: 9.42s\n",
            "883:\tlearn: 0.0748314\ttotal: 1m 11s\tremaining: 9.34s\n",
            "884:\tlearn: 0.0748308\ttotal: 1m 11s\tremaining: 9.26s\n",
            "885:\tlearn: 0.0748258\ttotal: 1m 11s\tremaining: 9.18s\n",
            "886:\tlearn: 0.0748203\ttotal: 1m 11s\tremaining: 9.1s\n",
            "887:\tlearn: 0.0748178\ttotal: 1m 11s\tremaining: 9.02s\n",
            "888:\tlearn: 0.0748093\ttotal: 1m 11s\tremaining: 8.94s\n",
            "889:\tlearn: 0.0748077\ttotal: 1m 11s\tremaining: 8.86s\n",
            "890:\tlearn: 0.0748058\ttotal: 1m 11s\tremaining: 8.78s\n",
            "891:\tlearn: 0.0748035\ttotal: 1m 11s\tremaining: 8.69s\n",
            "892:\tlearn: 0.0748020\ttotal: 1m 11s\tremaining: 8.61s\n",
            "893:\tlearn: 0.0747961\ttotal: 1m 11s\tremaining: 8.53s\n",
            "894:\tlearn: 0.0747950\ttotal: 1m 12s\tremaining: 8.45s\n",
            "895:\tlearn: 0.0747936\ttotal: 1m 12s\tremaining: 8.37s\n",
            "896:\tlearn: 0.0747823\ttotal: 1m 12s\tremaining: 8.29s\n",
            "897:\tlearn: 0.0747755\ttotal: 1m 12s\tremaining: 8.21s\n",
            "898:\tlearn: 0.0747723\ttotal: 1m 12s\tremaining: 8.13s\n",
            "899:\tlearn: 0.0747671\ttotal: 1m 12s\tremaining: 8.05s\n",
            "900:\tlearn: 0.0747616\ttotal: 1m 12s\tremaining: 7.97s\n",
            "901:\tlearn: 0.0747442\ttotal: 1m 12s\tremaining: 7.89s\n",
            "902:\tlearn: 0.0747396\ttotal: 1m 12s\tremaining: 7.81s\n",
            "903:\tlearn: 0.0747375\ttotal: 1m 12s\tremaining: 7.73s\n",
            "904:\tlearn: 0.0747245\ttotal: 1m 12s\tremaining: 7.65s\n",
            "905:\tlearn: 0.0747224\ttotal: 1m 12s\tremaining: 7.57s\n",
            "906:\tlearn: 0.0747193\ttotal: 1m 13s\tremaining: 7.49s\n",
            "907:\tlearn: 0.0747106\ttotal: 1m 13s\tremaining: 7.41s\n",
            "908:\tlearn: 0.0746997\ttotal: 1m 13s\tremaining: 7.32s\n",
            "909:\tlearn: 0.0746859\ttotal: 1m 13s\tremaining: 7.24s\n",
            "910:\tlearn: 0.0746739\ttotal: 1m 13s\tremaining: 7.16s\n",
            "911:\tlearn: 0.0746675\ttotal: 1m 13s\tremaining: 7.08s\n",
            "912:\tlearn: 0.0746664\ttotal: 1m 13s\tremaining: 7s\n",
            "913:\tlearn: 0.0746531\ttotal: 1m 13s\tremaining: 6.92s\n",
            "914:\tlearn: 0.0746412\ttotal: 1m 13s\tremaining: 6.84s\n",
            "915:\tlearn: 0.0746408\ttotal: 1m 13s\tremaining: 6.76s\n",
            "916:\tlearn: 0.0746393\ttotal: 1m 13s\tremaining: 6.68s\n",
            "917:\tlearn: 0.0746319\ttotal: 1m 13s\tremaining: 6.6s\n",
            "918:\tlearn: 0.0746285\ttotal: 1m 13s\tremaining: 6.52s\n",
            "919:\tlearn: 0.0746153\ttotal: 1m 14s\tremaining: 6.44s\n",
            "920:\tlearn: 0.0746125\ttotal: 1m 14s\tremaining: 6.36s\n",
            "921:\tlearn: 0.0746106\ttotal: 1m 14s\tremaining: 6.28s\n",
            "922:\tlearn: 0.0746095\ttotal: 1m 14s\tremaining: 6.2s\n",
            "923:\tlearn: 0.0746088\ttotal: 1m 14s\tremaining: 6.12s\n",
            "924:\tlearn: 0.0746065\ttotal: 1m 14s\tremaining: 6.04s\n",
            "925:\tlearn: 0.0746060\ttotal: 1m 14s\tremaining: 5.96s\n",
            "926:\tlearn: 0.0746017\ttotal: 1m 14s\tremaining: 5.88s\n",
            "927:\tlearn: 0.0745965\ttotal: 1m 14s\tremaining: 5.79s\n",
            "928:\tlearn: 0.0745850\ttotal: 1m 14s\tremaining: 5.71s\n",
            "929:\tlearn: 0.0745727\ttotal: 1m 14s\tremaining: 5.63s\n",
            "930:\tlearn: 0.0745674\ttotal: 1m 14s\tremaining: 5.55s\n",
            "931:\tlearn: 0.0745668\ttotal: 1m 15s\tremaining: 5.47s\n",
            "932:\tlearn: 0.0745631\ttotal: 1m 15s\tremaining: 5.39s\n",
            "933:\tlearn: 0.0745536\ttotal: 1m 15s\tremaining: 5.31s\n",
            "934:\tlearn: 0.0745528\ttotal: 1m 15s\tremaining: 5.23s\n",
            "935:\tlearn: 0.0745495\ttotal: 1m 15s\tremaining: 5.15s\n",
            "936:\tlearn: 0.0745475\ttotal: 1m 15s\tremaining: 5.07s\n",
            "937:\tlearn: 0.0745440\ttotal: 1m 15s\tremaining: 4.99s\n",
            "938:\tlearn: 0.0745405\ttotal: 1m 15s\tremaining: 4.91s\n",
            "939:\tlearn: 0.0745375\ttotal: 1m 15s\tremaining: 4.83s\n",
            "940:\tlearn: 0.0745349\ttotal: 1m 15s\tremaining: 4.75s\n",
            "941:\tlearn: 0.0745291\ttotal: 1m 15s\tremaining: 4.67s\n",
            "942:\tlearn: 0.0745248\ttotal: 1m 15s\tremaining: 4.59s\n",
            "943:\tlearn: 0.0745039\ttotal: 1m 15s\tremaining: 4.51s\n",
            "944:\tlearn: 0.0744875\ttotal: 1m 16s\tremaining: 4.43s\n",
            "945:\tlearn: 0.0744796\ttotal: 1m 16s\tremaining: 4.35s\n",
            "946:\tlearn: 0.0744743\ttotal: 1m 16s\tremaining: 4.27s\n",
            "947:\tlearn: 0.0744685\ttotal: 1m 16s\tremaining: 4.18s\n",
            "948:\tlearn: 0.0744630\ttotal: 1m 16s\tremaining: 4.11s\n",
            "949:\tlearn: 0.0744530\ttotal: 1m 16s\tremaining: 4.02s\n",
            "950:\tlearn: 0.0744444\ttotal: 1m 16s\tremaining: 3.94s\n",
            "951:\tlearn: 0.0744383\ttotal: 1m 16s\tremaining: 3.86s\n",
            "952:\tlearn: 0.0744317\ttotal: 1m 16s\tremaining: 3.78s\n",
            "953:\tlearn: 0.0744207\ttotal: 1m 16s\tremaining: 3.7s\n",
            "954:\tlearn: 0.0744198\ttotal: 1m 16s\tremaining: 3.62s\n",
            "955:\tlearn: 0.0744177\ttotal: 1m 16s\tremaining: 3.54s\n",
            "956:\tlearn: 0.0744140\ttotal: 1m 17s\tremaining: 3.46s\n",
            "957:\tlearn: 0.0744044\ttotal: 1m 17s\tremaining: 3.38s\n",
            "958:\tlearn: 0.0744039\ttotal: 1m 17s\tremaining: 3.3s\n",
            "959:\tlearn: 0.0743962\ttotal: 1m 17s\tremaining: 3.22s\n",
            "960:\tlearn: 0.0743729\ttotal: 1m 17s\tremaining: 3.14s\n",
            "961:\tlearn: 0.0743643\ttotal: 1m 17s\tremaining: 3.06s\n",
            "962:\tlearn: 0.0743614\ttotal: 1m 17s\tremaining: 2.98s\n",
            "963:\tlearn: 0.0743515\ttotal: 1m 17s\tremaining: 2.9s\n",
            "964:\tlearn: 0.0743456\ttotal: 1m 17s\tremaining: 2.82s\n",
            "965:\tlearn: 0.0743416\ttotal: 1m 17s\tremaining: 2.74s\n",
            "966:\tlearn: 0.0743383\ttotal: 1m 17s\tremaining: 2.66s\n",
            "967:\tlearn: 0.0743259\ttotal: 1m 17s\tremaining: 2.58s\n",
            "968:\tlearn: 0.0743217\ttotal: 1m 18s\tremaining: 2.5s\n",
            "969:\tlearn: 0.0743188\ttotal: 1m 18s\tremaining: 2.41s\n",
            "970:\tlearn: 0.0743139\ttotal: 1m 18s\tremaining: 2.33s\n",
            "971:\tlearn: 0.0743090\ttotal: 1m 18s\tremaining: 2.25s\n",
            "972:\tlearn: 0.0743043\ttotal: 1m 18s\tremaining: 2.17s\n",
            "973:\tlearn: 0.0743029\ttotal: 1m 18s\tremaining: 2.09s\n",
            "974:\tlearn: 0.0742977\ttotal: 1m 18s\tremaining: 2.01s\n",
            "975:\tlearn: 0.0742963\ttotal: 1m 18s\tremaining: 1.93s\n",
            "976:\tlearn: 0.0742903\ttotal: 1m 18s\tremaining: 1.85s\n",
            "977:\tlearn: 0.0742737\ttotal: 1m 18s\tremaining: 1.77s\n",
            "978:\tlearn: 0.0742616\ttotal: 1m 18s\tremaining: 1.69s\n",
            "979:\tlearn: 0.0742554\ttotal: 1m 18s\tremaining: 1.61s\n",
            "980:\tlearn: 0.0742506\ttotal: 1m 18s\tremaining: 1.53s\n",
            "981:\tlearn: 0.0742486\ttotal: 1m 19s\tremaining: 1.45s\n",
            "982:\tlearn: 0.0742441\ttotal: 1m 19s\tremaining: 1.37s\n",
            "983:\tlearn: 0.0742438\ttotal: 1m 19s\tremaining: 1.29s\n",
            "984:\tlearn: 0.0742403\ttotal: 1m 19s\tremaining: 1.21s\n",
            "985:\tlearn: 0.0742292\ttotal: 1m 19s\tremaining: 1.13s\n",
            "986:\tlearn: 0.0742240\ttotal: 1m 19s\tremaining: 1.05s\n",
            "987:\tlearn: 0.0742037\ttotal: 1m 19s\tremaining: 966ms\n",
            "988:\tlearn: 0.0741989\ttotal: 1m 19s\tremaining: 885ms\n",
            "989:\tlearn: 0.0741893\ttotal: 1m 19s\tremaining: 805ms\n",
            "990:\tlearn: 0.0741808\ttotal: 1m 19s\tremaining: 724ms\n",
            "991:\tlearn: 0.0741770\ttotal: 1m 19s\tremaining: 644ms\n",
            "992:\tlearn: 0.0741663\ttotal: 1m 19s\tremaining: 563ms\n",
            "993:\tlearn: 0.0741611\ttotal: 1m 20s\tremaining: 483ms\n",
            "994:\tlearn: 0.0741553\ttotal: 1m 20s\tremaining: 402ms\n",
            "995:\tlearn: 0.0741503\ttotal: 1m 20s\tremaining: 322ms\n",
            "996:\tlearn: 0.0741431\ttotal: 1m 20s\tremaining: 241ms\n",
            "997:\tlearn: 0.0741421\ttotal: 1m 20s\tremaining: 161ms\n",
            "998:\tlearn: 0.0741382\ttotal: 1m 20s\tremaining: 80.5ms\n",
            "999:\tlearn: 0.0741346\ttotal: 1m 20s\tremaining: 0us\n",
            "Accuracy of Model: 0.9777076194509683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Precision of Model: 0.974293595107081\n",
            "Recall of Model: 0.9777076194509683\n",
            "F1-score of Model: 0.9739388418021312\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     21576\n",
            "           1       0.91      0.99      0.95       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           4       0.92      0.97      0.94      1098\n",
            "           5       0.75      1.00      0.86         3\n",
            "           6       0.99      0.98      0.98       922\n",
            "           7       0.97      0.98      0.98        66\n",
            "           8       0.99      1.00      1.00       739\n",
            "           9       0.79      0.48      0.60       224\n",
            "          10       0.95      1.00      0.97        18\n",
            "          11       0.71      0.91      0.79       309\n",
            "          12       0.51      0.08      0.15       259\n",
            "          14       1.00      1.00      1.00       300\n",
            "\n",
            "    accuracy                           0.98     25973\n",
            "   macro avg       0.81      0.80      0.79     25973\n",
            "weighted avg       0.97      0.98      0.97     25973\n",
            "\n",
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  cat_model_new.sav\n",
            "Accuracy Model:  0.9777076194509683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ausBTxBWM47",
        "outputId": "59160c33-a318-4b7e-c65c-ae011dd8e6c7"
      },
      "source": [
        "build_model_new (light, X_train4 , y_train4 , X_test4 , y_test4)\n",
        "light_train_new , light_test_new = save_model_new(light,'light_model_new', X_train4 , y_train4 , X_test4 , y_test4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
            "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
            "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
            "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
            "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
            "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
            "Accuracy of Model: 0.8565433334616718\n",
            "Precision of Model: 0.8485782065202214\n",
            "Recall of Model: 0.8565433334616718\n",
            "F1-score of Model: 0.8495370007548657\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.92     21576\n",
            "           1       0.28      0.50      0.36       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           3       0.00      0.00      0.00         0\n",
            "           4       0.66      0.92      0.77      1098\n",
            "           5       0.00      0.00      0.00         3\n",
            "           6       0.64      0.37      0.47       922\n",
            "           7       0.00      0.00      0.00        66\n",
            "           8       0.82      0.75      0.78       739\n",
            "           9       0.00      0.00      0.00       224\n",
            "          10       0.00      0.00      0.00        18\n",
            "          11       0.00      0.00      0.00       309\n",
            "          12       0.00      0.00      0.00       259\n",
            "          14       0.69      0.48      0.57       300\n",
            "\n",
            "    accuracy                           0.86     25973\n",
            "   macro avg       0.29      0.28      0.28     25973\n",
            "weighted avg       0.85      0.86      0.85     25973\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  light_model_new.sav\n",
            "Accuracy Model:  0.8565433334616718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36-H0mvDuxy8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s19ltJMnu7R2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CDelKhB3j81"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzyAYhzW8H9j"
      },
      "source": [
        "### 8. Concate New Dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52HleKrP8b1I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TQFvaGL3W41"
      },
      "source": [
        "X_df = pd.DataFrame(X_new, columns = impocol)\n",
        "X_df.to_csv(\"X_new.csv\", index=False)\n",
        "X_df.head(2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYgDqcmB4L5c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "DDOCMXZn5JSM",
        "outputId": "be8dc92f-559d-48da-8798-7aca41ace1ac"
      },
      "source": [
        "y_df = pd.DataFrame(y_new,columns =['label'])\n",
        "y_df.to_csv(\"y_new.csv\", index = False)\n",
        "y_df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label\n",
              "0      0\n",
              "1      0"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "hvUfuvia5JK0",
        "outputId": "dd92817a-45c1-4800-cd8a-a0bbe4e6a505"
      },
      "source": [
        "df_new = pd.concat([X_df, y_df], axis=1)\n",
        "df_new.to_csv(\"df_new.csv\", index = False)\n",
        "df_new.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>protocol</th>\n",
              "      <th>flow_duration</th>\n",
              "      <th>tot_fwd_pkts</th>\n",
              "      <th>tot_bwd_pkts</th>\n",
              "      <th>totlen_fwd_pkts</th>\n",
              "      <th>totlen_bwd_pkts</th>\n",
              "      <th>fwd_pkt_len_max</th>\n",
              "      <th>fwd_pkt_len_min</th>\n",
              "      <th>fwd_pkt_len_mean</th>\n",
              "      <th>fwd_pkt_len_std</th>\n",
              "      <th>bwd_pkt_len_max</th>\n",
              "      <th>bwd_pkt_len_min</th>\n",
              "      <th>bwd_pkt_len_mean</th>\n",
              "      <th>bwd_pkt_len_std</th>\n",
              "      <th>flow_byts_s</th>\n",
              "      <th>flow_pkts_s</th>\n",
              "      <th>flow_iat_mean</th>\n",
              "      <th>flow_iat_std</th>\n",
              "      <th>flow_iat_max</th>\n",
              "      <th>flow_iat_min</th>\n",
              "      <th>fwd_iat_tot</th>\n",
              "      <th>fwd_iat_mean</th>\n",
              "      <th>fwd_iat_std</th>\n",
              "      <th>fwd_iat_max</th>\n",
              "      <th>fwd_iat_min</th>\n",
              "      <th>bwd_iat_tot</th>\n",
              "      <th>bwd_iat_mean</th>\n",
              "      <th>bwd_iat_std</th>\n",
              "      <th>bwd_iat_max</th>\n",
              "      <th>bwd_iat_min</th>\n",
              "      <th>fwd_psh_flags</th>\n",
              "      <th>fwd_urg_flags</th>\n",
              "      <th>fwd_header_len</th>\n",
              "      <th>bwd_header_len</th>\n",
              "      <th>fwd_pkts_s</th>\n",
              "      <th>bwd_pkts_s</th>\n",
              "      <th>pkt_len_min</th>\n",
              "      <th>pkt_len_max</th>\n",
              "      <th>pkt_len_mean</th>\n",
              "      <th>pkt_len_std</th>\n",
              "      <th>pkt_len_var</th>\n",
              "      <th>fin_flag_cnt</th>\n",
              "      <th>syn_flag_cnt</th>\n",
              "      <th>rst_flag_cnt</th>\n",
              "      <th>psh_flag_cnt</th>\n",
              "      <th>ack_flag_cnt</th>\n",
              "      <th>urg_flag_cnt</th>\n",
              "      <th>cwe_flag_count</th>\n",
              "      <th>ece_flag_cnt</th>\n",
              "      <th>down_up_ratio</th>\n",
              "      <th>pkt_size_avg</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>2653695.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1148.0</td>\n",
              "      <td>1581.0</td>\n",
              "      <td>677.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>143.50000</td>\n",
              "      <td>228.12967</td>\n",
              "      <td>1173.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>158.1000</td>\n",
              "      <td>367.73070</td>\n",
              "      <td>1028.377413</td>\n",
              "      <td>6.782995</td>\n",
              "      <td>1.560997e+05</td>\n",
              "      <td>2.540860e+05</td>\n",
              "      <td>1062847.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>2653695.0</td>\n",
              "      <td>379099.280</td>\n",
              "      <td>4.393966e+05</td>\n",
              "      <td>1219031.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>2566797.0</td>\n",
              "      <td>285199.660</td>\n",
              "      <td>315912.160</td>\n",
              "      <td>1062847.0</td>\n",
              "      <td>47458.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>3.014664</td>\n",
              "      <td>3.768331</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1173.0</td>\n",
              "      <td>143.631580</td>\n",
              "      <td>298.520260</td>\n",
              "      <td>89114.36000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>151.611110</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.0</td>\n",
              "      <td>15631448.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.75000</td>\n",
              "      <td>21.50000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.750865</td>\n",
              "      <td>0.383842</td>\n",
              "      <td>3.126290e+06</td>\n",
              "      <td>6.855864e+06</td>\n",
              "      <td>15389942.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>15631448.0</td>\n",
              "      <td>5210482.500</td>\n",
              "      <td>8.934060e+06</td>\n",
              "      <td>15526469.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>241454.0</td>\n",
              "      <td>241454.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>241454.0</td>\n",
              "      <td>241454.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>0.255894</td>\n",
              "      <td>0.127947</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>6.142857</td>\n",
              "      <td>16.252472</td>\n",
              "      <td>264.14285</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.166666</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.0</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>935.0</td>\n",
              "      <td>309.0</td>\n",
              "      <td>935.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>187.00000</td>\n",
              "      <td>418.14470</td>\n",
              "      <td>309.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>154.5000</td>\n",
              "      <td>218.49600</td>\n",
              "      <td>995200.000000</td>\n",
              "      <td>5600.000000</td>\n",
              "      <td>2.083333e+02</td>\n",
              "      <td>2.760034e+02</td>\n",
              "      <td>723.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>312.500</td>\n",
              "      <td>4.662492e+02</td>\n",
              "      <td>997.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>723.0</td>\n",
              "      <td>723.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>723.0</td>\n",
              "      <td>723.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>4000.000000</td>\n",
              "      <td>1600.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>935.0</td>\n",
              "      <td>155.500000</td>\n",
              "      <td>333.008800</td>\n",
              "      <td>110894.86000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>177.714280</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.0</td>\n",
              "      <td>87697.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>557.0</td>\n",
              "      <td>3782.0</td>\n",
              "      <td>206.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>61.88889</td>\n",
              "      <td>88.34371</td>\n",
              "      <td>1460.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>540.2857</td>\n",
              "      <td>654.36530</td>\n",
              "      <td>49477.177098</td>\n",
              "      <td>182.446378</td>\n",
              "      <td>5.846467e+03</td>\n",
              "      <td>1.247842e+04</td>\n",
              "      <td>47768.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>87697.0</td>\n",
              "      <td>10962.125</td>\n",
              "      <td>1.615766e+04</td>\n",
              "      <td>47782.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>75925.0</td>\n",
              "      <td>12654.167</td>\n",
              "      <td>23552.033</td>\n",
              "      <td>59778.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>102.626090</td>\n",
              "      <td>79.820290</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1460.0</td>\n",
              "      <td>255.235290</td>\n",
              "      <td>474.471220</td>\n",
              "      <td>225122.94000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>271.187500</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.0</td>\n",
              "      <td>14206.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>935.0</td>\n",
              "      <td>299.0</td>\n",
              "      <td>935.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>187.00000</td>\n",
              "      <td>418.14470</td>\n",
              "      <td>299.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>149.5000</td>\n",
              "      <td>211.42493</td>\n",
              "      <td>86864.705050</td>\n",
              "      <td>492.749542</td>\n",
              "      <td>2.367667e+03</td>\n",
              "      <td>5.541221e+03</td>\n",
              "      <td>13676.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>14206.0</td>\n",
              "      <td>3551.500</td>\n",
              "      <td>6.952886e+03</td>\n",
              "      <td>13980.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>13676.0</td>\n",
              "      <td>13676.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>13676.0</td>\n",
              "      <td>13676.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>351.963960</td>\n",
              "      <td>140.785580</td>\n",
              "      <td>0.0</td>\n",
              "      <td>935.0</td>\n",
              "      <td>154.250000</td>\n",
              "      <td>332.368440</td>\n",
              "      <td>110468.79000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>176.285720</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   protocol  flow_duration  tot_fwd_pkts  ...  down_up_ratio  pkt_size_avg  label\n",
              "0       6.0      2653695.0           8.0  ...            1.0    151.611110      0\n",
              "1       6.0     15631448.0           4.0  ...            0.0      7.166666      0\n",
              "2       6.0         1250.0           5.0  ...            0.0    177.714280      0\n",
              "3       6.0        87697.0           9.0  ...            0.0    271.187500      0\n",
              "4       6.0        14206.0           5.0  ...            0.0    176.285720      0\n",
              "\n",
              "[5 rows x 52 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4c54nFi5JDO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es0u4myzyrdy",
        "outputId": "1e77f11a-f8a2-4f77-9f25-b8a192963f5a"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data2018\n",
            "bot_02-03-2018.csv\n",
            "bruteforce-ftp-ssh_14-02-2018.csv\n",
            "bruteforce-web-xss_sql-injection_22-02-2018.csv\n",
            "bruteforce-web-xss_sql-injection_23-02-2018.csv\n",
            "ddos-loic-http-loic-udp_20-02-2018.csv\n",
            "ddos-loic-udp_hoic_21-02-2018.csv\n",
            "dos-goldeneye-slowloris_15-02-2018.csv\n",
            "dos-slowhttp-hulk_16-02-2018.csv\n",
            "infiltration_01-03-2018.csv\n",
            "infiltration_28-02-2018.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2t5TKzcu7Ck"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNMU79TpuzHW"
      },
      "source": [
        "### 9. Pre-Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98JdspTC8ytd",
        "outputId": "1e23419c-e91f-4941-dade-bcc7cea8ac7c"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Newi/2021Project3/Dataupdate/all_data2018"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3/Dataupdate/all_data2018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHIbz7Lw8Pkk",
        "outputId": "1c9b4d16-7ee1-458f-dbcb-493cb62dbcaa"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data2018.csv  catboost_info  df_new.csv  X_new.csv\ty_new.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7ydZnL-8dJD"
      },
      "source": [
        "df_new = pd.read_csv(\"df_new.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "gaIn9hhR8c5W",
        "outputId": "94d1673c-1b88-40bd-8bf9-e98894368406"
      },
      "source": [
        "df_new.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>protocol</th>\n",
              "      <th>flow_duration</th>\n",
              "      <th>tot_fwd_pkts</th>\n",
              "      <th>tot_bwd_pkts</th>\n",
              "      <th>totlen_fwd_pkts</th>\n",
              "      <th>totlen_bwd_pkts</th>\n",
              "      <th>fwd_pkt_len_max</th>\n",
              "      <th>fwd_pkt_len_min</th>\n",
              "      <th>fwd_pkt_len_mean</th>\n",
              "      <th>fwd_pkt_len_std</th>\n",
              "      <th>bwd_pkt_len_max</th>\n",
              "      <th>bwd_pkt_len_min</th>\n",
              "      <th>bwd_pkt_len_mean</th>\n",
              "      <th>bwd_pkt_len_std</th>\n",
              "      <th>flow_byts_s</th>\n",
              "      <th>flow_pkts_s</th>\n",
              "      <th>flow_iat_mean</th>\n",
              "      <th>flow_iat_std</th>\n",
              "      <th>flow_iat_max</th>\n",
              "      <th>flow_iat_min</th>\n",
              "      <th>fwd_iat_tot</th>\n",
              "      <th>fwd_iat_mean</th>\n",
              "      <th>fwd_iat_std</th>\n",
              "      <th>fwd_iat_max</th>\n",
              "      <th>fwd_iat_min</th>\n",
              "      <th>bwd_iat_tot</th>\n",
              "      <th>bwd_iat_mean</th>\n",
              "      <th>bwd_iat_std</th>\n",
              "      <th>bwd_iat_max</th>\n",
              "      <th>bwd_iat_min</th>\n",
              "      <th>fwd_psh_flags</th>\n",
              "      <th>fwd_urg_flags</th>\n",
              "      <th>fwd_header_len</th>\n",
              "      <th>bwd_header_len</th>\n",
              "      <th>fwd_pkts_s</th>\n",
              "      <th>bwd_pkts_s</th>\n",
              "      <th>pkt_len_min</th>\n",
              "      <th>pkt_len_max</th>\n",
              "      <th>pkt_len_mean</th>\n",
              "      <th>pkt_len_std</th>\n",
              "      <th>pkt_len_var</th>\n",
              "      <th>fin_flag_cnt</th>\n",
              "      <th>syn_flag_cnt</th>\n",
              "      <th>rst_flag_cnt</th>\n",
              "      <th>psh_flag_cnt</th>\n",
              "      <th>ack_flag_cnt</th>\n",
              "      <th>urg_flag_cnt</th>\n",
              "      <th>cwe_flag_count</th>\n",
              "      <th>ece_flag_cnt</th>\n",
              "      <th>down_up_ratio</th>\n",
              "      <th>pkt_size_avg</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>2653695.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1148.0</td>\n",
              "      <td>1581.0</td>\n",
              "      <td>677.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>143.50</td>\n",
              "      <td>228.12967</td>\n",
              "      <td>1173.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>158.1</td>\n",
              "      <td>367.7307</td>\n",
              "      <td>1028.377413</td>\n",
              "      <td>6.782995</td>\n",
              "      <td>1.560997e+05</td>\n",
              "      <td>2.540860e+05</td>\n",
              "      <td>1062847.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>2653695.0</td>\n",
              "      <td>379099.28</td>\n",
              "      <td>4.393966e+05</td>\n",
              "      <td>1219031.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>2566797.0</td>\n",
              "      <td>285199.66</td>\n",
              "      <td>315912.16</td>\n",
              "      <td>1062847.0</td>\n",
              "      <td>47458.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>3.014664</td>\n",
              "      <td>3.768331</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1173.0</td>\n",
              "      <td>143.631580</td>\n",
              "      <td>298.520260</td>\n",
              "      <td>89114.36000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>151.611110</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.0</td>\n",
              "      <td>15631448.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.75</td>\n",
              "      <td>21.50000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>2.750865</td>\n",
              "      <td>0.383842</td>\n",
              "      <td>3.126290e+06</td>\n",
              "      <td>6.855864e+06</td>\n",
              "      <td>15389942.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>15631448.0</td>\n",
              "      <td>5210482.50</td>\n",
              "      <td>8.934060e+06</td>\n",
              "      <td>15526469.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>241454.0</td>\n",
              "      <td>241454.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>241454.0</td>\n",
              "      <td>241454.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>0.255894</td>\n",
              "      <td>0.127947</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>6.142857</td>\n",
              "      <td>16.252472</td>\n",
              "      <td>264.14285</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.166666</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.0</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>935.0</td>\n",
              "      <td>309.0</td>\n",
              "      <td>935.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>187.00</td>\n",
              "      <td>418.14470</td>\n",
              "      <td>309.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>154.5</td>\n",
              "      <td>218.4960</td>\n",
              "      <td>995200.000000</td>\n",
              "      <td>5600.000000</td>\n",
              "      <td>2.083333e+02</td>\n",
              "      <td>2.760034e+02</td>\n",
              "      <td>723.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>312.50</td>\n",
              "      <td>4.662492e+02</td>\n",
              "      <td>997.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>723.0</td>\n",
              "      <td>723.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>723.0</td>\n",
              "      <td>723.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>4000.000000</td>\n",
              "      <td>1600.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>935.0</td>\n",
              "      <td>155.500000</td>\n",
              "      <td>333.008800</td>\n",
              "      <td>110894.86000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>177.714280</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   protocol  flow_duration  tot_fwd_pkts  ...  down_up_ratio  pkt_size_avg  label\n",
              "0       6.0      2653695.0           8.0  ...            1.0    151.611110      0\n",
              "1       6.0     15631448.0           4.0  ...            0.0      7.166666      0\n",
              "2       6.0         1250.0           5.0  ...            0.0    177.714280      0\n",
              "\n",
              "[3 rows x 52 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge8Nu1Bk9uoy"
      },
      "source": [
        "# X = df.drop(['label'],axis=1).values \n",
        "X_df = df_new.drop(columns=['label'])\n",
        "y_df = df_new.iloc[:, -1].values.reshape(-1,1)\n",
        "y_df = np.ravel(y_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1NWcgDR8dBi"
      },
      "source": [
        "#Min-max normalization\n",
        "numeric_features = X_df.dtypes[X_df.dtypes != 'object'].index\n",
        "X_df[numeric_features] = X_df[numeric_features].apply(\n",
        "    lambda x: (x - x.min()) / (x.max()-x.min()))\n",
        "\n",
        "#Fill empty values by 0\n",
        "X_df = X_df.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "e3_hpLNZ93Nj",
        "outputId": "b6dc2a99-805a-42a9-a1e0-1b73ba826474"
      },
      "source": [
        "X_df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>protocol</th>\n",
              "      <th>flow_duration</th>\n",
              "      <th>tot_fwd_pkts</th>\n",
              "      <th>tot_bwd_pkts</th>\n",
              "      <th>totlen_fwd_pkts</th>\n",
              "      <th>totlen_bwd_pkts</th>\n",
              "      <th>fwd_pkt_len_max</th>\n",
              "      <th>fwd_pkt_len_min</th>\n",
              "      <th>fwd_pkt_len_mean</th>\n",
              "      <th>fwd_pkt_len_std</th>\n",
              "      <th>bwd_pkt_len_max</th>\n",
              "      <th>bwd_pkt_len_min</th>\n",
              "      <th>bwd_pkt_len_mean</th>\n",
              "      <th>bwd_pkt_len_std</th>\n",
              "      <th>flow_byts_s</th>\n",
              "      <th>flow_pkts_s</th>\n",
              "      <th>flow_iat_mean</th>\n",
              "      <th>flow_iat_std</th>\n",
              "      <th>flow_iat_max</th>\n",
              "      <th>flow_iat_min</th>\n",
              "      <th>fwd_iat_tot</th>\n",
              "      <th>fwd_iat_mean</th>\n",
              "      <th>fwd_iat_std</th>\n",
              "      <th>fwd_iat_max</th>\n",
              "      <th>fwd_iat_min</th>\n",
              "      <th>bwd_iat_tot</th>\n",
              "      <th>bwd_iat_mean</th>\n",
              "      <th>bwd_iat_std</th>\n",
              "      <th>bwd_iat_max</th>\n",
              "      <th>bwd_iat_min</th>\n",
              "      <th>fwd_psh_flags</th>\n",
              "      <th>fwd_urg_flags</th>\n",
              "      <th>fwd_header_len</th>\n",
              "      <th>bwd_header_len</th>\n",
              "      <th>fwd_pkts_s</th>\n",
              "      <th>bwd_pkts_s</th>\n",
              "      <th>pkt_len_min</th>\n",
              "      <th>pkt_len_max</th>\n",
              "      <th>pkt_len_mean</th>\n",
              "      <th>pkt_len_std</th>\n",
              "      <th>pkt_len_var</th>\n",
              "      <th>fin_flag_cnt</th>\n",
              "      <th>syn_flag_cnt</th>\n",
              "      <th>rst_flag_cnt</th>\n",
              "      <th>psh_flag_cnt</th>\n",
              "      <th>ack_flag_cnt</th>\n",
              "      <th>urg_flag_cnt</th>\n",
              "      <th>cwe_flag_count</th>\n",
              "      <th>ece_flag_cnt</th>\n",
              "      <th>down_up_ratio</th>\n",
              "      <th>pkt_size_avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.022114</td>\n",
              "      <td>0.001108</td>\n",
              "      <td>0.000878</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.463699</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.099363</td>\n",
              "      <td>0.302522</td>\n",
              "      <td>0.803425</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.108456</td>\n",
              "      <td>0.468985</td>\n",
              "      <td>7.971918e-06</td>\n",
              "      <td>2.255224e-06</td>\n",
              "      <td>0.001354</td>\n",
              "      <td>0.003476</td>\n",
              "      <td>0.008880</td>\n",
              "      <td>5.202496e-07</td>\n",
              "      <td>0.022114</td>\n",
              "      <td>0.003287</td>\n",
              "      <td>0.005202</td>\n",
              "      <td>0.010185</td>\n",
              "      <td>1.985619e-06</td>\n",
              "      <td>0.021390</td>\n",
              "      <td>0.003423</td>\n",
              "      <td>0.003855</td>\n",
              "      <td>0.009170</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001150</td>\n",
              "      <td>0.000930</td>\n",
              "      <td>1.004888e-06</td>\n",
              "      <td>1.884165e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.803425</td>\n",
              "      <td>0.099606</td>\n",
              "      <td>0.400175</td>\n",
              "      <td>0.160140</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.006173</td>\n",
              "      <td>0.104977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.130262</td>\n",
              "      <td>0.000475</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029452</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007444</td>\n",
              "      <td>0.028511</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.132453e-08</td>\n",
              "      <td>1.221728e-07</td>\n",
              "      <td>0.027108</td>\n",
              "      <td>0.093791</td>\n",
              "      <td>0.128584</td>\n",
              "      <td>3.381622e-07</td>\n",
              "      <td>0.130262</td>\n",
              "      <td>0.045179</td>\n",
              "      <td>0.105778</td>\n",
              "      <td>0.129725</td>\n",
              "      <td>3.381622e-07</td>\n",
              "      <td>0.002012</td>\n",
              "      <td>0.002898</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>0.002898</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>8.529813e-08</td>\n",
              "      <td>6.397360e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.029452</td>\n",
              "      <td>0.004260</td>\n",
              "      <td>0.021787</td>\n",
              "      <td>0.000475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004962</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   protocol  flow_duration  ...  down_up_ratio  pkt_size_avg\n",
              "0  0.352941       0.022114  ...       0.006173      0.104977\n",
              "1  0.352941       0.130262  ...       0.000000      0.004962\n",
              "\n",
              "[2 rows x 51 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iImVbxLP93Ke"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF1Rw562_G1i"
      },
      "source": [
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X_df,y_df, train_size = 0.8, test_size = 0.2, random_state = 0, stratify = y_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nbcZVqh93H9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqJnFIjh-dwC"
      },
      "source": [
        "### 10. Hyperparameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyXiaKPDU5sk"
      },
      "source": [
        "#### 10.1 RF + Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am36cQAO-01k"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7QDvSb3WCPH",
        "outputId": "fd031afc-f333-480f-e15a-f4d58f1c120c"
      },
      "source": [
        "random_search = RandomizedSearchCV(RandomForestClassifier(random_state=0),\n",
        "                           {\n",
        "                              'n_estimators':np.arange(5,100,5),\n",
        "                              'max_features':np.arange(0.1,1.0,0.05),\n",
        "                            },cv=5, scoring=\"r2\",verbose=1,n_jobs=-1, \n",
        "                             n_iter=50, random_state = 0\n",
        "                           )\n",
        "random_search.fit(X_train5,y_train5)\n",
        "\n",
        "random_search.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_features': 0.25000000000000006, 'n_estimators': 45}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMhs770C93AD"
      },
      "source": [
        "#rf_params = {'max_features': 0.9500000000000003, 'n_estimators': 55}\n",
        "#rf_params = {'max_features': 0.6000000000000002, 'n_estimators': 85}\n",
        "rf_params = {'max_features': 0.25000000000000006, 'n_estimators': 45}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTa96QXU929W",
        "outputId": "1abe3f7f-9eb0-4617-c6f1-d3f1278b5ae5"
      },
      "source": [
        "rf = RandomForestClassifier(max_features = 0.25000000000000006, n_estimators = 45)\n",
        "\n",
        "build_model_new (rf, X_train5 , y_train5 , X_test5 , y_test5)\n",
        "rf_train_param , rf_test_param = save_model_new(rf,'rf_model_para', X_train5 , y_train5 , X_test5 , y_test5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None,\n",
            "                       max_features=0.25000000000000006, max_leaf_nodes=None,\n",
            "                       max_samples=None, min_impurity_decrease=0.0,\n",
            "                       min_impurity_split=None, min_samples_leaf=1,\n",
            "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                       n_estimators=45, n_jobs=None, oob_score=False,\n",
            "                       random_state=None, verbose=0, warm_start=False)\n",
            "Accuracy of Model: 0.9860485651214128\n",
            "Precision of Model: 0.9879521287749679\n",
            "Recall of Model: 0.9860485651214128\n",
            "F1-score of Model: 0.9859596452146835\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       0.86      1.00      0.93      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      0.84      0.91      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           0.99     33975\n",
            "   macro avg       0.99      0.99      0.99     33975\n",
            "weighted avg       0.99      0.99      0.99     33975\n",
            "\n",
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  rf_model_para.sav\n",
            "Accuracy Model:  0.9860485651214128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeFqAbuTkHsd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J2nORvtVHba"
      },
      "source": [
        "#### 10.2 XG + Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PppieKcxnRhz"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HPAu1hMm2Lo"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "folds = 3 ; param_comb = 5\n",
        "xgb = XGBClassifier(learning_rate=0.02, n_estimators=600)\n",
        "\n",
        "# A Parameter Grid for XGBoost\n",
        "params = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'max_depth': [3, 4, 5]\n",
        "        }\n",
        "\n",
        "#skf = StratifiedKFold(n_splits = folds, shuffle = True, random_state = 1001)\n",
        "xgb_search = RandomizedSearchCV(xgb, param_distributions = params, n_iter = param_comb, n_jobs=4, verbose=3, random_state=0)\n",
        "\n",
        "xgb_search.fit(X_train5,y_train5) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A44r2VesMfzX"
      },
      "source": [
        "xgb_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e6iY1rzSgYb"
      },
      "source": [
        "#xg_params = {'max_features': 0.40000000000000013, 'n_estimators': 95}\n",
        "xg_params = {'colsample_bytree': 0.8,\n",
        "             'gamma': 2,\n",
        "             'max_depth': 5,\n",
        "             'min_child_weight': 5,\n",
        "             'subsample': 0.8}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqhNUP2FmcAk"
      },
      "source": [
        "xg = XGBClassifier (colsample_bytree = 0.8, gamma = 2 , max_depth = 5, min_child_weight = 5, subsample = 0.8)\n",
        "\n",
        "build_model_new (xg, X_train5 , y_train5 , X_test5 , y_test5)\n",
        "xg_train_param , xg_test_param = save_model_new(xg,'xg_model_para', X_train5 , y_train5 , X_test5 , y_test5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PrcHC0wMfuz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quud7ZacYiRT"
      },
      "source": [
        "#### 10.3 Cat + Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjA_pJ7XsY8V"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from scipy.stats import uniform as sp_randFloat\n",
        "from scipy.stats import randint as sp_randInt  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs7mrAiowejj"
      },
      "source": [
        "grid = {'learning_rate': [0.03, 0.1],\n",
        "        'depth': [4, 6, 10],\n",
        "        'l2_leaf_reg': [1, 3, 5, 7, 9]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km8NQM9IudJ7",
        "outputId": "aa852ba7-3672-416d-ad3c-e1d625f74153"
      },
      "source": [
        "cat = CatBoostClassifier()\n",
        "parameters = {'depth'         : sp_randInt(4, 10),\n",
        "              'learning_rate' : sp_randFloat(),\n",
        "              'iterations'    : sp_randInt(10, 140)\n",
        "              }\n",
        "\n",
        "cat_search = RandomizedSearchCV(estimator=cat, param_distributions = parameters, cv = 2, n_iter = 10, n_jobs=-1)\n",
        "cat_search.fit(X_train5, y_train5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.5680655\ttotal: 132ms\tremaining: 16.3s\n",
            "1:\tlearn: 0.3515091\ttotal: 249ms\tremaining: 15.2s\n",
            "2:\tlearn: 0.2547334\ttotal: 371ms\tremaining: 15s\n",
            "3:\tlearn: 0.1688755\ttotal: 491ms\tremaining: 14.7s\n",
            "4:\tlearn: 0.1160511\ttotal: 611ms\tremaining: 14.6s\n",
            "5:\tlearn: 0.0944144\ttotal: 730ms\tremaining: 14.3s\n",
            "6:\tlearn: 0.0798211\ttotal: 846ms\tremaining: 14.1s\n",
            "7:\tlearn: 0.0699876\ttotal: 968ms\tremaining: 14s\n",
            "8:\tlearn: 0.0629264\ttotal: 1.08s\tremaining: 13.8s\n",
            "9:\tlearn: 0.0588544\ttotal: 1.2s\tremaining: 13.7s\n",
            "10:\tlearn: 0.0544495\ttotal: 1.31s\tremaining: 13.5s\n",
            "11:\tlearn: 0.0507548\ttotal: 1.43s\tremaining: 13.4s\n",
            "12:\tlearn: 0.0490623\ttotal: 1.56s\tremaining: 13.3s\n",
            "13:\tlearn: 0.0481178\ttotal: 1.67s\tremaining: 13.1s\n",
            "14:\tlearn: 0.0461105\ttotal: 1.79s\tremaining: 13s\n",
            "15:\tlearn: 0.0452035\ttotal: 1.91s\tremaining: 12.9s\n",
            "16:\tlearn: 0.0443365\ttotal: 2.03s\tremaining: 12.8s\n",
            "17:\tlearn: 0.0437168\ttotal: 2.16s\tremaining: 12.7s\n",
            "18:\tlearn: 0.0430725\ttotal: 2.28s\tremaining: 12.6s\n",
            "19:\tlearn: 0.0418632\ttotal: 2.39s\tremaining: 12.4s\n",
            "20:\tlearn: 0.0414273\ttotal: 2.51s\tremaining: 12.3s\n",
            "21:\tlearn: 0.0410092\ttotal: 2.63s\tremaining: 12.2s\n",
            "22:\tlearn: 0.0406743\ttotal: 2.75s\tremaining: 12.1s\n",
            "23:\tlearn: 0.0404692\ttotal: 2.87s\tremaining: 12s\n",
            "24:\tlearn: 0.0402923\ttotal: 2.99s\tremaining: 11.8s\n",
            "25:\tlearn: 0.0400384\ttotal: 3.11s\tremaining: 11.7s\n",
            "26:\tlearn: 0.0396201\ttotal: 3.24s\tremaining: 11.6s\n",
            "27:\tlearn: 0.0394646\ttotal: 3.36s\tremaining: 11.5s\n",
            "28:\tlearn: 0.0391630\ttotal: 3.49s\tremaining: 11.4s\n",
            "29:\tlearn: 0.0391043\ttotal: 3.6s\tremaining: 11.3s\n",
            "30:\tlearn: 0.0389467\ttotal: 3.72s\tremaining: 11.2s\n",
            "31:\tlearn: 0.0387222\ttotal: 3.84s\tremaining: 11s\n",
            "32:\tlearn: 0.0385071\ttotal: 3.96s\tremaining: 10.9s\n",
            "33:\tlearn: 0.0383529\ttotal: 4.08s\tremaining: 10.8s\n",
            "34:\tlearn: 0.0382979\ttotal: 4.2s\tremaining: 10.7s\n",
            "35:\tlearn: 0.0382315\ttotal: 4.33s\tremaining: 10.6s\n",
            "36:\tlearn: 0.0381391\ttotal: 4.44s\tremaining: 10.4s\n",
            "37:\tlearn: 0.0380590\ttotal: 4.55s\tremaining: 10.3s\n",
            "38:\tlearn: 0.0380110\ttotal: 4.67s\tremaining: 10.2s\n",
            "39:\tlearn: 0.0379760\ttotal: 4.79s\tremaining: 10.1s\n",
            "40:\tlearn: 0.0379299\ttotal: 4.91s\tremaining: 9.94s\n",
            "41:\tlearn: 0.0379003\ttotal: 5.02s\tremaining: 9.81s\n",
            "42:\tlearn: 0.0378711\ttotal: 5.14s\tremaining: 9.68s\n",
            "43:\tlearn: 0.0378527\ttotal: 5.26s\tremaining: 9.56s\n",
            "44:\tlearn: 0.0378437\ttotal: 5.37s\tremaining: 9.44s\n",
            "45:\tlearn: 0.0378016\ttotal: 5.49s\tremaining: 9.31s\n",
            "46:\tlearn: 0.0377876\ttotal: 5.61s\tremaining: 9.18s\n",
            "47:\tlearn: 0.0377578\ttotal: 5.72s\tremaining: 9.06s\n",
            "48:\tlearn: 0.0377257\ttotal: 5.84s\tremaining: 8.94s\n",
            "49:\tlearn: 0.0377009\ttotal: 5.95s\tremaining: 8.81s\n",
            "50:\tlearn: 0.0376573\ttotal: 6.07s\tremaining: 8.69s\n",
            "51:\tlearn: 0.0376457\ttotal: 6.19s\tremaining: 8.57s\n",
            "52:\tlearn: 0.0376203\ttotal: 6.3s\tremaining: 8.45s\n",
            "53:\tlearn: 0.0376002\ttotal: 6.42s\tremaining: 8.32s\n",
            "54:\tlearn: 0.0375751\ttotal: 6.54s\tremaining: 8.2s\n",
            "55:\tlearn: 0.0375559\ttotal: 6.65s\tremaining: 8.08s\n",
            "56:\tlearn: 0.0375529\ttotal: 6.77s\tremaining: 7.96s\n",
            "57:\tlearn: 0.0375003\ttotal: 6.89s\tremaining: 7.84s\n",
            "58:\tlearn: 0.0374862\ttotal: 7.01s\tremaining: 7.72s\n",
            "59:\tlearn: 0.0374810\ttotal: 7.12s\tremaining: 7.6s\n",
            "60:\tlearn: 0.0374627\ttotal: 7.24s\tremaining: 7.48s\n",
            "61:\tlearn: 0.0374488\ttotal: 7.36s\tremaining: 7.36s\n",
            "62:\tlearn: 0.0374454\ttotal: 7.47s\tremaining: 7.23s\n",
            "63:\tlearn: 0.0374431\ttotal: 7.58s\tremaining: 7.11s\n",
            "64:\tlearn: 0.0374355\ttotal: 7.7s\tremaining: 6.99s\n",
            "65:\tlearn: 0.0374220\ttotal: 7.82s\tremaining: 6.87s\n",
            "66:\tlearn: 0.0374081\ttotal: 7.94s\tremaining: 6.76s\n",
            "67:\tlearn: 0.0373947\ttotal: 8.06s\tremaining: 6.64s\n",
            "68:\tlearn: 0.0373892\ttotal: 8.18s\tremaining: 6.52s\n",
            "69:\tlearn: 0.0373784\ttotal: 8.31s\tremaining: 6.41s\n",
            "70:\tlearn: 0.0373759\ttotal: 8.42s\tremaining: 6.28s\n",
            "71:\tlearn: 0.0373635\ttotal: 8.53s\tremaining: 6.16s\n",
            "72:\tlearn: 0.0373560\ttotal: 8.65s\tremaining: 6.05s\n",
            "73:\tlearn: 0.0373506\ttotal: 8.78s\tremaining: 5.93s\n",
            "74:\tlearn: 0.0373436\ttotal: 8.89s\tremaining: 5.81s\n",
            "75:\tlearn: 0.0373311\ttotal: 9.01s\tremaining: 5.69s\n",
            "76:\tlearn: 0.0373249\ttotal: 9.14s\tremaining: 5.58s\n",
            "77:\tlearn: 0.0373123\ttotal: 9.26s\tremaining: 5.46s\n",
            "78:\tlearn: 0.0373057\ttotal: 9.38s\tremaining: 5.34s\n",
            "79:\tlearn: 0.0372981\ttotal: 9.5s\tremaining: 5.22s\n",
            "80:\tlearn: 0.0372963\ttotal: 9.62s\tremaining: 5.1s\n",
            "81:\tlearn: 0.0372866\ttotal: 9.73s\tremaining: 4.98s\n",
            "82:\tlearn: 0.0372778\ttotal: 9.84s\tremaining: 4.86s\n",
            "83:\tlearn: 0.0372660\ttotal: 9.96s\tremaining: 4.74s\n",
            "84:\tlearn: 0.0372612\ttotal: 10.1s\tremaining: 4.62s\n",
            "85:\tlearn: 0.0372545\ttotal: 10.2s\tremaining: 4.5s\n",
            "86:\tlearn: 0.0372525\ttotal: 10.3s\tremaining: 4.38s\n",
            "87:\tlearn: 0.0372502\ttotal: 10.4s\tremaining: 4.26s\n",
            "88:\tlearn: 0.0372446\ttotal: 10.5s\tremaining: 4.15s\n",
            "89:\tlearn: 0.0372399\ttotal: 10.7s\tremaining: 4.03s\n",
            "90:\tlearn: 0.0372384\ttotal: 10.8s\tremaining: 3.9s\n",
            "91:\tlearn: 0.0372324\ttotal: 10.9s\tremaining: 3.79s\n",
            "92:\tlearn: 0.0372308\ttotal: 11s\tremaining: 3.67s\n",
            "93:\tlearn: 0.0372186\ttotal: 11.1s\tremaining: 3.55s\n",
            "94:\tlearn: 0.0372152\ttotal: 11.2s\tremaining: 3.43s\n",
            "95:\tlearn: 0.0371976\ttotal: 11.4s\tremaining: 3.31s\n",
            "96:\tlearn: 0.0371915\ttotal: 11.5s\tremaining: 3.19s\n",
            "97:\tlearn: 0.0371893\ttotal: 11.6s\tremaining: 3.07s\n",
            "98:\tlearn: 0.0371869\ttotal: 11.7s\tremaining: 2.95s\n",
            "99:\tlearn: 0.0371822\ttotal: 11.8s\tremaining: 2.83s\n",
            "100:\tlearn: 0.0371813\ttotal: 11.9s\tremaining: 2.71s\n",
            "101:\tlearn: 0.0371751\ttotal: 12s\tremaining: 2.6s\n",
            "102:\tlearn: 0.0371735\ttotal: 12.2s\tremaining: 2.48s\n",
            "103:\tlearn: 0.0371700\ttotal: 12.3s\tremaining: 2.36s\n",
            "104:\tlearn: 0.0371679\ttotal: 12.4s\tremaining: 2.24s\n",
            "105:\tlearn: 0.0371661\ttotal: 12.5s\tremaining: 2.12s\n",
            "106:\tlearn: 0.0371606\ttotal: 12.6s\tremaining: 2.01s\n",
            "107:\tlearn: 0.0371578\ttotal: 12.8s\tremaining: 1.89s\n",
            "108:\tlearn: 0.0371575\ttotal: 12.9s\tremaining: 1.77s\n",
            "109:\tlearn: 0.0371540\ttotal: 13s\tremaining: 1.66s\n",
            "110:\tlearn: 0.0371532\ttotal: 13.1s\tremaining: 1.54s\n",
            "111:\tlearn: 0.0371505\ttotal: 13.3s\tremaining: 1.42s\n",
            "112:\tlearn: 0.0371458\ttotal: 13.4s\tremaining: 1.3s\n",
            "113:\tlearn: 0.0371437\ttotal: 13.5s\tremaining: 1.18s\n",
            "114:\tlearn: 0.0371429\ttotal: 13.6s\tremaining: 1.06s\n",
            "115:\tlearn: 0.0371406\ttotal: 13.7s\tremaining: 946ms\n",
            "116:\tlearn: 0.0371396\ttotal: 13.8s\tremaining: 828ms\n",
            "117:\tlearn: 0.0371338\ttotal: 13.9s\tremaining: 709ms\n",
            "118:\tlearn: 0.0371299\ttotal: 14.1s\tremaining: 591ms\n",
            "119:\tlearn: 0.0371292\ttotal: 14.2s\tremaining: 473ms\n",
            "120:\tlearn: 0.0371248\ttotal: 14.3s\tremaining: 354ms\n",
            "121:\tlearn: 0.0371232\ttotal: 14.4s\tremaining: 236ms\n",
            "122:\tlearn: 0.0371216\ttotal: 14.5s\tremaining: 118ms\n",
            "123:\tlearn: 0.0371200\ttotal: 14.6s\tremaining: 0us\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=2, error_score=nan,\n",
              "                   estimator=<catboost.core.CatBoostClassifier object at 0x7fe5a44b8310>,\n",
              "                   iid='deprecated', n_iter=10, n_jobs=-1,\n",
              "                   param_distributions={'depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe5a44b85d0>,\n",
              "                                        'iterations': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe5a44c88d0>,\n",
              "                                        'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe5a44b80d0>},\n",
              "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0rbqaQtvCTw",
        "outputId": "e7ef4b49-eea8-42de-f271-3ef76df0bd5e"
      },
      "source": [
        "print (\"Best Estimator : \" , cat_search.best_estimator_)\n",
        "print (\"Best Params : \" , cat_search.best_params_)\n",
        "print (\"Best Score : \" , cat_search.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Estimator :  <catboost.core.CatBoostClassifier object at 0x7fe5a44d4910>\n",
            "Best Params :  {'depth': 6, 'iterations': 103, 'learning_rate': 0.6833949363478163}\n",
            "Best Score :  0.9857174392935982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjE88yYBvCK-"
      },
      "source": [
        "#bestTest = 0.03697023278\n",
        "#'depth': 10, 'l2_leaf_reg': 1, 'learning_rate': 0.1, 'Iteration': 999}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doCJPGEk6WdO",
        "outputId": "dd195657-0709-4e26-aa57-8f15e47ef890"
      },
      "source": [
        "cat = CatBoostClassifier (depth = 10, iterations = 999 , learning_rate = 0.1 , l2_leaf_reg = 1 )\n",
        "\n",
        "build_model_new (cat, X_train5 , y_train5 , X_test5 , y_test5)\n",
        "cat_train_param , cat_test_param = save_model_new(cat,'cat_model_para', X_train5 , y_train5 , X_test5 , y_test5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<catboost.core.CatBoostClassifier object at 0x7fe5a446fc90>\n",
            "0:\tlearn: 1.5973741\ttotal: 629ms\tremaining: 10m 28s\n",
            "1:\tlearn: 1.3177550\ttotal: 1.05s\tremaining: 8m 42s\n",
            "2:\tlearn: 1.1013161\ttotal: 1.47s\tremaining: 8m 8s\n",
            "3:\tlearn: 0.9391367\ttotal: 1.89s\tremaining: 7m 50s\n",
            "4:\tlearn: 0.8176408\ttotal: 2.3s\tremaining: 7m 37s\n",
            "5:\tlearn: 0.7202423\ttotal: 2.73s\tremaining: 7m 31s\n",
            "6:\tlearn: 0.6381896\ttotal: 3.16s\tremaining: 7m 28s\n",
            "7:\tlearn: 0.5676678\ttotal: 3.59s\tremaining: 7m 24s\n",
            "8:\tlearn: 0.5095470\ttotal: 4.01s\tremaining: 7m 21s\n",
            "9:\tlearn: 0.4578925\ttotal: 4.43s\tremaining: 7m 17s\n",
            "10:\tlearn: 0.4154545\ttotal: 4.84s\tremaining: 7m 14s\n",
            "11:\tlearn: 0.3768529\ttotal: 5.25s\tremaining: 7m 12s\n",
            "12:\tlearn: 0.3435940\ttotal: 5.68s\tremaining: 7m 10s\n",
            "13:\tlearn: 0.3146415\ttotal: 6.1s\tremaining: 7m 9s\n",
            "14:\tlearn: 0.2883819\ttotal: 6.52s\tremaining: 7m 7s\n",
            "15:\tlearn: 0.2639328\ttotal: 6.94s\tremaining: 7m 6s\n",
            "16:\tlearn: 0.2432605\ttotal: 7.37s\tremaining: 7m 5s\n",
            "17:\tlearn: 0.2249160\ttotal: 7.79s\tremaining: 7m 4s\n",
            "18:\tlearn: 0.2086757\ttotal: 8.21s\tremaining: 7m 3s\n",
            "19:\tlearn: 0.1937615\ttotal: 8.65s\tremaining: 7m 3s\n",
            "20:\tlearn: 0.1801498\ttotal: 9.06s\tremaining: 7m 2s\n",
            "21:\tlearn: 0.1680523\ttotal: 9.49s\tremaining: 7m 1s\n",
            "22:\tlearn: 0.1569869\ttotal: 9.91s\tremaining: 7m\n",
            "23:\tlearn: 0.1476093\ttotal: 10.3s\tremaining: 6m 59s\n",
            "24:\tlearn: 0.1384638\ttotal: 10.7s\tremaining: 6m 58s\n",
            "25:\tlearn: 0.1301839\ttotal: 11.2s\tremaining: 6m 58s\n",
            "26:\tlearn: 0.1234877\ttotal: 11.6s\tremaining: 6m 57s\n",
            "27:\tlearn: 0.1176443\ttotal: 12s\tremaining: 6m 57s\n",
            "28:\tlearn: 0.1113204\ttotal: 12.5s\tremaining: 6m 56s\n",
            "29:\tlearn: 0.1054544\ttotal: 12.9s\tremaining: 6m 56s\n",
            "30:\tlearn: 0.1008215\ttotal: 13.3s\tremaining: 6m 55s\n",
            "31:\tlearn: 0.0962345\ttotal: 13.7s\tremaining: 6m 55s\n",
            "32:\tlearn: 0.0922777\ttotal: 14.2s\tremaining: 6m 54s\n",
            "33:\tlearn: 0.0883250\ttotal: 14.6s\tremaining: 6m 53s\n",
            "34:\tlearn: 0.0853997\ttotal: 15s\tremaining: 6m 53s\n",
            "35:\tlearn: 0.0826831\ttotal: 15.4s\tremaining: 6m 52s\n",
            "36:\tlearn: 0.0794657\ttotal: 15.9s\tremaining: 6m 52s\n",
            "37:\tlearn: 0.0767063\ttotal: 16.3s\tremaining: 6m 51s\n",
            "38:\tlearn: 0.0748295\ttotal: 16.7s\tremaining: 6m 51s\n",
            "39:\tlearn: 0.0725304\ttotal: 17.1s\tremaining: 6m 50s\n",
            "40:\tlearn: 0.0701987\ttotal: 17.6s\tremaining: 6m 50s\n",
            "41:\tlearn: 0.0686047\ttotal: 18s\tremaining: 6m 49s\n",
            "42:\tlearn: 0.0667598\ttotal: 18.4s\tremaining: 6m 49s\n",
            "43:\tlearn: 0.0649467\ttotal: 18.9s\tremaining: 6m 49s\n",
            "44:\tlearn: 0.0632266\ttotal: 19.3s\tremaining: 6m 48s\n",
            "45:\tlearn: 0.0618041\ttotal: 19.7s\tremaining: 6m 48s\n",
            "46:\tlearn: 0.0603271\ttotal: 20.1s\tremaining: 6m 47s\n",
            "47:\tlearn: 0.0594851\ttotal: 20.6s\tremaining: 6m 47s\n",
            "48:\tlearn: 0.0582179\ttotal: 21s\tremaining: 6m 46s\n",
            "49:\tlearn: 0.0573376\ttotal: 21.4s\tremaining: 6m 45s\n",
            "50:\tlearn: 0.0563551\ttotal: 21.8s\tremaining: 6m 45s\n",
            "51:\tlearn: 0.0554731\ttotal: 22.2s\tremaining: 6m 45s\n",
            "52:\tlearn: 0.0546480\ttotal: 22.7s\tremaining: 6m 44s\n",
            "53:\tlearn: 0.0538289\ttotal: 23.1s\tremaining: 6m 44s\n",
            "54:\tlearn: 0.0530269\ttotal: 23.5s\tremaining: 6m 43s\n",
            "55:\tlearn: 0.0522946\ttotal: 24s\tremaining: 6m 43s\n",
            "56:\tlearn: 0.0516804\ttotal: 24.4s\tremaining: 6m 43s\n",
            "57:\tlearn: 0.0509504\ttotal: 24.8s\tremaining: 6m 43s\n",
            "58:\tlearn: 0.0503345\ttotal: 25.3s\tremaining: 6m 42s\n",
            "59:\tlearn: 0.0498152\ttotal: 25.7s\tremaining: 6m 42s\n",
            "60:\tlearn: 0.0491022\ttotal: 26.1s\tremaining: 6m 41s\n",
            "61:\tlearn: 0.0485864\ttotal: 26.6s\tremaining: 6m 41s\n",
            "62:\tlearn: 0.0480307\ttotal: 27s\tremaining: 6m 41s\n",
            "63:\tlearn: 0.0476455\ttotal: 27.4s\tremaining: 6m 40s\n",
            "64:\tlearn: 0.0472466\ttotal: 27.9s\tremaining: 6m 40s\n",
            "65:\tlearn: 0.0468175\ttotal: 28.3s\tremaining: 6m 40s\n",
            "66:\tlearn: 0.0463811\ttotal: 28.7s\tremaining: 6m 39s\n",
            "67:\tlearn: 0.0460371\ttotal: 29.2s\tremaining: 6m 39s\n",
            "68:\tlearn: 0.0456939\ttotal: 29.6s\tremaining: 6m 38s\n",
            "69:\tlearn: 0.0453643\ttotal: 30s\tremaining: 6m 38s\n",
            "70:\tlearn: 0.0450017\ttotal: 30.4s\tremaining: 6m 37s\n",
            "71:\tlearn: 0.0447018\ttotal: 30.9s\tremaining: 6m 37s\n",
            "72:\tlearn: 0.0444155\ttotal: 31.3s\tremaining: 6m 37s\n",
            "73:\tlearn: 0.0441866\ttotal: 31.7s\tremaining: 6m 36s\n",
            "74:\tlearn: 0.0439607\ttotal: 32.1s\tremaining: 6m 35s\n",
            "75:\tlearn: 0.0436709\ttotal: 32.6s\tremaining: 6m 35s\n",
            "76:\tlearn: 0.0434137\ttotal: 33s\tremaining: 6m 34s\n",
            "77:\tlearn: 0.0431617\ttotal: 33.4s\tremaining: 6m 34s\n",
            "78:\tlearn: 0.0429688\ttotal: 33.8s\tremaining: 6m 34s\n",
            "79:\tlearn: 0.0428029\ttotal: 34.3s\tremaining: 6m 33s\n",
            "80:\tlearn: 0.0425540\ttotal: 34.7s\tremaining: 6m 33s\n",
            "81:\tlearn: 0.0424229\ttotal: 35.1s\tremaining: 6m 32s\n",
            "82:\tlearn: 0.0422816\ttotal: 35.5s\tremaining: 6m 32s\n",
            "83:\tlearn: 0.0421262\ttotal: 36s\tremaining: 6m 31s\n",
            "84:\tlearn: 0.0419538\ttotal: 36.4s\tremaining: 6m 31s\n",
            "85:\tlearn: 0.0417982\ttotal: 36.8s\tremaining: 6m 30s\n",
            "86:\tlearn: 0.0416893\ttotal: 37.3s\tremaining: 6m 30s\n",
            "87:\tlearn: 0.0413223\ttotal: 37.7s\tremaining: 6m 30s\n",
            "88:\tlearn: 0.0412146\ttotal: 38.1s\tremaining: 6m 29s\n",
            "89:\tlearn: 0.0410520\ttotal: 38.5s\tremaining: 6m 29s\n",
            "90:\tlearn: 0.0409577\ttotal: 39s\tremaining: 6m 28s\n",
            "91:\tlearn: 0.0408440\ttotal: 39.4s\tremaining: 6m 28s\n",
            "92:\tlearn: 0.0407640\ttotal: 39.8s\tremaining: 6m 27s\n",
            "93:\tlearn: 0.0406658\ttotal: 40.2s\tremaining: 6m 27s\n",
            "94:\tlearn: 0.0405816\ttotal: 40.6s\tremaining: 6m 26s\n",
            "95:\tlearn: 0.0404726\ttotal: 41s\tremaining: 6m 25s\n",
            "96:\tlearn: 0.0404001\ttotal: 41.5s\tremaining: 6m 25s\n",
            "97:\tlearn: 0.0403322\ttotal: 41.9s\tremaining: 6m 25s\n",
            "98:\tlearn: 0.0402799\ttotal: 42.3s\tremaining: 6m 24s\n",
            "99:\tlearn: 0.0402250\ttotal: 42.8s\tremaining: 6m 24s\n",
            "100:\tlearn: 0.0401553\ttotal: 43.2s\tremaining: 6m 23s\n",
            "101:\tlearn: 0.0400598\ttotal: 43.6s\tremaining: 6m 23s\n",
            "102:\tlearn: 0.0399942\ttotal: 44.1s\tremaining: 6m 23s\n",
            "103:\tlearn: 0.0399196\ttotal: 44.5s\tremaining: 6m 22s\n",
            "104:\tlearn: 0.0398510\ttotal: 44.9s\tremaining: 6m 22s\n",
            "105:\tlearn: 0.0397697\ttotal: 45.3s\tremaining: 6m 21s\n",
            "106:\tlearn: 0.0397225\ttotal: 45.7s\tremaining: 6m 21s\n",
            "107:\tlearn: 0.0396455\ttotal: 46.1s\tremaining: 6m 20s\n",
            "108:\tlearn: 0.0395888\ttotal: 46.6s\tremaining: 6m 20s\n",
            "109:\tlearn: 0.0394904\ttotal: 47s\tremaining: 6m 19s\n",
            "110:\tlearn: 0.0394525\ttotal: 47.4s\tremaining: 6m 19s\n",
            "111:\tlearn: 0.0393514\ttotal: 47.8s\tremaining: 6m 18s\n",
            "112:\tlearn: 0.0393079\ttotal: 48.3s\tremaining: 6m 18s\n",
            "113:\tlearn: 0.0392622\ttotal: 48.7s\tremaining: 6m 17s\n",
            "114:\tlearn: 0.0392046\ttotal: 49.1s\tremaining: 6m 17s\n",
            "115:\tlearn: 0.0391466\ttotal: 49.6s\tremaining: 6m 17s\n",
            "116:\tlearn: 0.0391170\ttotal: 50s\tremaining: 6m 16s\n",
            "117:\tlearn: 0.0390489\ttotal: 50.4s\tremaining: 6m 16s\n",
            "118:\tlearn: 0.0390262\ttotal: 50.8s\tremaining: 6m 15s\n",
            "119:\tlearn: 0.0389931\ttotal: 51.2s\tremaining: 6m 15s\n",
            "120:\tlearn: 0.0389378\ttotal: 51.6s\tremaining: 6m 14s\n",
            "121:\tlearn: 0.0389055\ttotal: 52.1s\tremaining: 6m 14s\n",
            "122:\tlearn: 0.0388719\ttotal: 52.5s\tremaining: 6m 13s\n",
            "123:\tlearn: 0.0388522\ttotal: 52.9s\tremaining: 6m 13s\n",
            "124:\tlearn: 0.0388185\ttotal: 53.4s\tremaining: 6m 13s\n",
            "125:\tlearn: 0.0387847\ttotal: 53.8s\tremaining: 6m 12s\n",
            "126:\tlearn: 0.0387464\ttotal: 54.2s\tremaining: 6m 12s\n",
            "127:\tlearn: 0.0387144\ttotal: 54.6s\tremaining: 6m 11s\n",
            "128:\tlearn: 0.0386882\ttotal: 55.1s\tremaining: 6m 11s\n",
            "129:\tlearn: 0.0386494\ttotal: 55.5s\tremaining: 6m 10s\n",
            "130:\tlearn: 0.0386056\ttotal: 55.9s\tremaining: 6m 10s\n",
            "131:\tlearn: 0.0385876\ttotal: 56.3s\tremaining: 6m 9s\n",
            "132:\tlearn: 0.0385700\ttotal: 56.8s\tremaining: 6m 9s\n",
            "133:\tlearn: 0.0385244\ttotal: 57.2s\tremaining: 6m 9s\n",
            "134:\tlearn: 0.0384875\ttotal: 57.6s\tremaining: 6m 8s\n",
            "135:\tlearn: 0.0384622\ttotal: 58s\tremaining: 6m 8s\n",
            "136:\tlearn: 0.0384434\ttotal: 58.5s\tremaining: 6m 8s\n",
            "137:\tlearn: 0.0384119\ttotal: 58.9s\tremaining: 6m 7s\n",
            "138:\tlearn: 0.0383955\ttotal: 59.3s\tremaining: 6m 7s\n",
            "139:\tlearn: 0.0383543\ttotal: 59.8s\tremaining: 6m 6s\n",
            "140:\tlearn: 0.0383212\ttotal: 1m\tremaining: 6m 6s\n",
            "141:\tlearn: 0.0382800\ttotal: 1m\tremaining: 6m 5s\n",
            "142:\tlearn: 0.0382579\ttotal: 1m 1s\tremaining: 6m 5s\n",
            "143:\tlearn: 0.0382276\ttotal: 1m 1s\tremaining: 6m 5s\n",
            "144:\tlearn: 0.0381823\ttotal: 1m 1s\tremaining: 6m 4s\n",
            "145:\tlearn: 0.0381477\ttotal: 1m 2s\tremaining: 6m 3s\n",
            "146:\tlearn: 0.0381267\ttotal: 1m 2s\tremaining: 6m 3s\n",
            "147:\tlearn: 0.0381083\ttotal: 1m 3s\tremaining: 6m 2s\n",
            "148:\tlearn: 0.0380883\ttotal: 1m 3s\tremaining: 6m 2s\n",
            "149:\tlearn: 0.0380653\ttotal: 1m 3s\tremaining: 6m 1s\n",
            "150:\tlearn: 0.0380367\ttotal: 1m 4s\tremaining: 6m 1s\n",
            "151:\tlearn: 0.0380206\ttotal: 1m 4s\tremaining: 6m\n",
            "152:\tlearn: 0.0380025\ttotal: 1m 5s\tremaining: 6m\n",
            "153:\tlearn: 0.0379838\ttotal: 1m 5s\tremaining: 5m 59s\n",
            "154:\tlearn: 0.0379615\ttotal: 1m 6s\tremaining: 5m 59s\n",
            "155:\tlearn: 0.0379376\ttotal: 1m 6s\tremaining: 5m 58s\n",
            "156:\tlearn: 0.0379256\ttotal: 1m 6s\tremaining: 5m 58s\n",
            "157:\tlearn: 0.0379112\ttotal: 1m 7s\tremaining: 5m 58s\n",
            "158:\tlearn: 0.0378928\ttotal: 1m 7s\tremaining: 5m 57s\n",
            "159:\tlearn: 0.0378816\ttotal: 1m 8s\tremaining: 5m 57s\n",
            "160:\tlearn: 0.0378536\ttotal: 1m 8s\tremaining: 5m 56s\n",
            "161:\tlearn: 0.0378287\ttotal: 1m 8s\tremaining: 5m 56s\n",
            "162:\tlearn: 0.0378174\ttotal: 1m 9s\tremaining: 5m 55s\n",
            "163:\tlearn: 0.0378026\ttotal: 1m 9s\tremaining: 5m 55s\n",
            "164:\tlearn: 0.0377849\ttotal: 1m 10s\tremaining: 5m 54s\n",
            "165:\tlearn: 0.0377744\ttotal: 1m 10s\tremaining: 5m 54s\n",
            "166:\tlearn: 0.0377635\ttotal: 1m 11s\tremaining: 5m 53s\n",
            "167:\tlearn: 0.0377510\ttotal: 1m 11s\tremaining: 5m 53s\n",
            "168:\tlearn: 0.0377395\ttotal: 1m 11s\tremaining: 5m 53s\n",
            "169:\tlearn: 0.0377266\ttotal: 1m 12s\tremaining: 5m 52s\n",
            "170:\tlearn: 0.0377085\ttotal: 1m 12s\tremaining: 5m 52s\n",
            "171:\tlearn: 0.0376909\ttotal: 1m 13s\tremaining: 5m 51s\n",
            "172:\tlearn: 0.0376792\ttotal: 1m 13s\tremaining: 5m 51s\n",
            "173:\tlearn: 0.0376668\ttotal: 1m 13s\tremaining: 5m 50s\n",
            "174:\tlearn: 0.0376609\ttotal: 1m 14s\tremaining: 5m 50s\n",
            "175:\tlearn: 0.0376442\ttotal: 1m 14s\tremaining: 5m 50s\n",
            "176:\tlearn: 0.0376361\ttotal: 1m 15s\tremaining: 5m 50s\n",
            "177:\tlearn: 0.0376253\ttotal: 1m 15s\tremaining: 5m 49s\n",
            "178:\tlearn: 0.0376129\ttotal: 1m 16s\tremaining: 5m 49s\n",
            "179:\tlearn: 0.0376034\ttotal: 1m 16s\tremaining: 5m 48s\n",
            "180:\tlearn: 0.0375948\ttotal: 1m 17s\tremaining: 5m 48s\n",
            "181:\tlearn: 0.0375845\ttotal: 1m 17s\tremaining: 5m 47s\n",
            "182:\tlearn: 0.0375738\ttotal: 1m 17s\tremaining: 5m 47s\n",
            "183:\tlearn: 0.0375651\ttotal: 1m 18s\tremaining: 5m 47s\n",
            "184:\tlearn: 0.0375588\ttotal: 1m 18s\tremaining: 5m 46s\n",
            "185:\tlearn: 0.0375488\ttotal: 1m 19s\tremaining: 5m 46s\n",
            "186:\tlearn: 0.0375385\ttotal: 1m 19s\tremaining: 5m 45s\n",
            "187:\tlearn: 0.0375295\ttotal: 1m 20s\tremaining: 5m 45s\n",
            "188:\tlearn: 0.0375238\ttotal: 1m 20s\tremaining: 5m 44s\n",
            "189:\tlearn: 0.0375164\ttotal: 1m 20s\tremaining: 5m 44s\n",
            "190:\tlearn: 0.0375125\ttotal: 1m 21s\tremaining: 5m 43s\n",
            "191:\tlearn: 0.0375066\ttotal: 1m 21s\tremaining: 5m 43s\n",
            "192:\tlearn: 0.0375006\ttotal: 1m 22s\tremaining: 5m 43s\n",
            "193:\tlearn: 0.0374958\ttotal: 1m 22s\tremaining: 5m 42s\n",
            "194:\tlearn: 0.0374857\ttotal: 1m 23s\tremaining: 5m 42s\n",
            "195:\tlearn: 0.0374751\ttotal: 1m 23s\tremaining: 5m 41s\n",
            "196:\tlearn: 0.0374683\ttotal: 1m 23s\tremaining: 5m 41s\n",
            "197:\tlearn: 0.0374598\ttotal: 1m 24s\tremaining: 5m 41s\n",
            "198:\tlearn: 0.0374550\ttotal: 1m 24s\tremaining: 5m 40s\n",
            "199:\tlearn: 0.0374436\ttotal: 1m 25s\tremaining: 5m 40s\n",
            "200:\tlearn: 0.0374378\ttotal: 1m 25s\tremaining: 5m 39s\n",
            "201:\tlearn: 0.0374327\ttotal: 1m 26s\tremaining: 5m 39s\n",
            "202:\tlearn: 0.0374243\ttotal: 1m 26s\tremaining: 5m 38s\n",
            "203:\tlearn: 0.0374158\ttotal: 1m 26s\tremaining: 5m 38s\n",
            "204:\tlearn: 0.0374111\ttotal: 1m 27s\tremaining: 5m 38s\n",
            "205:\tlearn: 0.0374020\ttotal: 1m 27s\tremaining: 5m 37s\n",
            "206:\tlearn: 0.0373960\ttotal: 1m 28s\tremaining: 5m 37s\n",
            "207:\tlearn: 0.0373925\ttotal: 1m 28s\tremaining: 5m 36s\n",
            "208:\tlearn: 0.0373858\ttotal: 1m 29s\tremaining: 5m 36s\n",
            "209:\tlearn: 0.0373809\ttotal: 1m 29s\tremaining: 5m 35s\n",
            "210:\tlearn: 0.0373770\ttotal: 1m 29s\tremaining: 5m 35s\n",
            "211:\tlearn: 0.0373723\ttotal: 1m 30s\tremaining: 5m 35s\n",
            "212:\tlearn: 0.0373672\ttotal: 1m 30s\tremaining: 5m 34s\n",
            "213:\tlearn: 0.0373628\ttotal: 1m 31s\tremaining: 5m 34s\n",
            "214:\tlearn: 0.0373593\ttotal: 1m 31s\tremaining: 5m 33s\n",
            "215:\tlearn: 0.0373547\ttotal: 1m 31s\tremaining: 5m 33s\n",
            "216:\tlearn: 0.0373497\ttotal: 1m 32s\tremaining: 5m 32s\n",
            "217:\tlearn: 0.0373433\ttotal: 1m 32s\tremaining: 5m 32s\n",
            "218:\tlearn: 0.0373414\ttotal: 1m 33s\tremaining: 5m 32s\n",
            "219:\tlearn: 0.0373393\ttotal: 1m 33s\tremaining: 5m 31s\n",
            "220:\tlearn: 0.0373321\ttotal: 1m 34s\tremaining: 5m 31s\n",
            "221:\tlearn: 0.0373286\ttotal: 1m 34s\tremaining: 5m 30s\n",
            "222:\tlearn: 0.0373263\ttotal: 1m 34s\tremaining: 5m 30s\n",
            "223:\tlearn: 0.0373229\ttotal: 1m 35s\tremaining: 5m 29s\n",
            "224:\tlearn: 0.0373186\ttotal: 1m 35s\tremaining: 5m 29s\n",
            "225:\tlearn: 0.0373145\ttotal: 1m 36s\tremaining: 5m 29s\n",
            "226:\tlearn: 0.0373091\ttotal: 1m 36s\tremaining: 5m 28s\n",
            "227:\tlearn: 0.0373062\ttotal: 1m 37s\tremaining: 5m 28s\n",
            "228:\tlearn: 0.0373036\ttotal: 1m 37s\tremaining: 5m 27s\n",
            "229:\tlearn: 0.0373003\ttotal: 1m 37s\tremaining: 5m 27s\n",
            "230:\tlearn: 0.0372965\ttotal: 1m 38s\tremaining: 5m 27s\n",
            "231:\tlearn: 0.0372931\ttotal: 1m 38s\tremaining: 5m 26s\n",
            "232:\tlearn: 0.0372887\ttotal: 1m 39s\tremaining: 5m 26s\n",
            "233:\tlearn: 0.0372842\ttotal: 1m 39s\tremaining: 5m 25s\n",
            "234:\tlearn: 0.0372823\ttotal: 1m 40s\tremaining: 5m 25s\n",
            "235:\tlearn: 0.0372777\ttotal: 1m 40s\tremaining: 5m 24s\n",
            "236:\tlearn: 0.0372755\ttotal: 1m 40s\tremaining: 5m 24s\n",
            "237:\tlearn: 0.0372734\ttotal: 1m 41s\tremaining: 5m 24s\n",
            "238:\tlearn: 0.0372706\ttotal: 1m 41s\tremaining: 5m 23s\n",
            "239:\tlearn: 0.0372684\ttotal: 1m 42s\tremaining: 5m 23s\n",
            "240:\tlearn: 0.0372671\ttotal: 1m 42s\tremaining: 5m 22s\n",
            "241:\tlearn: 0.0372656\ttotal: 1m 43s\tremaining: 5m 22s\n",
            "242:\tlearn: 0.0372636\ttotal: 1m 43s\tremaining: 5m 22s\n",
            "243:\tlearn: 0.0372594\ttotal: 1m 43s\tremaining: 5m 21s\n",
            "244:\tlearn: 0.0372571\ttotal: 1m 44s\tremaining: 5m 21s\n",
            "245:\tlearn: 0.0372552\ttotal: 1m 44s\tremaining: 5m 20s\n",
            "246:\tlearn: 0.0372500\ttotal: 1m 45s\tremaining: 5m 20s\n",
            "247:\tlearn: 0.0372464\ttotal: 1m 45s\tremaining: 5m 20s\n",
            "248:\tlearn: 0.0372441\ttotal: 1m 46s\tremaining: 5m 19s\n",
            "249:\tlearn: 0.0372428\ttotal: 1m 46s\tremaining: 5m 19s\n",
            "250:\tlearn: 0.0372409\ttotal: 1m 46s\tremaining: 5m 18s\n",
            "251:\tlearn: 0.0372394\ttotal: 1m 47s\tremaining: 5m 18s\n",
            "252:\tlearn: 0.0372363\ttotal: 1m 47s\tremaining: 5m 17s\n",
            "253:\tlearn: 0.0372342\ttotal: 1m 48s\tremaining: 5m 17s\n",
            "254:\tlearn: 0.0372318\ttotal: 1m 48s\tremaining: 5m 16s\n",
            "255:\tlearn: 0.0372277\ttotal: 1m 49s\tremaining: 5m 16s\n",
            "256:\tlearn: 0.0372258\ttotal: 1m 49s\tremaining: 5m 16s\n",
            "257:\tlearn: 0.0372227\ttotal: 1m 49s\tremaining: 5m 15s\n",
            "258:\tlearn: 0.0372209\ttotal: 1m 50s\tremaining: 5m 15s\n",
            "259:\tlearn: 0.0372181\ttotal: 1m 50s\tremaining: 5m 14s\n",
            "260:\tlearn: 0.0372154\ttotal: 1m 51s\tremaining: 5m 14s\n",
            "261:\tlearn: 0.0372129\ttotal: 1m 51s\tremaining: 5m 13s\n",
            "262:\tlearn: 0.0372108\ttotal: 1m 51s\tremaining: 5m 13s\n",
            "263:\tlearn: 0.0372086\ttotal: 1m 52s\tremaining: 5m 12s\n",
            "264:\tlearn: 0.0372068\ttotal: 1m 52s\tremaining: 5m 12s\n",
            "265:\tlearn: 0.0372048\ttotal: 1m 53s\tremaining: 5m 12s\n",
            "266:\tlearn: 0.0372027\ttotal: 1m 53s\tremaining: 5m 11s\n",
            "267:\tlearn: 0.0372013\ttotal: 1m 54s\tremaining: 5m 11s\n",
            "268:\tlearn: 0.0371991\ttotal: 1m 54s\tremaining: 5m 10s\n",
            "269:\tlearn: 0.0371963\ttotal: 1m 54s\tremaining: 5m 10s\n",
            "270:\tlearn: 0.0371952\ttotal: 1m 55s\tremaining: 5m 9s\n",
            "271:\tlearn: 0.0371920\ttotal: 1m 55s\tremaining: 5m 9s\n",
            "272:\tlearn: 0.0371897\ttotal: 1m 56s\tremaining: 5m 9s\n",
            "273:\tlearn: 0.0371883\ttotal: 1m 56s\tremaining: 5m 8s\n",
            "274:\tlearn: 0.0371857\ttotal: 1m 57s\tremaining: 5m 8s\n",
            "275:\tlearn: 0.0371838\ttotal: 1m 57s\tremaining: 5m 7s\n",
            "276:\tlearn: 0.0371820\ttotal: 1m 57s\tremaining: 5m 7s\n",
            "277:\tlearn: 0.0371811\ttotal: 1m 58s\tremaining: 5m 6s\n",
            "278:\tlearn: 0.0371791\ttotal: 1m 58s\tremaining: 5m 6s\n",
            "279:\tlearn: 0.0371776\ttotal: 1m 59s\tremaining: 5m 6s\n",
            "280:\tlearn: 0.0371761\ttotal: 1m 59s\tremaining: 5m 5s\n",
            "281:\tlearn: 0.0371752\ttotal: 2m\tremaining: 5m 5s\n",
            "282:\tlearn: 0.0371736\ttotal: 2m\tremaining: 5m 4s\n",
            "283:\tlearn: 0.0371719\ttotal: 2m\tremaining: 5m 4s\n",
            "284:\tlearn: 0.0371684\ttotal: 2m 1s\tremaining: 5m 3s\n",
            "285:\tlearn: 0.0371652\ttotal: 2m 1s\tremaining: 5m 3s\n",
            "286:\tlearn: 0.0371628\ttotal: 2m 2s\tremaining: 5m 3s\n",
            "287:\tlearn: 0.0371613\ttotal: 2m 2s\tremaining: 5m 2s\n",
            "288:\tlearn: 0.0371600\ttotal: 2m 3s\tremaining: 5m 2s\n",
            "289:\tlearn: 0.0371588\ttotal: 2m 3s\tremaining: 5m 1s\n",
            "290:\tlearn: 0.0371577\ttotal: 2m 3s\tremaining: 5m 1s\n",
            "291:\tlearn: 0.0371573\ttotal: 2m 4s\tremaining: 5m\n",
            "292:\tlearn: 0.0371563\ttotal: 2m 4s\tremaining: 5m\n",
            "293:\tlearn: 0.0371537\ttotal: 2m 5s\tremaining: 5m\n",
            "294:\tlearn: 0.0371517\ttotal: 2m 5s\tremaining: 4m 59s\n",
            "295:\tlearn: 0.0371509\ttotal: 2m 5s\tremaining: 4m 59s\n",
            "296:\tlearn: 0.0371498\ttotal: 2m 6s\tremaining: 4m 58s\n",
            "297:\tlearn: 0.0371483\ttotal: 2m 6s\tremaining: 4m 58s\n",
            "298:\tlearn: 0.0371466\ttotal: 2m 7s\tremaining: 4m 57s\n",
            "299:\tlearn: 0.0371460\ttotal: 2m 7s\tremaining: 4m 57s\n",
            "300:\tlearn: 0.0371436\ttotal: 2m 8s\tremaining: 4m 57s\n",
            "301:\tlearn: 0.0371420\ttotal: 2m 8s\tremaining: 4m 56s\n",
            "302:\tlearn: 0.0371408\ttotal: 2m 8s\tremaining: 4m 56s\n",
            "303:\tlearn: 0.0371391\ttotal: 2m 9s\tremaining: 4m 55s\n",
            "304:\tlearn: 0.0371376\ttotal: 2m 9s\tremaining: 4m 55s\n",
            "305:\tlearn: 0.0371367\ttotal: 2m 10s\tremaining: 4m 54s\n",
            "306:\tlearn: 0.0371358\ttotal: 2m 10s\tremaining: 4m 54s\n",
            "307:\tlearn: 0.0371349\ttotal: 2m 11s\tremaining: 4m 54s\n",
            "308:\tlearn: 0.0371331\ttotal: 2m 11s\tremaining: 4m 53s\n",
            "309:\tlearn: 0.0371316\ttotal: 2m 11s\tremaining: 4m 53s\n",
            "310:\tlearn: 0.0371304\ttotal: 2m 12s\tremaining: 4m 52s\n",
            "311:\tlearn: 0.0371294\ttotal: 2m 12s\tremaining: 4m 52s\n",
            "312:\tlearn: 0.0371286\ttotal: 2m 13s\tremaining: 4m 51s\n",
            "313:\tlearn: 0.0371276\ttotal: 2m 13s\tremaining: 4m 51s\n",
            "314:\tlearn: 0.0371266\ttotal: 2m 14s\tremaining: 4m 51s\n",
            "315:\tlearn: 0.0371257\ttotal: 2m 14s\tremaining: 4m 50s\n",
            "316:\tlearn: 0.0371242\ttotal: 2m 14s\tremaining: 4m 50s\n",
            "317:\tlearn: 0.0371224\ttotal: 2m 15s\tremaining: 4m 49s\n",
            "318:\tlearn: 0.0371205\ttotal: 2m 15s\tremaining: 4m 49s\n",
            "319:\tlearn: 0.0371194\ttotal: 2m 16s\tremaining: 4m 48s\n",
            "320:\tlearn: 0.0371186\ttotal: 2m 16s\tremaining: 4m 48s\n",
            "321:\tlearn: 0.0371178\ttotal: 2m 17s\tremaining: 4m 48s\n",
            "322:\tlearn: 0.0371168\ttotal: 2m 17s\tremaining: 4m 47s\n",
            "323:\tlearn: 0.0371163\ttotal: 2m 17s\tremaining: 4m 47s\n",
            "324:\tlearn: 0.0371153\ttotal: 2m 18s\tremaining: 4m 46s\n",
            "325:\tlearn: 0.0371144\ttotal: 2m 18s\tremaining: 4m 46s\n",
            "326:\tlearn: 0.0371129\ttotal: 2m 19s\tremaining: 4m 45s\n",
            "327:\tlearn: 0.0371116\ttotal: 2m 19s\tremaining: 4m 45s\n",
            "328:\tlearn: 0.0371106\ttotal: 2m 20s\tremaining: 4m 45s\n",
            "329:\tlearn: 0.0371093\ttotal: 2m 20s\tremaining: 4m 44s\n",
            "330:\tlearn: 0.0371081\ttotal: 2m 20s\tremaining: 4m 44s\n",
            "331:\tlearn: 0.0371078\ttotal: 2m 21s\tremaining: 4m 43s\n",
            "332:\tlearn: 0.0371069\ttotal: 2m 21s\tremaining: 4m 43s\n",
            "333:\tlearn: 0.0371062\ttotal: 2m 22s\tremaining: 4m 42s\n",
            "334:\tlearn: 0.0371056\ttotal: 2m 22s\tremaining: 4m 42s\n",
            "335:\tlearn: 0.0371051\ttotal: 2m 22s\tremaining: 4m 42s\n",
            "336:\tlearn: 0.0371036\ttotal: 2m 23s\tremaining: 4m 41s\n",
            "337:\tlearn: 0.0371028\ttotal: 2m 23s\tremaining: 4m 41s\n",
            "338:\tlearn: 0.0371020\ttotal: 2m 24s\tremaining: 4m 40s\n",
            "339:\tlearn: 0.0371004\ttotal: 2m 24s\tremaining: 4m 40s\n",
            "340:\tlearn: 0.0370996\ttotal: 2m 25s\tremaining: 4m 40s\n",
            "341:\tlearn: 0.0370986\ttotal: 2m 25s\tremaining: 4m 39s\n",
            "342:\tlearn: 0.0370977\ttotal: 2m 25s\tremaining: 4m 39s\n",
            "343:\tlearn: 0.0370969\ttotal: 2m 26s\tremaining: 4m 38s\n",
            "344:\tlearn: 0.0370961\ttotal: 2m 26s\tremaining: 4m 38s\n",
            "345:\tlearn: 0.0370949\ttotal: 2m 27s\tremaining: 4m 37s\n",
            "346:\tlearn: 0.0370940\ttotal: 2m 27s\tremaining: 4m 37s\n",
            "347:\tlearn: 0.0370934\ttotal: 2m 28s\tremaining: 4m 37s\n",
            "348:\tlearn: 0.0370927\ttotal: 2m 28s\tremaining: 4m 36s\n",
            "349:\tlearn: 0.0370921\ttotal: 2m 28s\tremaining: 4m 36s\n",
            "350:\tlearn: 0.0370916\ttotal: 2m 29s\tremaining: 4m 35s\n",
            "351:\tlearn: 0.0370909\ttotal: 2m 29s\tremaining: 4m 35s\n",
            "352:\tlearn: 0.0370902\ttotal: 2m 30s\tremaining: 4m 35s\n",
            "353:\tlearn: 0.0370891\ttotal: 2m 30s\tremaining: 4m 34s\n",
            "354:\tlearn: 0.0370882\ttotal: 2m 31s\tremaining: 4m 34s\n",
            "355:\tlearn: 0.0370869\ttotal: 2m 31s\tremaining: 4m 33s\n",
            "356:\tlearn: 0.0370863\ttotal: 2m 31s\tremaining: 4m 33s\n",
            "357:\tlearn: 0.0370856\ttotal: 2m 32s\tremaining: 4m 32s\n",
            "358:\tlearn: 0.0370848\ttotal: 2m 32s\tremaining: 4m 32s\n",
            "359:\tlearn: 0.0370838\ttotal: 2m 33s\tremaining: 4m 31s\n",
            "360:\tlearn: 0.0370833\ttotal: 2m 33s\tremaining: 4m 31s\n",
            "361:\tlearn: 0.0370828\ttotal: 2m 34s\tremaining: 4m 31s\n",
            "362:\tlearn: 0.0370819\ttotal: 2m 34s\tremaining: 4m 30s\n",
            "363:\tlearn: 0.0370813\ttotal: 2m 34s\tremaining: 4m 30s\n",
            "364:\tlearn: 0.0370807\ttotal: 2m 35s\tremaining: 4m 29s\n",
            "365:\tlearn: 0.0370804\ttotal: 2m 35s\tremaining: 4m 29s\n",
            "366:\tlearn: 0.0370800\ttotal: 2m 36s\tremaining: 4m 28s\n",
            "367:\tlearn: 0.0370794\ttotal: 2m 36s\tremaining: 4m 28s\n",
            "368:\tlearn: 0.0370784\ttotal: 2m 36s\tremaining: 4m 28s\n",
            "369:\tlearn: 0.0370780\ttotal: 2m 37s\tremaining: 4m 27s\n",
            "370:\tlearn: 0.0370777\ttotal: 2m 37s\tremaining: 4m 27s\n",
            "371:\tlearn: 0.0370774\ttotal: 2m 38s\tremaining: 4m 26s\n",
            "372:\tlearn: 0.0370764\ttotal: 2m 38s\tremaining: 4m 26s\n",
            "373:\tlearn: 0.0370759\ttotal: 2m 39s\tremaining: 4m 25s\n",
            "374:\tlearn: 0.0370750\ttotal: 2m 39s\tremaining: 4m 25s\n",
            "375:\tlearn: 0.0370744\ttotal: 2m 39s\tremaining: 4m 24s\n",
            "376:\tlearn: 0.0370741\ttotal: 2m 40s\tremaining: 4m 24s\n",
            "377:\tlearn: 0.0370735\ttotal: 2m 40s\tremaining: 4m 24s\n",
            "378:\tlearn: 0.0370729\ttotal: 2m 41s\tremaining: 4m 23s\n",
            "379:\tlearn: 0.0370723\ttotal: 2m 41s\tremaining: 4m 23s\n",
            "380:\tlearn: 0.0370717\ttotal: 2m 42s\tremaining: 4m 22s\n",
            "381:\tlearn: 0.0370710\ttotal: 2m 42s\tremaining: 4m 22s\n",
            "382:\tlearn: 0.0370707\ttotal: 2m 42s\tremaining: 4m 21s\n",
            "383:\tlearn: 0.0370704\ttotal: 2m 43s\tremaining: 4m 21s\n",
            "384:\tlearn: 0.0370697\ttotal: 2m 43s\tremaining: 4m 21s\n",
            "385:\tlearn: 0.0370690\ttotal: 2m 44s\tremaining: 4m 20s\n",
            "386:\tlearn: 0.0370686\ttotal: 2m 44s\tremaining: 4m 20s\n",
            "387:\tlearn: 0.0370681\ttotal: 2m 45s\tremaining: 4m 19s\n",
            "388:\tlearn: 0.0370672\ttotal: 2m 45s\tremaining: 4m 19s\n",
            "389:\tlearn: 0.0370665\ttotal: 2m 45s\tremaining: 4m 19s\n",
            "390:\tlearn: 0.0370661\ttotal: 2m 46s\tremaining: 4m 18s\n",
            "391:\tlearn: 0.0370657\ttotal: 2m 46s\tremaining: 4m 18s\n",
            "392:\tlearn: 0.0370654\ttotal: 2m 47s\tremaining: 4m 17s\n",
            "393:\tlearn: 0.0370647\ttotal: 2m 47s\tremaining: 4m 17s\n",
            "394:\tlearn: 0.0370640\ttotal: 2m 47s\tremaining: 4m 16s\n",
            "395:\tlearn: 0.0370635\ttotal: 2m 48s\tremaining: 4m 16s\n",
            "396:\tlearn: 0.0370627\ttotal: 2m 48s\tremaining: 4m 16s\n",
            "397:\tlearn: 0.0370621\ttotal: 2m 49s\tremaining: 4m 15s\n",
            "398:\tlearn: 0.0370616\ttotal: 2m 49s\tremaining: 4m 15s\n",
            "399:\tlearn: 0.0370611\ttotal: 2m 50s\tremaining: 4m 14s\n",
            "400:\tlearn: 0.0370605\ttotal: 2m 50s\tremaining: 4m 14s\n",
            "401:\tlearn: 0.0370600\ttotal: 2m 50s\tremaining: 4m 13s\n",
            "402:\tlearn: 0.0370597\ttotal: 2m 51s\tremaining: 4m 13s\n",
            "403:\tlearn: 0.0370593\ttotal: 2m 51s\tremaining: 4m 12s\n",
            "404:\tlearn: 0.0370587\ttotal: 2m 52s\tremaining: 4m 12s\n",
            "405:\tlearn: 0.0370580\ttotal: 2m 52s\tremaining: 4m 12s\n",
            "406:\tlearn: 0.0370575\ttotal: 2m 53s\tremaining: 4m 11s\n",
            "407:\tlearn: 0.0370572\ttotal: 2m 53s\tremaining: 4m 11s\n",
            "408:\tlearn: 0.0370568\ttotal: 2m 53s\tremaining: 4m 10s\n",
            "409:\tlearn: 0.0370561\ttotal: 2m 54s\tremaining: 4m 10s\n",
            "410:\tlearn: 0.0370560\ttotal: 2m 54s\tremaining: 4m 9s\n",
            "411:\tlearn: 0.0370556\ttotal: 2m 55s\tremaining: 4m 9s\n",
            "412:\tlearn: 0.0370553\ttotal: 2m 55s\tremaining: 4m 9s\n",
            "413:\tlearn: 0.0370548\ttotal: 2m 55s\tremaining: 4m 8s\n",
            "414:\tlearn: 0.0370543\ttotal: 2m 56s\tremaining: 4m 8s\n",
            "415:\tlearn: 0.0370540\ttotal: 2m 56s\tremaining: 4m 7s\n",
            "416:\tlearn: 0.0370535\ttotal: 2m 57s\tremaining: 4m 7s\n",
            "417:\tlearn: 0.0370532\ttotal: 2m 57s\tremaining: 4m 6s\n",
            "418:\tlearn: 0.0370526\ttotal: 2m 58s\tremaining: 4m 6s\n",
            "419:\tlearn: 0.0370517\ttotal: 2m 58s\tremaining: 4m 6s\n",
            "420:\tlearn: 0.0370514\ttotal: 2m 58s\tremaining: 4m 5s\n",
            "421:\tlearn: 0.0370512\ttotal: 2m 59s\tremaining: 4m 5s\n",
            "422:\tlearn: 0.0370506\ttotal: 2m 59s\tremaining: 4m 4s\n",
            "423:\tlearn: 0.0370503\ttotal: 3m\tremaining: 4m 4s\n",
            "424:\tlearn: 0.0370500\ttotal: 3m\tremaining: 4m 4s\n",
            "425:\tlearn: 0.0370496\ttotal: 3m 1s\tremaining: 4m 3s\n",
            "426:\tlearn: 0.0370492\ttotal: 3m 1s\tremaining: 4m 3s\n",
            "427:\tlearn: 0.0370488\ttotal: 3m 1s\tremaining: 4m 2s\n",
            "428:\tlearn: 0.0370484\ttotal: 3m 2s\tremaining: 4m 2s\n",
            "429:\tlearn: 0.0370481\ttotal: 3m 2s\tremaining: 4m 1s\n",
            "430:\tlearn: 0.0370477\ttotal: 3m 3s\tremaining: 4m 1s\n",
            "431:\tlearn: 0.0370475\ttotal: 3m 3s\tremaining: 4m 1s\n",
            "432:\tlearn: 0.0370471\ttotal: 3m 4s\tremaining: 4m\n",
            "433:\tlearn: 0.0370465\ttotal: 3m 4s\tremaining: 4m\n",
            "434:\tlearn: 0.0370460\ttotal: 3m 4s\tremaining: 3m 59s\n",
            "435:\tlearn: 0.0370449\ttotal: 3m 5s\tremaining: 3m 59s\n",
            "436:\tlearn: 0.0370446\ttotal: 3m 5s\tremaining: 3m 58s\n",
            "437:\tlearn: 0.0370444\ttotal: 3m 6s\tremaining: 3m 58s\n",
            "438:\tlearn: 0.0370440\ttotal: 3m 6s\tremaining: 3m 57s\n",
            "439:\tlearn: 0.0370437\ttotal: 3m 7s\tremaining: 3m 57s\n",
            "440:\tlearn: 0.0370434\ttotal: 3m 7s\tremaining: 3m 57s\n",
            "441:\tlearn: 0.0370431\ttotal: 3m 7s\tremaining: 3m 56s\n",
            "442:\tlearn: 0.0370428\ttotal: 3m 8s\tremaining: 3m 56s\n",
            "443:\tlearn: 0.0370426\ttotal: 3m 8s\tremaining: 3m 55s\n",
            "444:\tlearn: 0.0370419\ttotal: 3m 9s\tremaining: 3m 55s\n",
            "445:\tlearn: 0.0370417\ttotal: 3m 9s\tremaining: 3m 55s\n",
            "446:\tlearn: 0.0370412\ttotal: 3m 9s\tremaining: 3m 54s\n",
            "447:\tlearn: 0.0370407\ttotal: 3m 10s\tremaining: 3m 54s\n",
            "448:\tlearn: 0.0370402\ttotal: 3m 10s\tremaining: 3m 53s\n",
            "449:\tlearn: 0.0370398\ttotal: 3m 11s\tremaining: 3m 53s\n",
            "450:\tlearn: 0.0370395\ttotal: 3m 11s\tremaining: 3m 52s\n",
            "451:\tlearn: 0.0370393\ttotal: 3m 12s\tremaining: 3m 52s\n",
            "452:\tlearn: 0.0370386\ttotal: 3m 12s\tremaining: 3m 52s\n",
            "453:\tlearn: 0.0370383\ttotal: 3m 12s\tremaining: 3m 51s\n",
            "454:\tlearn: 0.0370380\ttotal: 3m 13s\tremaining: 3m 51s\n",
            "455:\tlearn: 0.0370378\ttotal: 3m 13s\tremaining: 3m 50s\n",
            "456:\tlearn: 0.0370375\ttotal: 3m 14s\tremaining: 3m 50s\n",
            "457:\tlearn: 0.0370371\ttotal: 3m 14s\tremaining: 3m 49s\n",
            "458:\tlearn: 0.0370368\ttotal: 3m 15s\tremaining: 3m 49s\n",
            "459:\tlearn: 0.0370364\ttotal: 3m 15s\tremaining: 3m 49s\n",
            "460:\tlearn: 0.0370358\ttotal: 3m 15s\tremaining: 3m 48s\n",
            "461:\tlearn: 0.0370356\ttotal: 3m 16s\tremaining: 3m 48s\n",
            "462:\tlearn: 0.0370354\ttotal: 3m 16s\tremaining: 3m 47s\n",
            "463:\tlearn: 0.0370350\ttotal: 3m 17s\tremaining: 3m 47s\n",
            "464:\tlearn: 0.0370347\ttotal: 3m 17s\tremaining: 3m 47s\n",
            "465:\tlearn: 0.0370345\ttotal: 3m 18s\tremaining: 3m 46s\n",
            "466:\tlearn: 0.0370343\ttotal: 3m 18s\tremaining: 3m 46s\n",
            "467:\tlearn: 0.0370340\ttotal: 3m 18s\tremaining: 3m 45s\n",
            "468:\tlearn: 0.0370337\ttotal: 3m 19s\tremaining: 3m 45s\n",
            "469:\tlearn: 0.0370333\ttotal: 3m 19s\tremaining: 3m 44s\n",
            "470:\tlearn: 0.0370331\ttotal: 3m 20s\tremaining: 3m 44s\n",
            "471:\tlearn: 0.0370326\ttotal: 3m 20s\tremaining: 3m 44s\n",
            "472:\tlearn: 0.0370324\ttotal: 3m 21s\tremaining: 3m 43s\n",
            "473:\tlearn: 0.0370323\ttotal: 3m 21s\tremaining: 3m 43s\n",
            "474:\tlearn: 0.0370319\ttotal: 3m 21s\tremaining: 3m 42s\n",
            "475:\tlearn: 0.0370317\ttotal: 3m 22s\tremaining: 3m 42s\n",
            "476:\tlearn: 0.0370315\ttotal: 3m 22s\tremaining: 3m 41s\n",
            "477:\tlearn: 0.0370312\ttotal: 3m 23s\tremaining: 3m 41s\n",
            "478:\tlearn: 0.0370309\ttotal: 3m 23s\tremaining: 3m 41s\n",
            "479:\tlearn: 0.0370308\ttotal: 3m 24s\tremaining: 3m 40s\n",
            "480:\tlearn: 0.0370306\ttotal: 3m 24s\tremaining: 3m 40s\n",
            "481:\tlearn: 0.0370304\ttotal: 3m 24s\tremaining: 3m 39s\n",
            "482:\tlearn: 0.0370300\ttotal: 3m 25s\tremaining: 3m 39s\n",
            "483:\tlearn: 0.0370296\ttotal: 3m 25s\tremaining: 3m 38s\n",
            "484:\tlearn: 0.0370292\ttotal: 3m 26s\tremaining: 3m 38s\n",
            "485:\tlearn: 0.0370291\ttotal: 3m 26s\tremaining: 3m 38s\n",
            "486:\tlearn: 0.0370289\ttotal: 3m 27s\tremaining: 3m 37s\n",
            "487:\tlearn: 0.0370285\ttotal: 3m 27s\tremaining: 3m 37s\n",
            "488:\tlearn: 0.0370283\ttotal: 3m 27s\tremaining: 3m 36s\n",
            "489:\tlearn: 0.0370280\ttotal: 3m 28s\tremaining: 3m 36s\n",
            "490:\tlearn: 0.0370278\ttotal: 3m 28s\tremaining: 3m 35s\n",
            "491:\tlearn: 0.0370276\ttotal: 3m 29s\tremaining: 3m 35s\n",
            "492:\tlearn: 0.0370275\ttotal: 3m 29s\tremaining: 3m 35s\n",
            "493:\tlearn: 0.0370272\ttotal: 3m 30s\tremaining: 3m 34s\n",
            "494:\tlearn: 0.0370268\ttotal: 3m 30s\tremaining: 3m 34s\n",
            "495:\tlearn: 0.0370266\ttotal: 3m 30s\tremaining: 3m 33s\n",
            "496:\tlearn: 0.0370264\ttotal: 3m 31s\tremaining: 3m 33s\n",
            "497:\tlearn: 0.0370262\ttotal: 3m 31s\tremaining: 3m 32s\n",
            "498:\tlearn: 0.0370261\ttotal: 3m 32s\tremaining: 3m 32s\n",
            "499:\tlearn: 0.0370257\ttotal: 3m 32s\tremaining: 3m 32s\n",
            "500:\tlearn: 0.0370256\ttotal: 3m 32s\tremaining: 3m 31s\n",
            "501:\tlearn: 0.0370252\ttotal: 3m 33s\tremaining: 3m 31s\n",
            "502:\tlearn: 0.0370249\ttotal: 3m 33s\tremaining: 3m 30s\n",
            "503:\tlearn: 0.0370247\ttotal: 3m 34s\tremaining: 3m 30s\n",
            "504:\tlearn: 0.0370246\ttotal: 3m 34s\tremaining: 3m 29s\n",
            "505:\tlearn: 0.0370243\ttotal: 3m 35s\tremaining: 3m 29s\n",
            "506:\tlearn: 0.0370239\ttotal: 3m 35s\tremaining: 3m 29s\n",
            "507:\tlearn: 0.0370235\ttotal: 3m 35s\tremaining: 3m 28s\n",
            "508:\tlearn: 0.0370233\ttotal: 3m 36s\tremaining: 3m 28s\n",
            "509:\tlearn: 0.0370232\ttotal: 3m 36s\tremaining: 3m 27s\n",
            "510:\tlearn: 0.0370230\ttotal: 3m 37s\tremaining: 3m 27s\n",
            "511:\tlearn: 0.0370229\ttotal: 3m 37s\tremaining: 3m 26s\n",
            "512:\tlearn: 0.0370228\ttotal: 3m 37s\tremaining: 3m 26s\n",
            "513:\tlearn: 0.0370225\ttotal: 3m 38s\tremaining: 3m 26s\n",
            "514:\tlearn: 0.0370223\ttotal: 3m 38s\tremaining: 3m 25s\n",
            "515:\tlearn: 0.0370219\ttotal: 3m 39s\tremaining: 3m 25s\n",
            "516:\tlearn: 0.0370217\ttotal: 3m 39s\tremaining: 3m 24s\n",
            "517:\tlearn: 0.0370215\ttotal: 3m 40s\tremaining: 3m 24s\n",
            "518:\tlearn: 0.0370213\ttotal: 3m 40s\tremaining: 3m 23s\n",
            "519:\tlearn: 0.0370210\ttotal: 3m 40s\tremaining: 3m 23s\n",
            "520:\tlearn: 0.0370208\ttotal: 3m 41s\tremaining: 3m 23s\n",
            "521:\tlearn: 0.0370206\ttotal: 3m 41s\tremaining: 3m 22s\n",
            "522:\tlearn: 0.0370205\ttotal: 3m 42s\tremaining: 3m 22s\n",
            "523:\tlearn: 0.0370201\ttotal: 3m 42s\tremaining: 3m 21s\n",
            "524:\tlearn: 0.0370200\ttotal: 3m 43s\tremaining: 3m 21s\n",
            "525:\tlearn: 0.0370199\ttotal: 3m 43s\tremaining: 3m 21s\n",
            "526:\tlearn: 0.0370196\ttotal: 3m 43s\tremaining: 3m 20s\n",
            "527:\tlearn: 0.0370195\ttotal: 3m 44s\tremaining: 3m 20s\n",
            "528:\tlearn: 0.0370193\ttotal: 3m 44s\tremaining: 3m 19s\n",
            "529:\tlearn: 0.0370191\ttotal: 3m 45s\tremaining: 3m 19s\n",
            "530:\tlearn: 0.0370190\ttotal: 3m 45s\tremaining: 3m 18s\n",
            "531:\tlearn: 0.0370189\ttotal: 3m 46s\tremaining: 3m 18s\n",
            "532:\tlearn: 0.0370187\ttotal: 3m 46s\tremaining: 3m 18s\n",
            "533:\tlearn: 0.0370187\ttotal: 3m 46s\tremaining: 3m 17s\n",
            "534:\tlearn: 0.0370185\ttotal: 3m 47s\tremaining: 3m 17s\n",
            "535:\tlearn: 0.0370184\ttotal: 3m 47s\tremaining: 3m 16s\n",
            "536:\tlearn: 0.0370181\ttotal: 3m 48s\tremaining: 3m 16s\n",
            "537:\tlearn: 0.0370180\ttotal: 3m 48s\tremaining: 3m 15s\n",
            "538:\tlearn: 0.0370177\ttotal: 3m 49s\tremaining: 3m 15s\n",
            "539:\tlearn: 0.0370173\ttotal: 3m 49s\tremaining: 3m 15s\n",
            "540:\tlearn: 0.0370173\ttotal: 3m 49s\tremaining: 3m 14s\n",
            "541:\tlearn: 0.0370170\ttotal: 3m 50s\tremaining: 3m 14s\n",
            "542:\tlearn: 0.0370169\ttotal: 3m 50s\tremaining: 3m 13s\n",
            "543:\tlearn: 0.0370167\ttotal: 3m 51s\tremaining: 3m 13s\n",
            "544:\tlearn: 0.0370165\ttotal: 3m 51s\tremaining: 3m 12s\n",
            "545:\tlearn: 0.0370164\ttotal: 3m 52s\tremaining: 3m 12s\n",
            "546:\tlearn: 0.0370160\ttotal: 3m 52s\tremaining: 3m 12s\n",
            "547:\tlearn: 0.0370158\ttotal: 3m 52s\tremaining: 3m 11s\n",
            "548:\tlearn: 0.0370155\ttotal: 3m 53s\tremaining: 3m 11s\n",
            "549:\tlearn: 0.0370153\ttotal: 3m 53s\tremaining: 3m 10s\n",
            "550:\tlearn: 0.0370151\ttotal: 3m 54s\tremaining: 3m 10s\n",
            "551:\tlearn: 0.0370150\ttotal: 3m 54s\tremaining: 3m 9s\n",
            "552:\tlearn: 0.0370148\ttotal: 3m 55s\tremaining: 3m 9s\n",
            "553:\tlearn: 0.0370146\ttotal: 3m 55s\tremaining: 3m 9s\n",
            "554:\tlearn: 0.0370145\ttotal: 3m 55s\tremaining: 3m 8s\n",
            "555:\tlearn: 0.0370144\ttotal: 3m 56s\tremaining: 3m 8s\n",
            "556:\tlearn: 0.0370142\ttotal: 3m 56s\tremaining: 3m 7s\n",
            "557:\tlearn: 0.0370141\ttotal: 3m 57s\tremaining: 3m 7s\n",
            "558:\tlearn: 0.0370139\ttotal: 3m 57s\tremaining: 3m 6s\n",
            "559:\tlearn: 0.0370137\ttotal: 3m 58s\tremaining: 3m 6s\n",
            "560:\tlearn: 0.0370135\ttotal: 3m 58s\tremaining: 3m 6s\n",
            "561:\tlearn: 0.0370132\ttotal: 3m 58s\tremaining: 3m 5s\n",
            "562:\tlearn: 0.0370130\ttotal: 3m 59s\tremaining: 3m 5s\n",
            "563:\tlearn: 0.0370128\ttotal: 3m 59s\tremaining: 3m 4s\n",
            "564:\tlearn: 0.0370126\ttotal: 4m\tremaining: 3m 4s\n",
            "565:\tlearn: 0.0370124\ttotal: 4m\tremaining: 3m 4s\n",
            "566:\tlearn: 0.0370124\ttotal: 4m 1s\tremaining: 3m 3s\n",
            "567:\tlearn: 0.0370122\ttotal: 4m 1s\tremaining: 3m 3s\n",
            "568:\tlearn: 0.0370121\ttotal: 4m 1s\tremaining: 3m 2s\n",
            "569:\tlearn: 0.0370120\ttotal: 4m 2s\tremaining: 3m 2s\n",
            "570:\tlearn: 0.0370118\ttotal: 4m 2s\tremaining: 3m 1s\n",
            "571:\tlearn: 0.0370117\ttotal: 4m 3s\tremaining: 3m 1s\n",
            "572:\tlearn: 0.0370114\ttotal: 4m 3s\tremaining: 3m 1s\n",
            "573:\tlearn: 0.0370114\ttotal: 4m 3s\tremaining: 3m\n",
            "574:\tlearn: 0.0370112\ttotal: 4m 4s\tremaining: 3m\n",
            "575:\tlearn: 0.0370111\ttotal: 4m 4s\tremaining: 2m 59s\n",
            "576:\tlearn: 0.0370110\ttotal: 4m 5s\tremaining: 2m 59s\n",
            "577:\tlearn: 0.0370108\ttotal: 4m 5s\tremaining: 2m 58s\n",
            "578:\tlearn: 0.0370107\ttotal: 4m 6s\tremaining: 2m 58s\n",
            "579:\tlearn: 0.0370106\ttotal: 4m 6s\tremaining: 2m 58s\n",
            "580:\tlearn: 0.0370105\ttotal: 4m 6s\tremaining: 2m 57s\n",
            "581:\tlearn: 0.0370104\ttotal: 4m 7s\tremaining: 2m 57s\n",
            "582:\tlearn: 0.0370102\ttotal: 4m 7s\tremaining: 2m 56s\n",
            "583:\tlearn: 0.0370102\ttotal: 4m 8s\tremaining: 2m 56s\n",
            "584:\tlearn: 0.0370101\ttotal: 4m 8s\tremaining: 2m 55s\n",
            "585:\tlearn: 0.0370100\ttotal: 4m 9s\tremaining: 2m 55s\n",
            "586:\tlearn: 0.0370099\ttotal: 4m 9s\tremaining: 2m 55s\n",
            "587:\tlearn: 0.0370097\ttotal: 4m 9s\tremaining: 2m 54s\n",
            "588:\tlearn: 0.0370095\ttotal: 4m 10s\tremaining: 2m 54s\n",
            "589:\tlearn: 0.0370093\ttotal: 4m 10s\tremaining: 2m 53s\n",
            "590:\tlearn: 0.0370093\ttotal: 4m 11s\tremaining: 2m 53s\n",
            "591:\tlearn: 0.0370091\ttotal: 4m 11s\tremaining: 2m 52s\n",
            "592:\tlearn: 0.0370090\ttotal: 4m 12s\tremaining: 2m 52s\n",
            "593:\tlearn: 0.0370088\ttotal: 4m 12s\tremaining: 2m 52s\n",
            "594:\tlearn: 0.0370087\ttotal: 4m 12s\tremaining: 2m 51s\n",
            "595:\tlearn: 0.0370086\ttotal: 4m 13s\tremaining: 2m 51s\n",
            "596:\tlearn: 0.0370085\ttotal: 4m 13s\tremaining: 2m 50s\n",
            "597:\tlearn: 0.0370084\ttotal: 4m 14s\tremaining: 2m 50s\n",
            "598:\tlearn: 0.0370082\ttotal: 4m 14s\tremaining: 2m 50s\n",
            "599:\tlearn: 0.0370081\ttotal: 4m 15s\tremaining: 2m 49s\n",
            "600:\tlearn: 0.0370080\ttotal: 4m 15s\tremaining: 2m 49s\n",
            "601:\tlearn: 0.0370079\ttotal: 4m 15s\tremaining: 2m 48s\n",
            "602:\tlearn: 0.0370078\ttotal: 4m 16s\tremaining: 2m 48s\n",
            "603:\tlearn: 0.0370077\ttotal: 4m 16s\tremaining: 2m 47s\n",
            "604:\tlearn: 0.0370076\ttotal: 4m 17s\tremaining: 2m 47s\n",
            "605:\tlearn: 0.0370075\ttotal: 4m 17s\tremaining: 2m 47s\n",
            "606:\tlearn: 0.0370072\ttotal: 4m 18s\tremaining: 2m 46s\n",
            "607:\tlearn: 0.0370071\ttotal: 4m 18s\tremaining: 2m 46s\n",
            "608:\tlearn: 0.0370070\ttotal: 4m 18s\tremaining: 2m 45s\n",
            "609:\tlearn: 0.0370069\ttotal: 4m 19s\tremaining: 2m 45s\n",
            "610:\tlearn: 0.0370068\ttotal: 4m 19s\tremaining: 2m 44s\n",
            "611:\tlearn: 0.0370067\ttotal: 4m 20s\tremaining: 2m 44s\n",
            "612:\tlearn: 0.0370066\ttotal: 4m 20s\tremaining: 2m 44s\n",
            "613:\tlearn: 0.0370065\ttotal: 4m 20s\tremaining: 2m 43s\n",
            "614:\tlearn: 0.0370064\ttotal: 4m 21s\tremaining: 2m 43s\n",
            "615:\tlearn: 0.0370062\ttotal: 4m 21s\tremaining: 2m 42s\n",
            "616:\tlearn: 0.0370061\ttotal: 4m 22s\tremaining: 2m 42s\n",
            "617:\tlearn: 0.0370059\ttotal: 4m 22s\tremaining: 2m 41s\n",
            "618:\tlearn: 0.0370059\ttotal: 4m 23s\tremaining: 2m 41s\n",
            "619:\tlearn: 0.0370058\ttotal: 4m 23s\tremaining: 2m 41s\n",
            "620:\tlearn: 0.0370056\ttotal: 4m 23s\tremaining: 2m 40s\n",
            "621:\tlearn: 0.0370055\ttotal: 4m 24s\tremaining: 2m 40s\n",
            "622:\tlearn: 0.0370055\ttotal: 4m 24s\tremaining: 2m 39s\n",
            "623:\tlearn: 0.0370053\ttotal: 4m 25s\tremaining: 2m 39s\n",
            "624:\tlearn: 0.0370051\ttotal: 4m 25s\tremaining: 2m 38s\n",
            "625:\tlearn: 0.0370049\ttotal: 4m 25s\tremaining: 2m 38s\n",
            "626:\tlearn: 0.0370047\ttotal: 4m 26s\tremaining: 2m 38s\n",
            "627:\tlearn: 0.0370045\ttotal: 4m 26s\tremaining: 2m 37s\n",
            "628:\tlearn: 0.0370044\ttotal: 4m 27s\tremaining: 2m 37s\n",
            "629:\tlearn: 0.0370042\ttotal: 4m 27s\tremaining: 2m 36s\n",
            "630:\tlearn: 0.0370041\ttotal: 4m 28s\tremaining: 2m 36s\n",
            "631:\tlearn: 0.0370039\ttotal: 4m 28s\tremaining: 2m 35s\n",
            "632:\tlearn: 0.0370037\ttotal: 4m 28s\tremaining: 2m 35s\n",
            "633:\tlearn: 0.0370036\ttotal: 4m 29s\tremaining: 2m 35s\n",
            "634:\tlearn: 0.0370035\ttotal: 4m 29s\tremaining: 2m 34s\n",
            "635:\tlearn: 0.0370034\ttotal: 4m 30s\tremaining: 2m 34s\n",
            "636:\tlearn: 0.0370032\ttotal: 4m 30s\tremaining: 2m 33s\n",
            "637:\tlearn: 0.0370031\ttotal: 4m 31s\tremaining: 2m 33s\n",
            "638:\tlearn: 0.0370030\ttotal: 4m 31s\tremaining: 2m 32s\n",
            "639:\tlearn: 0.0370029\ttotal: 4m 31s\tremaining: 2m 32s\n",
            "640:\tlearn: 0.0370026\ttotal: 4m 32s\tremaining: 2m 32s\n",
            "641:\tlearn: 0.0370025\ttotal: 4m 32s\tremaining: 2m 31s\n",
            "642:\tlearn: 0.0370024\ttotal: 4m 33s\tremaining: 2m 31s\n",
            "643:\tlearn: 0.0370023\ttotal: 4m 33s\tremaining: 2m 30s\n",
            "644:\tlearn: 0.0370022\ttotal: 4m 33s\tremaining: 2m 30s\n",
            "645:\tlearn: 0.0370021\ttotal: 4m 34s\tremaining: 2m 29s\n",
            "646:\tlearn: 0.0370020\ttotal: 4m 34s\tremaining: 2m 29s\n",
            "647:\tlearn: 0.0370019\ttotal: 4m 35s\tremaining: 2m 29s\n",
            "648:\tlearn: 0.0370018\ttotal: 4m 35s\tremaining: 2m 28s\n",
            "649:\tlearn: 0.0370017\ttotal: 4m 36s\tremaining: 2m 28s\n",
            "650:\tlearn: 0.0370015\ttotal: 4m 36s\tremaining: 2m 27s\n",
            "651:\tlearn: 0.0370013\ttotal: 4m 36s\tremaining: 2m 27s\n",
            "652:\tlearn: 0.0370012\ttotal: 4m 37s\tremaining: 2m 26s\n",
            "653:\tlearn: 0.0370011\ttotal: 4m 37s\tremaining: 2m 26s\n",
            "654:\tlearn: 0.0370009\ttotal: 4m 38s\tremaining: 2m 26s\n",
            "655:\tlearn: 0.0370007\ttotal: 4m 38s\tremaining: 2m 25s\n",
            "656:\tlearn: 0.0370005\ttotal: 4m 39s\tremaining: 2m 25s\n",
            "657:\tlearn: 0.0370004\ttotal: 4m 39s\tremaining: 2m 24s\n",
            "658:\tlearn: 0.0370002\ttotal: 4m 39s\tremaining: 2m 24s\n",
            "659:\tlearn: 0.0370001\ttotal: 4m 40s\tremaining: 2m 23s\n",
            "660:\tlearn: 0.0370001\ttotal: 4m 40s\tremaining: 2m 23s\n",
            "661:\tlearn: 0.0370000\ttotal: 4m 40s\tremaining: 2m 23s\n",
            "662:\tlearn: 0.0370000\ttotal: 4m 41s\tremaining: 2m 22s\n",
            "663:\tlearn: 0.0369999\ttotal: 4m 41s\tremaining: 2m 22s\n",
            "664:\tlearn: 0.0369998\ttotal: 4m 42s\tremaining: 2m 21s\n",
            "665:\tlearn: 0.0369997\ttotal: 4m 42s\tremaining: 2m 21s\n",
            "666:\tlearn: 0.0369995\ttotal: 4m 43s\tremaining: 2m 20s\n",
            "667:\tlearn: 0.0369994\ttotal: 4m 43s\tremaining: 2m 20s\n",
            "668:\tlearn: 0.0369993\ttotal: 4m 43s\tremaining: 2m 20s\n",
            "669:\tlearn: 0.0369993\ttotal: 4m 44s\tremaining: 2m 19s\n",
            "670:\tlearn: 0.0369991\ttotal: 4m 44s\tremaining: 2m 19s\n",
            "671:\tlearn: 0.0369990\ttotal: 4m 45s\tremaining: 2m 18s\n",
            "672:\tlearn: 0.0369990\ttotal: 4m 45s\tremaining: 2m 18s\n",
            "673:\tlearn: 0.0369989\ttotal: 4m 46s\tremaining: 2m 17s\n",
            "674:\tlearn: 0.0369989\ttotal: 4m 46s\tremaining: 2m 17s\n",
            "675:\tlearn: 0.0369987\ttotal: 4m 46s\tremaining: 2m 17s\n",
            "676:\tlearn: 0.0369987\ttotal: 4m 47s\tremaining: 2m 16s\n",
            "677:\tlearn: 0.0369986\ttotal: 4m 47s\tremaining: 2m 16s\n",
            "678:\tlearn: 0.0369986\ttotal: 4m 48s\tremaining: 2m 15s\n",
            "679:\tlearn: 0.0369984\ttotal: 4m 48s\tremaining: 2m 15s\n",
            "680:\tlearn: 0.0369983\ttotal: 4m 49s\tremaining: 2m 14s\n",
            "681:\tlearn: 0.0369982\ttotal: 4m 49s\tremaining: 2m 14s\n",
            "682:\tlearn: 0.0369981\ttotal: 4m 49s\tremaining: 2m 14s\n",
            "683:\tlearn: 0.0369980\ttotal: 4m 50s\tremaining: 2m 13s\n",
            "684:\tlearn: 0.0369979\ttotal: 4m 50s\tremaining: 2m 13s\n",
            "685:\tlearn: 0.0369978\ttotal: 4m 51s\tremaining: 2m 12s\n",
            "686:\tlearn: 0.0369977\ttotal: 4m 51s\tremaining: 2m 12s\n",
            "687:\tlearn: 0.0369977\ttotal: 4m 51s\tremaining: 2m 11s\n",
            "688:\tlearn: 0.0369975\ttotal: 4m 52s\tremaining: 2m 11s\n",
            "689:\tlearn: 0.0369973\ttotal: 4m 52s\tremaining: 2m 11s\n",
            "690:\tlearn: 0.0369972\ttotal: 4m 53s\tremaining: 2m 10s\n",
            "691:\tlearn: 0.0369972\ttotal: 4m 53s\tremaining: 2m 10s\n",
            "692:\tlearn: 0.0369971\ttotal: 4m 54s\tremaining: 2m 9s\n",
            "693:\tlearn: 0.0369971\ttotal: 4m 54s\tremaining: 2m 9s\n",
            "694:\tlearn: 0.0369970\ttotal: 4m 54s\tremaining: 2m 8s\n",
            "695:\tlearn: 0.0369969\ttotal: 4m 55s\tremaining: 2m 8s\n",
            "696:\tlearn: 0.0369968\ttotal: 4m 55s\tremaining: 2m 8s\n",
            "697:\tlearn: 0.0369967\ttotal: 4m 56s\tremaining: 2m 7s\n",
            "698:\tlearn: 0.0369966\ttotal: 4m 56s\tremaining: 2m 7s\n",
            "699:\tlearn: 0.0369965\ttotal: 4m 56s\tremaining: 2m 6s\n",
            "700:\tlearn: 0.0369963\ttotal: 4m 57s\tremaining: 2m 6s\n",
            "701:\tlearn: 0.0369962\ttotal: 4m 57s\tremaining: 2m 6s\n",
            "702:\tlearn: 0.0369961\ttotal: 4m 58s\tremaining: 2m 5s\n",
            "703:\tlearn: 0.0369960\ttotal: 4m 58s\tremaining: 2m 5s\n",
            "704:\tlearn: 0.0369959\ttotal: 4m 59s\tremaining: 2m 4s\n",
            "705:\tlearn: 0.0369958\ttotal: 4m 59s\tremaining: 2m 4s\n",
            "706:\tlearn: 0.0369957\ttotal: 4m 59s\tremaining: 2m 3s\n",
            "707:\tlearn: 0.0369955\ttotal: 5m\tremaining: 2m 3s\n",
            "708:\tlearn: 0.0369955\ttotal: 5m\tremaining: 2m 3s\n",
            "709:\tlearn: 0.0369954\ttotal: 5m 1s\tremaining: 2m 2s\n",
            "710:\tlearn: 0.0369953\ttotal: 5m 1s\tremaining: 2m 2s\n",
            "711:\tlearn: 0.0369952\ttotal: 5m 2s\tremaining: 2m 1s\n",
            "712:\tlearn: 0.0369951\ttotal: 5m 2s\tremaining: 2m 1s\n",
            "713:\tlearn: 0.0369949\ttotal: 5m 2s\tremaining: 2m\n",
            "714:\tlearn: 0.0369948\ttotal: 5m 3s\tremaining: 2m\n",
            "715:\tlearn: 0.0369947\ttotal: 5m 3s\tremaining: 2m\n",
            "716:\tlearn: 0.0369946\ttotal: 5m 4s\tremaining: 1m 59s\n",
            "717:\tlearn: 0.0369945\ttotal: 5m 4s\tremaining: 1m 59s\n",
            "718:\tlearn: 0.0369944\ttotal: 5m 4s\tremaining: 1m 58s\n",
            "719:\tlearn: 0.0369943\ttotal: 5m 5s\tremaining: 1m 58s\n",
            "720:\tlearn: 0.0369942\ttotal: 5m 5s\tremaining: 1m 57s\n",
            "721:\tlearn: 0.0369941\ttotal: 5m 6s\tremaining: 1m 57s\n",
            "722:\tlearn: 0.0369941\ttotal: 5m 6s\tremaining: 1m 57s\n",
            "723:\tlearn: 0.0369940\ttotal: 5m 7s\tremaining: 1m 56s\n",
            "724:\tlearn: 0.0369938\ttotal: 5m 7s\tremaining: 1m 56s\n",
            "725:\tlearn: 0.0369937\ttotal: 5m 7s\tremaining: 1m 55s\n",
            "726:\tlearn: 0.0369937\ttotal: 5m 8s\tremaining: 1m 55s\n",
            "727:\tlearn: 0.0369936\ttotal: 5m 8s\tremaining: 1m 54s\n",
            "728:\tlearn: 0.0369935\ttotal: 5m 9s\tremaining: 1m 54s\n",
            "729:\tlearn: 0.0369934\ttotal: 5m 9s\tremaining: 1m 54s\n",
            "730:\tlearn: 0.0369933\ttotal: 5m 10s\tremaining: 1m 53s\n",
            "731:\tlearn: 0.0369932\ttotal: 5m 10s\tremaining: 1m 53s\n",
            "732:\tlearn: 0.0369932\ttotal: 5m 10s\tremaining: 1m 52s\n",
            "733:\tlearn: 0.0369931\ttotal: 5m 11s\tremaining: 1m 52s\n",
            "734:\tlearn: 0.0369930\ttotal: 5m 11s\tremaining: 1m 51s\n",
            "735:\tlearn: 0.0369929\ttotal: 5m 12s\tremaining: 1m 51s\n",
            "736:\tlearn: 0.0369929\ttotal: 5m 12s\tremaining: 1m 51s\n",
            "737:\tlearn: 0.0369928\ttotal: 5m 12s\tremaining: 1m 50s\n",
            "738:\tlearn: 0.0369927\ttotal: 5m 13s\tremaining: 1m 50s\n",
            "739:\tlearn: 0.0369927\ttotal: 5m 13s\tremaining: 1m 49s\n",
            "740:\tlearn: 0.0369926\ttotal: 5m 14s\tremaining: 1m 49s\n",
            "741:\tlearn: 0.0369924\ttotal: 5m 14s\tremaining: 1m 48s\n",
            "742:\tlearn: 0.0369923\ttotal: 5m 15s\tremaining: 1m 48s\n",
            "743:\tlearn: 0.0369923\ttotal: 5m 15s\tremaining: 1m 48s\n",
            "744:\tlearn: 0.0369922\ttotal: 5m 16s\tremaining: 1m 47s\n",
            "745:\tlearn: 0.0369922\ttotal: 5m 16s\tremaining: 1m 47s\n",
            "746:\tlearn: 0.0369921\ttotal: 5m 16s\tremaining: 1m 46s\n",
            "747:\tlearn: 0.0369921\ttotal: 5m 17s\tremaining: 1m 46s\n",
            "748:\tlearn: 0.0369920\ttotal: 5m 17s\tremaining: 1m 46s\n",
            "749:\tlearn: 0.0369919\ttotal: 5m 18s\tremaining: 1m 45s\n",
            "750:\tlearn: 0.0369918\ttotal: 5m 18s\tremaining: 1m 45s\n",
            "751:\tlearn: 0.0369918\ttotal: 5m 19s\tremaining: 1m 44s\n",
            "752:\tlearn: 0.0369917\ttotal: 5m 19s\tremaining: 1m 44s\n",
            "753:\tlearn: 0.0369917\ttotal: 5m 19s\tremaining: 1m 43s\n",
            "754:\tlearn: 0.0369916\ttotal: 5m 20s\tremaining: 1m 43s\n",
            "755:\tlearn: 0.0369915\ttotal: 5m 20s\tremaining: 1m 43s\n",
            "756:\tlearn: 0.0369915\ttotal: 5m 21s\tremaining: 1m 42s\n",
            "757:\tlearn: 0.0369914\ttotal: 5m 21s\tremaining: 1m 42s\n",
            "758:\tlearn: 0.0369914\ttotal: 5m 22s\tremaining: 1m 41s\n",
            "759:\tlearn: 0.0369913\ttotal: 5m 22s\tremaining: 1m 41s\n",
            "760:\tlearn: 0.0369913\ttotal: 5m 22s\tremaining: 1m 40s\n",
            "761:\tlearn: 0.0369912\ttotal: 5m 23s\tremaining: 1m 40s\n",
            "762:\tlearn: 0.0369912\ttotal: 5m 23s\tremaining: 1m 40s\n",
            "763:\tlearn: 0.0369911\ttotal: 5m 24s\tremaining: 1m 39s\n",
            "764:\tlearn: 0.0369910\ttotal: 5m 24s\tremaining: 1m 39s\n",
            "765:\tlearn: 0.0369909\ttotal: 5m 25s\tremaining: 1m 38s\n",
            "766:\tlearn: 0.0369909\ttotal: 5m 25s\tremaining: 1m 38s\n",
            "767:\tlearn: 0.0369908\ttotal: 5m 25s\tremaining: 1m 38s\n",
            "768:\tlearn: 0.0369907\ttotal: 5m 26s\tremaining: 1m 37s\n",
            "769:\tlearn: 0.0369907\ttotal: 5m 26s\tremaining: 1m 37s\n",
            "770:\tlearn: 0.0369906\ttotal: 5m 27s\tremaining: 1m 36s\n",
            "771:\tlearn: 0.0369905\ttotal: 5m 27s\tremaining: 1m 36s\n",
            "772:\tlearn: 0.0369905\ttotal: 5m 28s\tremaining: 1m 35s\n",
            "773:\tlearn: 0.0369903\ttotal: 5m 28s\tremaining: 1m 35s\n",
            "774:\tlearn: 0.0369902\ttotal: 5m 28s\tremaining: 1m 34s\n",
            "775:\tlearn: 0.0369902\ttotal: 5m 29s\tremaining: 1m 34s\n",
            "776:\tlearn: 0.0369901\ttotal: 5m 29s\tremaining: 1m 34s\n",
            "777:\tlearn: 0.0369900\ttotal: 5m 29s\tremaining: 1m 33s\n",
            "778:\tlearn: 0.0369899\ttotal: 5m 30s\tremaining: 1m 33s\n",
            "779:\tlearn: 0.0369899\ttotal: 5m 30s\tremaining: 1m 32s\n",
            "780:\tlearn: 0.0369898\ttotal: 5m 31s\tremaining: 1m 32s\n",
            "781:\tlearn: 0.0369898\ttotal: 5m 31s\tremaining: 1m 32s\n",
            "782:\tlearn: 0.0369897\ttotal: 5m 32s\tremaining: 1m 31s\n",
            "783:\tlearn: 0.0369897\ttotal: 5m 32s\tremaining: 1m 31s\n",
            "784:\tlearn: 0.0369896\ttotal: 5m 32s\tremaining: 1m 30s\n",
            "785:\tlearn: 0.0369896\ttotal: 5m 33s\tremaining: 1m 30s\n",
            "786:\tlearn: 0.0369895\ttotal: 5m 33s\tremaining: 1m 29s\n",
            "787:\tlearn: 0.0369895\ttotal: 5m 34s\tremaining: 1m 29s\n",
            "788:\tlearn: 0.0369894\ttotal: 5m 34s\tremaining: 1m 29s\n",
            "789:\tlearn: 0.0369893\ttotal: 5m 35s\tremaining: 1m 28s\n",
            "790:\tlearn: 0.0369892\ttotal: 5m 35s\tremaining: 1m 28s\n",
            "791:\tlearn: 0.0369892\ttotal: 5m 35s\tremaining: 1m 27s\n",
            "792:\tlearn: 0.0369891\ttotal: 5m 36s\tremaining: 1m 27s\n",
            "793:\tlearn: 0.0369890\ttotal: 5m 36s\tremaining: 1m 26s\n",
            "794:\tlearn: 0.0369890\ttotal: 5m 37s\tremaining: 1m 26s\n",
            "795:\tlearn: 0.0369890\ttotal: 5m 37s\tremaining: 1m 26s\n",
            "796:\tlearn: 0.0369889\ttotal: 5m 38s\tremaining: 1m 25s\n",
            "797:\tlearn: 0.0369888\ttotal: 5m 38s\tremaining: 1m 25s\n",
            "798:\tlearn: 0.0369888\ttotal: 5m 38s\tremaining: 1m 24s\n",
            "799:\tlearn: 0.0369887\ttotal: 5m 39s\tremaining: 1m 24s\n",
            "800:\tlearn: 0.0369886\ttotal: 5m 39s\tremaining: 1m 23s\n",
            "801:\tlearn: 0.0369886\ttotal: 5m 40s\tremaining: 1m 23s\n",
            "802:\tlearn: 0.0369885\ttotal: 5m 40s\tremaining: 1m 23s\n",
            "803:\tlearn: 0.0369885\ttotal: 5m 40s\tremaining: 1m 22s\n",
            "804:\tlearn: 0.0369885\ttotal: 5m 41s\tremaining: 1m 22s\n",
            "805:\tlearn: 0.0369884\ttotal: 5m 41s\tremaining: 1m 21s\n",
            "806:\tlearn: 0.0369883\ttotal: 5m 42s\tremaining: 1m 21s\n",
            "807:\tlearn: 0.0369883\ttotal: 5m 42s\tremaining: 1m 21s\n",
            "808:\tlearn: 0.0369882\ttotal: 5m 43s\tremaining: 1m 20s\n",
            "809:\tlearn: 0.0369882\ttotal: 5m 43s\tremaining: 1m 20s\n",
            "810:\tlearn: 0.0369881\ttotal: 5m 43s\tremaining: 1m 19s\n",
            "811:\tlearn: 0.0369880\ttotal: 5m 44s\tremaining: 1m 19s\n",
            "812:\tlearn: 0.0369880\ttotal: 5m 44s\tremaining: 1m 18s\n",
            "813:\tlearn: 0.0369879\ttotal: 5m 45s\tremaining: 1m 18s\n",
            "814:\tlearn: 0.0369879\ttotal: 5m 45s\tremaining: 1m 18s\n",
            "815:\tlearn: 0.0369878\ttotal: 5m 46s\tremaining: 1m 17s\n",
            "816:\tlearn: 0.0369878\ttotal: 5m 46s\tremaining: 1m 17s\n",
            "817:\tlearn: 0.0369877\ttotal: 5m 47s\tremaining: 1m 16s\n",
            "818:\tlearn: 0.0369877\ttotal: 5m 47s\tremaining: 1m 16s\n",
            "819:\tlearn: 0.0369876\ttotal: 5m 47s\tremaining: 1m 15s\n",
            "820:\tlearn: 0.0369876\ttotal: 5m 48s\tremaining: 1m 15s\n",
            "821:\tlearn: 0.0369875\ttotal: 5m 48s\tremaining: 1m 15s\n",
            "822:\tlearn: 0.0369875\ttotal: 5m 49s\tremaining: 1m 14s\n",
            "823:\tlearn: 0.0369875\ttotal: 5m 49s\tremaining: 1m 14s\n",
            "824:\tlearn: 0.0369874\ttotal: 5m 50s\tremaining: 1m 13s\n",
            "825:\tlearn: 0.0369873\ttotal: 5m 50s\tremaining: 1m 13s\n",
            "826:\tlearn: 0.0369873\ttotal: 5m 50s\tremaining: 1m 12s\n",
            "827:\tlearn: 0.0369872\ttotal: 5m 51s\tremaining: 1m 12s\n",
            "828:\tlearn: 0.0369871\ttotal: 5m 51s\tremaining: 1m 12s\n",
            "829:\tlearn: 0.0369871\ttotal: 5m 52s\tremaining: 1m 11s\n",
            "830:\tlearn: 0.0369870\ttotal: 5m 52s\tremaining: 1m 11s\n",
            "831:\tlearn: 0.0369870\ttotal: 5m 53s\tremaining: 1m 10s\n",
            "832:\tlearn: 0.0369869\ttotal: 5m 53s\tremaining: 1m 10s\n",
            "833:\tlearn: 0.0369869\ttotal: 5m 54s\tremaining: 1m 10s\n",
            "834:\tlearn: 0.0369868\ttotal: 5m 54s\tremaining: 1m 9s\n",
            "835:\tlearn: 0.0369867\ttotal: 5m 54s\tremaining: 1m 9s\n",
            "836:\tlearn: 0.0369867\ttotal: 5m 55s\tremaining: 1m 8s\n",
            "837:\tlearn: 0.0369866\ttotal: 5m 55s\tremaining: 1m 8s\n",
            "838:\tlearn: 0.0369865\ttotal: 5m 56s\tremaining: 1m 7s\n",
            "839:\tlearn: 0.0369865\ttotal: 5m 56s\tremaining: 1m 7s\n",
            "840:\tlearn: 0.0369864\ttotal: 5m 57s\tremaining: 1m 7s\n",
            "841:\tlearn: 0.0369863\ttotal: 5m 57s\tremaining: 1m 6s\n",
            "842:\tlearn: 0.0369863\ttotal: 5m 57s\tremaining: 1m 6s\n",
            "843:\tlearn: 0.0369862\ttotal: 5m 58s\tremaining: 1m 5s\n",
            "844:\tlearn: 0.0369862\ttotal: 5m 58s\tremaining: 1m 5s\n",
            "845:\tlearn: 0.0369861\ttotal: 5m 59s\tremaining: 1m 4s\n",
            "846:\tlearn: 0.0369861\ttotal: 5m 59s\tremaining: 1m 4s\n",
            "847:\tlearn: 0.0369860\ttotal: 6m\tremaining: 1m 4s\n",
            "848:\tlearn: 0.0369859\ttotal: 6m\tremaining: 1m 3s\n",
            "849:\tlearn: 0.0369859\ttotal: 6m\tremaining: 1m 3s\n",
            "850:\tlearn: 0.0369858\ttotal: 6m 1s\tremaining: 1m 2s\n",
            "851:\tlearn: 0.0369858\ttotal: 6m 1s\tremaining: 1m 2s\n",
            "852:\tlearn: 0.0369857\ttotal: 6m 2s\tremaining: 1m 2s\n",
            "853:\tlearn: 0.0369857\ttotal: 6m 2s\tremaining: 1m 1s\n",
            "854:\tlearn: 0.0369856\ttotal: 6m 3s\tremaining: 1m 1s\n",
            "855:\tlearn: 0.0369856\ttotal: 6m 3s\tremaining: 1m\n",
            "856:\tlearn: 0.0369855\ttotal: 6m 3s\tremaining: 1m\n",
            "857:\tlearn: 0.0369855\ttotal: 6m 4s\tremaining: 59.9s\n",
            "858:\tlearn: 0.0369855\ttotal: 6m 4s\tremaining: 59.5s\n",
            "859:\tlearn: 0.0369854\ttotal: 6m 5s\tremaining: 59s\n",
            "860:\tlearn: 0.0369854\ttotal: 6m 5s\tremaining: 58.6s\n",
            "861:\tlearn: 0.0369853\ttotal: 6m 6s\tremaining: 58.2s\n",
            "862:\tlearn: 0.0369852\ttotal: 6m 6s\tremaining: 57.8s\n",
            "863:\tlearn: 0.0369851\ttotal: 6m 6s\tremaining: 57.3s\n",
            "864:\tlearn: 0.0369851\ttotal: 6m 7s\tremaining: 56.9s\n",
            "865:\tlearn: 0.0369850\ttotal: 6m 7s\tremaining: 56.5s\n",
            "866:\tlearn: 0.0369849\ttotal: 6m 8s\tremaining: 56.1s\n",
            "867:\tlearn: 0.0369849\ttotal: 6m 8s\tremaining: 55.6s\n",
            "868:\tlearn: 0.0369848\ttotal: 6m 9s\tremaining: 55.2s\n",
            "869:\tlearn: 0.0369848\ttotal: 6m 9s\tremaining: 54.8s\n",
            "870:\tlearn: 0.0369847\ttotal: 6m 9s\tremaining: 54.3s\n",
            "871:\tlearn: 0.0369847\ttotal: 6m 10s\tremaining: 53.9s\n",
            "872:\tlearn: 0.0369847\ttotal: 6m 10s\tremaining: 53.5s\n",
            "873:\tlearn: 0.0369846\ttotal: 6m 11s\tremaining: 53.1s\n",
            "874:\tlearn: 0.0369846\ttotal: 6m 11s\tremaining: 52.6s\n",
            "875:\tlearn: 0.0369846\ttotal: 6m 11s\tremaining: 52.2s\n",
            "876:\tlearn: 0.0369845\ttotal: 6m 12s\tremaining: 51.8s\n",
            "877:\tlearn: 0.0369845\ttotal: 6m 12s\tremaining: 51.4s\n",
            "878:\tlearn: 0.0369845\ttotal: 6m 13s\tremaining: 50.9s\n",
            "879:\tlearn: 0.0369844\ttotal: 6m 13s\tremaining: 50.5s\n",
            "880:\tlearn: 0.0369843\ttotal: 6m 14s\tremaining: 50.1s\n",
            "881:\tlearn: 0.0369843\ttotal: 6m 14s\tremaining: 49.7s\n",
            "882:\tlearn: 0.0369842\ttotal: 6m 14s\tremaining: 49.3s\n",
            "883:\tlearn: 0.0369842\ttotal: 6m 15s\tremaining: 48.8s\n",
            "884:\tlearn: 0.0369841\ttotal: 6m 15s\tremaining: 48.4s\n",
            "885:\tlearn: 0.0369841\ttotal: 6m 16s\tremaining: 48s\n",
            "886:\tlearn: 0.0369840\ttotal: 6m 16s\tremaining: 47.6s\n",
            "887:\tlearn: 0.0369840\ttotal: 6m 17s\tremaining: 47.1s\n",
            "888:\tlearn: 0.0369840\ttotal: 6m 17s\tremaining: 46.7s\n",
            "889:\tlearn: 0.0369839\ttotal: 6m 17s\tremaining: 46.3s\n",
            "890:\tlearn: 0.0369839\ttotal: 6m 18s\tremaining: 45.9s\n",
            "891:\tlearn: 0.0369839\ttotal: 6m 18s\tremaining: 45.4s\n",
            "892:\tlearn: 0.0369838\ttotal: 6m 19s\tremaining: 45s\n",
            "893:\tlearn: 0.0369837\ttotal: 6m 19s\tremaining: 44.6s\n",
            "894:\tlearn: 0.0369837\ttotal: 6m 19s\tremaining: 44.2s\n",
            "895:\tlearn: 0.0369837\ttotal: 6m 20s\tremaining: 43.7s\n",
            "896:\tlearn: 0.0369836\ttotal: 6m 20s\tremaining: 43.3s\n",
            "897:\tlearn: 0.0369836\ttotal: 6m 21s\tremaining: 42.9s\n",
            "898:\tlearn: 0.0369835\ttotal: 6m 21s\tremaining: 42.5s\n",
            "899:\tlearn: 0.0369835\ttotal: 6m 22s\tremaining: 42s\n",
            "900:\tlearn: 0.0369834\ttotal: 6m 22s\tremaining: 41.6s\n",
            "901:\tlearn: 0.0369834\ttotal: 6m 22s\tremaining: 41.2s\n",
            "902:\tlearn: 0.0369833\ttotal: 6m 23s\tremaining: 40.8s\n",
            "903:\tlearn: 0.0369833\ttotal: 6m 23s\tremaining: 40.3s\n",
            "904:\tlearn: 0.0369833\ttotal: 6m 24s\tremaining: 39.9s\n",
            "905:\tlearn: 0.0369832\ttotal: 6m 24s\tremaining: 39.5s\n",
            "906:\tlearn: 0.0369832\ttotal: 6m 25s\tremaining: 39.1s\n",
            "907:\tlearn: 0.0369831\ttotal: 6m 25s\tremaining: 38.6s\n",
            "908:\tlearn: 0.0369831\ttotal: 6m 25s\tremaining: 38.2s\n",
            "909:\tlearn: 0.0369830\ttotal: 6m 26s\tremaining: 37.8s\n",
            "910:\tlearn: 0.0369830\ttotal: 6m 26s\tremaining: 37.4s\n",
            "911:\tlearn: 0.0369829\ttotal: 6m 27s\tremaining: 36.9s\n",
            "912:\tlearn: 0.0369829\ttotal: 6m 27s\tremaining: 36.5s\n",
            "913:\tlearn: 0.0369828\ttotal: 6m 28s\tremaining: 36.1s\n",
            "914:\tlearn: 0.0369827\ttotal: 6m 28s\tremaining: 35.7s\n",
            "915:\tlearn: 0.0369827\ttotal: 6m 28s\tremaining: 35.2s\n",
            "916:\tlearn: 0.0369826\ttotal: 6m 29s\tremaining: 34.8s\n",
            "917:\tlearn: 0.0369826\ttotal: 6m 29s\tremaining: 34.4s\n",
            "918:\tlearn: 0.0369825\ttotal: 6m 30s\tremaining: 34s\n",
            "919:\tlearn: 0.0369825\ttotal: 6m 30s\tremaining: 33.5s\n",
            "920:\tlearn: 0.0369824\ttotal: 6m 31s\tremaining: 33.1s\n",
            "921:\tlearn: 0.0369824\ttotal: 6m 31s\tremaining: 32.7s\n",
            "922:\tlearn: 0.0369824\ttotal: 6m 31s\tremaining: 32.3s\n",
            "923:\tlearn: 0.0369823\ttotal: 6m 32s\tremaining: 31.9s\n",
            "924:\tlearn: 0.0369823\ttotal: 6m 32s\tremaining: 31.4s\n",
            "925:\tlearn: 0.0369823\ttotal: 6m 33s\tremaining: 31s\n",
            "926:\tlearn: 0.0369822\ttotal: 6m 33s\tremaining: 30.6s\n",
            "927:\tlearn: 0.0369822\ttotal: 6m 34s\tremaining: 30.2s\n",
            "928:\tlearn: 0.0369822\ttotal: 6m 34s\tremaining: 29.7s\n",
            "929:\tlearn: 0.0369821\ttotal: 6m 34s\tremaining: 29.3s\n",
            "930:\tlearn: 0.0369821\ttotal: 6m 35s\tremaining: 28.9s\n",
            "931:\tlearn: 0.0369821\ttotal: 6m 35s\tremaining: 28.5s\n",
            "932:\tlearn: 0.0369820\ttotal: 6m 36s\tremaining: 28s\n",
            "933:\tlearn: 0.0369819\ttotal: 6m 36s\tremaining: 27.6s\n",
            "934:\tlearn: 0.0369819\ttotal: 6m 37s\tremaining: 27.2s\n",
            "935:\tlearn: 0.0369819\ttotal: 6m 37s\tremaining: 26.8s\n",
            "936:\tlearn: 0.0369818\ttotal: 6m 37s\tremaining: 26.3s\n",
            "937:\tlearn: 0.0369818\ttotal: 6m 38s\tremaining: 25.9s\n",
            "938:\tlearn: 0.0369817\ttotal: 6m 38s\tremaining: 25.5s\n",
            "939:\tlearn: 0.0369817\ttotal: 6m 39s\tremaining: 25.1s\n",
            "940:\tlearn: 0.0369817\ttotal: 6m 39s\tremaining: 24.6s\n",
            "941:\tlearn: 0.0369816\ttotal: 6m 40s\tremaining: 24.2s\n",
            "942:\tlearn: 0.0369816\ttotal: 6m 40s\tremaining: 23.8s\n",
            "943:\tlearn: 0.0369816\ttotal: 6m 40s\tremaining: 23.4s\n",
            "944:\tlearn: 0.0369815\ttotal: 6m 41s\tremaining: 22.9s\n",
            "945:\tlearn: 0.0369815\ttotal: 6m 41s\tremaining: 22.5s\n",
            "946:\tlearn: 0.0369814\ttotal: 6m 42s\tremaining: 22.1s\n",
            "947:\tlearn: 0.0369814\ttotal: 6m 42s\tremaining: 21.7s\n",
            "948:\tlearn: 0.0369814\ttotal: 6m 43s\tremaining: 21.2s\n",
            "949:\tlearn: 0.0369813\ttotal: 6m 43s\tremaining: 20.8s\n",
            "950:\tlearn: 0.0369813\ttotal: 6m 44s\tremaining: 20.4s\n",
            "951:\tlearn: 0.0369813\ttotal: 6m 44s\tremaining: 20s\n",
            "952:\tlearn: 0.0369812\ttotal: 6m 44s\tremaining: 19.5s\n",
            "953:\tlearn: 0.0369812\ttotal: 6m 45s\tremaining: 19.1s\n",
            "954:\tlearn: 0.0369811\ttotal: 6m 45s\tremaining: 18.7s\n",
            "955:\tlearn: 0.0369811\ttotal: 6m 46s\tremaining: 18.3s\n",
            "956:\tlearn: 0.0369811\ttotal: 6m 46s\tremaining: 17.8s\n",
            "957:\tlearn: 0.0369810\ttotal: 6m 47s\tremaining: 17.4s\n",
            "958:\tlearn: 0.0369810\ttotal: 6m 47s\tremaining: 17s\n",
            "959:\tlearn: 0.0369809\ttotal: 6m 47s\tremaining: 16.6s\n",
            "960:\tlearn: 0.0369809\ttotal: 6m 48s\tremaining: 16.1s\n",
            "961:\tlearn: 0.0369809\ttotal: 6m 48s\tremaining: 15.7s\n",
            "962:\tlearn: 0.0369809\ttotal: 6m 49s\tremaining: 15.3s\n",
            "963:\tlearn: 0.0369808\ttotal: 6m 49s\tremaining: 14.9s\n",
            "964:\tlearn: 0.0369808\ttotal: 6m 50s\tremaining: 14.5s\n",
            "965:\tlearn: 0.0369808\ttotal: 6m 50s\tremaining: 14s\n",
            "966:\tlearn: 0.0369807\ttotal: 6m 51s\tremaining: 13.6s\n",
            "967:\tlearn: 0.0369807\ttotal: 6m 51s\tremaining: 13.2s\n",
            "968:\tlearn: 0.0369807\ttotal: 6m 51s\tremaining: 12.8s\n",
            "969:\tlearn: 0.0369806\ttotal: 6m 52s\tremaining: 12.3s\n",
            "970:\tlearn: 0.0369806\ttotal: 6m 52s\tremaining: 11.9s\n",
            "971:\tlearn: 0.0369805\ttotal: 6m 53s\tremaining: 11.5s\n",
            "972:\tlearn: 0.0369805\ttotal: 6m 53s\tremaining: 11.1s\n",
            "973:\tlearn: 0.0369804\ttotal: 6m 54s\tremaining: 10.6s\n",
            "974:\tlearn: 0.0369804\ttotal: 6m 54s\tremaining: 10.2s\n",
            "975:\tlearn: 0.0369804\ttotal: 6m 54s\tremaining: 9.78s\n",
            "976:\tlearn: 0.0369803\ttotal: 6m 55s\tremaining: 9.35s\n",
            "977:\tlearn: 0.0369803\ttotal: 6m 55s\tremaining: 8.93s\n",
            "978:\tlearn: 0.0369803\ttotal: 6m 56s\tremaining: 8.5s\n",
            "979:\tlearn: 0.0369802\ttotal: 6m 56s\tremaining: 8.08s\n",
            "980:\tlearn: 0.0369802\ttotal: 6m 57s\tremaining: 7.65s\n",
            "981:\tlearn: 0.0369802\ttotal: 6m 57s\tremaining: 7.23s\n",
            "982:\tlearn: 0.0369801\ttotal: 6m 57s\tremaining: 6.8s\n",
            "983:\tlearn: 0.0369801\ttotal: 6m 58s\tremaining: 6.38s\n",
            "984:\tlearn: 0.0369800\ttotal: 6m 58s\tremaining: 5.95s\n",
            "985:\tlearn: 0.0369800\ttotal: 6m 59s\tremaining: 5.53s\n",
            "986:\tlearn: 0.0369799\ttotal: 6m 59s\tremaining: 5.1s\n",
            "987:\tlearn: 0.0369798\ttotal: 7m\tremaining: 4.68s\n",
            "988:\tlearn: 0.0369798\ttotal: 7m\tremaining: 4.25s\n",
            "989:\tlearn: 0.0369798\ttotal: 7m 1s\tremaining: 3.83s\n",
            "990:\tlearn: 0.0369798\ttotal: 7m 1s\tremaining: 3.4s\n",
            "991:\tlearn: 0.0369797\ttotal: 7m 1s\tremaining: 2.98s\n",
            "992:\tlearn: 0.0369796\ttotal: 7m 2s\tremaining: 2.55s\n",
            "993:\tlearn: 0.0369795\ttotal: 7m 2s\tremaining: 2.13s\n",
            "994:\tlearn: 0.0369795\ttotal: 7m 3s\tremaining: 1.7s\n",
            "995:\tlearn: 0.0369795\ttotal: 7m 3s\tremaining: 1.28s\n",
            "996:\tlearn: 0.0369794\ttotal: 7m 4s\tremaining: 851ms\n",
            "997:\tlearn: 0.0369794\ttotal: 7m 4s\tremaining: 425ms\n",
            "998:\tlearn: 0.0369793\ttotal: 7m 4s\tremaining: 0us\n",
            "Accuracy of Model: 0.986019131714496\n",
            "Precision of Model: 0.9879227051759173\n",
            "Recall of Model: 0.986019131714496\n",
            "F1-score of Model: 0.9859302124682627\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       0.86      1.00      0.93      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      0.84      0.91      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           0.99     33975\n",
            "   macro avg       0.99      0.99      0.99     33975\n",
            "weighted avg       0.99      0.99      0.99     33975\n",
            "\n",
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  cat_model_para.sav\n",
            "Accuracy Model:  0.986019131714496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WY9F5eM6WZI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtJSP8gj6WL9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpJQnSJvsw9y"
      },
      "source": [
        "cat = CatBoostClassifier()\n",
        "\n",
        "grid = {'learning_rate': [0.03, 0.1],\n",
        "        'depth': [4, 6, 10],\n",
        "        'l2_leaf_reg': [1, 3, 5, 7, 9]}\n",
        "\n",
        "cat_search = cat.randomized_search(grid, X = X_train5, y = y_train5, plot = True)\n",
        "\n",
        "#cat_search.fit(X_train5,y_train5)\n",
        "#Grid_CBC.fit(X_train5, y_train5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJeN9VEZqRCf",
        "outputId": "a801a433-bf96-4c6f-b663-d1b3661fcc9c"
      },
      "source": [
        "cat_search.values()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_values([{'depth': 10, 'l2_leaf_reg': 1, 'learning_rate': 0.1}, defaultdict(<class 'list'>, {'iterations': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999], 'test-MultiClass-mean': [1.5943054798951888, 1.2848560270789746, 1.0772228678967417, 0.9210028139009011, 0.805009803424103, 0.7074772390211188, 0.6312224693784011, 0.5662844578121643, 0.5098162909733661, 0.4583260188541929, 0.4152197639744449, 0.378013858067962, 0.344104267720965, 0.31456297483727375, 0.2893122868987754, 0.26665659656598595, 0.24571800335115213, 0.22677699429259937, 0.2098820334538676, 0.194740801278639, 0.1817022453486192, 0.16934832084701287, 0.1590559275693506, 0.14958298264498104, 0.14070493851383334, 0.13270240121093999, 0.125836587846461, 0.12048457463826649, 0.1141691531546607, 0.10881862204124527, 0.10385591629153924, 0.09895187747655078, 0.09473732043985385, 0.09072915964456951, 0.08741225927664953, 0.08417334154231265, 0.08105152730390157, 0.07882432857006184, 0.07620955910497886, 0.07377291622303438, 0.07186589620095234, 0.0699844273872861, 0.06815186023167952, 0.06646175084033025, 0.06481685755816254, 0.06334161597306848, 0.06188024540842268, 0.06065914192439184, 0.059446695295645814, 0.05848961110764515, 0.05747145970440449, 0.05644611723579603, 0.05557475527120831, 0.054749034664016066, 0.05405853938719896, 0.05327978474795662, 0.05256196896575913, 0.051935772493761016, 0.051246674079026965, 0.050671451704607844, 0.050103649028525286, 0.049540312962895273, 0.04903746137605683, 0.04855275987198212, 0.04811819650466711, 0.047703671798874796, 0.04738133518634615, 0.04706443051640511, 0.04673429909168524, 0.04641110122394728, 0.04606495750277112, 0.04573865281916036, 0.04544448587362093, 0.04519326030336599, 0.04495555888491873, 0.04471243547064128, 0.04446441781802801, 0.04425341341119755, 0.04402226637003378, 0.043791990844697294, 0.04361312911720331, 0.043463532029887904, 0.04329389474864784, 0.04313276943562566, 0.042914257487003726, 0.0427287790275948, 0.04258538056473108, 0.042437459203941236, 0.04231461331332855, 0.0421726314841435, 0.042019572000549794, 0.04189485859104342, 0.04177254333915228, 0.041664581405686386, 0.04156049067483988, 0.041439206119305814, 0.04133827374216643, 0.041242152698056596, 0.04116146121625681, 0.04108869862157736, 0.041020279888230704, 0.04094120494170718, 0.04084874602762444, 0.040762939150176515, 0.04068225713610104, 0.040613366870159116, 0.04054793702968363, 0.04047807531975724, 0.04039951769808701, 0.040332990561558923, 0.04027727845772019, 0.040225245759275455, 0.0401727328355321, 0.040094365493679694, 0.0400226590492262, 0.03995335596158836, 0.039878838549055516, 0.03981132940588051, 0.03973002065297092, 0.03968654385422571, 0.0396410619646802, 0.03959725338405202, 0.039537292645063, 0.03947901515953768, 0.03943913269530044, 0.03936900642997801, 0.039334894808913175, 0.039284716487055116, 0.039242002783505675, 0.03920354350775834, 0.03915556523222095, 0.03911413501560896, 0.039067930276958464, 0.03904266701520368, 0.03899408662308124, 0.03895044826839255, 0.03890942466643446, 0.03886875973841575, 0.03883858340747521, 0.03880873460610441, 0.03877981149965273, 0.0387319272834579, 0.03870830809722605, 0.03868188676659493, 0.03865395629405214, 0.0386278670811075, 0.03859698580591333, 0.03856889875847405, 0.038537838049159605, 0.038503575143135564, 0.0384712723529362, 0.03844784207059762, 0.03841793772777584, 0.038398017047364986, 0.038367193119112596, 0.03834632787010352, 0.038328623541408195, 0.038312594160981824, 0.03829674523398022, 0.038277464879314924, 0.038256429669083165, 0.03824069929610025, 0.03821642153193206, 0.03819214187145389, 0.03817287419500954, 0.03815515605111142, 0.03813853831462066, 0.03812839570204735, 0.03811484790671277, 0.03810071378742829, 0.03808672443604068, 0.038074384634032336, 0.03805761297373373, 0.03804237180761533, 0.03802765226090319, 0.03801232202904328, 0.03800198430056801, 0.03798742481817963, 0.03797054915657578, 0.03795540639144363, 0.03794066100453328, 0.037929967289035284, 0.03791486739868983, 0.037904741742873116, 0.03789649911341518, 0.03788812132949263, 0.037880865495191346, 0.03787222307769587, 0.037864931720495335, 0.0378546778435219, 0.03784710292912954, 0.03783868332883351, 0.037831406389965835, 0.037821699036666795, 0.03781367619406862, 0.037806488320252636, 0.03779817741850223, 0.03779000886179257, 0.03778181812428814, 0.037772350700465904, 0.03776308408730901, 0.03775189112632683, 0.0377452527840648, 0.03773844366430562, 0.03773074487912042, 0.037724488179639, 0.03771672597997048, 0.037709838320800076, 0.03770380878137217, 0.03769699579957405, 0.03768733113791222, 0.03768291885435342, 0.03767717632745398, 0.03767133303731367, 0.037666327923347315, 0.03765957477827902, 0.037652470474394795, 0.037646592142672186, 0.03764008065255337, 0.0376329326866764, 0.03762511119590851, 0.03761835257464412, 0.037612706385639426, 0.0376075716451715, 0.03760254685497998, 0.03759669578415873, 0.03759038128334577, 0.03758548810879046, 0.03758057750396293, 0.03757554108390665, 0.037569785334689666, 0.03756666561016515, 0.037561593211907524, 0.03755628779314319, 0.03755150648202776, 0.037546882905590225, 0.03754325089789204, 0.03753996941334786, 0.03753708692821751, 0.03753188228219326, 0.03752791363643591, 0.03752544364735025, 0.03752087322763453, 0.03751725970461461, 0.03751316360799259, 0.0375097650093748, 0.03750542678732518, 0.03750060325250543, 0.03749788087721458, 0.03749265377219378, 0.037489841908434185, 0.03748720235000916, 0.03748440731073651, 0.03748090337578054, 0.03747737767998329, 0.037475266167376044, 0.03747050632517895, 0.037467293076432116, 0.0374640447756084, 0.037461562898074285, 0.03745841864209348, 0.037454646478657934, 0.037451857335254794, 0.037448411480128764, 0.037445572626715155, 0.037442690404577636, 0.03743979720780561, 0.03743684803826785, 0.03743376479467061, 0.03743131131825821, 0.03742872042838518, 0.037426385765238275, 0.0374234138686925, 0.03742143305378253, 0.037419602900899955, 0.037416422248675536, 0.03741453606566066, 0.03741216331317456, 0.037409846962315634, 0.03740662118834525, 0.03740336182059648, 0.0374007271794117, 0.03739822589365605, 0.03739595446515864, 0.037394041364977613, 0.03739135600730258, 0.03738947157300838, 0.03738747458210239, 0.03738561329671767, 0.03738335134398326, 0.037381537417389275, 0.0373799581779365, 0.03737783420922854, 0.03737603324817803, 0.03737442803379068, 0.037371424022008916, 0.037368971664655214, 0.03736686938660017, 0.03736435424331364, 0.03736245574565072, 0.037360432679751365, 0.03735886714723505, 0.037358036348325185, 0.037355800821250784, 0.03735374072402794, 0.037351845549783, 0.03735018440742762, 0.037348537665361954, 0.037346064279083775, 0.037344211952201306, 0.03734186469440836, 0.037339576524806274, 0.037337785660966634, 0.03733609536279197, 0.03733455101658245, 0.0373328084021655, 0.03733151583461746, 0.037330463058499, 0.03732833557869234, 0.03732669053169223, 0.03732499747909802, 0.03732238684830814, 0.037320888599523504, 0.037319128598331135, 0.037317626565613986, 0.037316280507958814, 0.037314974322327214, 0.03731371271622686, 0.0373126373102987, 0.03731080447949569, 0.03730965875184348, 0.03730821016789616, 0.037307189335884, 0.03730548009976835, 0.03730383051328886, 0.03730268708941752, 0.03730155439086759, 0.0373001712132624, 0.037299249387146655, 0.03729746066998358, 0.037296482205782557, 0.03729496556451055, 0.03729394363820516, 0.037292804900111665, 0.03729095265007241, 0.037289875328086176, 0.03728872513648645, 0.03728708157394668, 0.03728592511076544, 0.03728416296562659, 0.037282777437824106, 0.037281782208434845, 0.037280692145165856, 0.03727950986025901, 0.03727888991136547, 0.03727783251525625, 0.037277046894048954, 0.037275597836808984, 0.03727416675459829, 0.03727297678994163, 0.037271644555868844, 0.03727079910812079, 0.03726966739315548, 0.03726806818624404, 0.037267178485635044, 0.03726618336743551, 0.03726499693083286, 0.037264272806136846, 0.037263319437854, 0.03726228322689686, 0.03726123015373303, 0.037260268461081214, 0.037259532665595556, 0.03725850443097775, 0.037257778949048505, 0.03725707062193618, 0.03725600104082347, 0.037254887537791576, 0.03725376488060703, 0.03725309142342387, 0.03725228337584293, 0.03725126592300071, 0.03725014045099074, 0.037249237999514136, 0.03724845392962312, 0.03724712608251594, 0.037246028999459384, 0.037245134421792926, 0.03724415828999625, 0.03724343902638566, 0.037242758886531946, 0.037242245402725936, 0.037241051865417645, 0.03724043009527223, 0.037239764204822355, 0.037238672431315845, 0.037237669177511735, 0.03723714721303432, 0.037236330156912405, 0.03723571643260124, 0.03723522862145651, 0.0372344717791464, 0.03723381634069219, 0.037233309966433197, 0.03723230892559971, 0.03723151982540302, 0.03723034143865886, 0.037229753102754976, 0.0372290975615247, 0.037228438897154104, 0.03722770538558253, 0.03722681035616093, 0.0372259473869974, 0.03722558924056688, 0.03722453556715573, 0.03722369443690376, 0.03722306678195856, 0.03722217235034878, 0.0372214450766312, 0.03722053230963667, 0.03721954590106755, 0.03721877195974418, 0.037218213936648585, 0.03721736704496259, 0.03721678822212298, 0.03721606612171607, 0.03721566161546062, 0.0372149409408156, 0.03721449159021608, 0.0372137134374014, 0.03721344668125028, 0.037212824388563874, 0.03721190387209539, 0.037210958764309854, 0.03721045576966354, 0.037209867479026555, 0.0372093385998093, 0.03720871155157623, 0.03720807425464067, 0.03720753645486387, 0.03720690853739228, 0.037206331605333744, 0.037205470010896864, 0.037204590206358544, 0.03720367152771524, 0.03720316725828473, 0.03720288773832941, 0.037202300875810806, 0.037201900100560424, 0.037201526515858827, 0.037201174795706886, 0.03720063917486164, 0.03719985559437253, 0.03719934707177477, 0.037198807501239634, 0.037198105410248504, 0.037197600593531936, 0.03719725845734562, 0.03719703964434971, 0.037196715660673764, 0.03719603669736529, 0.037195925930123476, 0.03719571718683835, 0.03719510105395983, 0.03719458077961813, 0.037194052850790894, 0.0371933237351614, 0.037192857702088765, 0.037192201616120295, 0.03719184920936263, 0.0371913397054108, 0.03719066372725785, 0.03719002223761341, 0.03718930427407054, 0.037188733125314614, 0.0371882815271212, 0.037187660093175694, 0.03718738710338343, 0.03718700678752082, 0.03718635984634643, 0.03718578685226731, 0.03718532111623082, 0.03718497841927353, 0.0371845484706692, 0.03718410022384932, 0.03718370919625961, 0.037183057861595536, 0.03718265969106129, 0.03718220183960521, 0.0371819160638572, 0.03718160184990756, 0.03718118838614755, 0.037180774780762704, 0.03718039783995384, 0.03717998664834828, 0.03717957366278803, 0.03717928933231919, 0.037179023781211275, 0.03717876189070279, 0.03717841749526051, 0.037178191060199074, 0.037177750751395365, 0.03717741869029185, 0.037176984070103614, 0.03717648278267891, 0.037176155081377914, 0.037175803606662235, 0.037175260908011966, 0.03717488007592932, 0.03717459806480526, 0.03717432302543102, 0.03717396941396569, 0.03717356280559651, 0.03717313692629831, 0.0371726155079269, 0.037172165201167555, 0.03717186239340219, 0.03717172281388192, 0.03717128381382305, 0.03717085872337227, 0.03717033299560008, 0.0371697945360109, 0.037169533487821205, 0.03716928593057149, 0.037168943631330886, 0.037168655137582725, 0.037168330120643495, 0.037167692423485256, 0.037167361710486735, 0.03716706557399136, 0.03716687501642374, 0.03716645601642054, 0.03716589469736839, 0.037165603218613945, 0.037164992459435366, 0.0371647327851908, 0.037164563656044924, 0.03716432435604647, 0.03716405366232958, 0.03716374979602908, 0.03716343286514095, 0.0371632703966108, 0.03716287063240232, 0.03716242363868488, 0.03716206112257304, 0.0371617887470538, 0.03716147518829995, 0.03716124993279268, 0.037161013782449015, 0.0371605095376372, 0.03716010707623847, 0.037159772435488715, 0.03715940835553915, 0.03715898862011767, 0.03715865919878044, 0.03715839929164246, 0.037157908697507645, 0.03715755542648711, 0.03715716161119504, 0.03715687677700127, 0.037156328269706995, 0.037156147279276, 0.03715558048661682, 0.037155320096586265, 0.037155146134481516, 0.03715474128230628, 0.037154409151631033, 0.037153926656105604, 0.03715367386678652, 0.03715356402570537, 0.03715314022848562, 0.03715278827436999, 0.03715241607758429, 0.03715200628250032, 0.037151707435067204, 0.03715150066443183, 0.03715117059314973, 0.03715104911413885, 0.03715074359990056, 0.03715031741433287, 0.037150069729460326, 0.03714973870132872, 0.037149457662582215, 0.037149274530865355, 0.03714901792558615, 0.0371487082888347, 0.03714848710784991, 0.037147996999026954, 0.03714788430100523, 0.03714763838178373, 0.03714730559291745, 0.03714697947313729, 0.03714658611627328, 0.03714631482576931, 0.03714607048555025, 0.03714580595909616, 0.03714556277911119, 0.037145507433347026, 0.0371452937829203, 0.03714519859444843, 0.03714500786954051, 0.03714482106358996, 0.03714452938067322, 0.03714440314315711, 0.037144096491242146, 0.037143964088519804, 0.03714377982641747, 0.037143600677912614, 0.037143114615740665, 0.037142989043349985, 0.03714279181597236, 0.03714248171055456, 0.03714224135859589, 0.03714210888621531, 0.03714177643423295, 0.03714152087140409, 0.037141186275264616, 0.03714115481506581, 0.037141027698109765, 0.03714090604901118, 0.03714066496498886, 0.03714053778306619, 0.037140234259365336, 0.03713984552535001, 0.037139578653816695, 0.037139476156994505, 0.037139221794806804, 0.0371390880954738, 0.03713880444619069, 0.03713847409901631, 0.037138192841866875, 0.03713802789350764, 0.03713782076885569, 0.037137754447938855, 0.037137455747299736, 0.03713724071941371, 0.037137026568733585, 0.03713687840735363, 0.03713650840057017, 0.0371363872692045, 0.03713622635183606, 0.037136045300365766, 0.03713567244013651, 0.037135589491547334, 0.03713549243467254, 0.037135244698435405, 0.037135136509304105, 0.03713504329217554, 0.03713480794227473, 0.03713456863102418, 0.03713435426577097, 0.03713409876733614, 0.0371339160019619, 0.0371337167319859, 0.03713348661652955, 0.03713326280558615, 0.0371330904603075, 0.03713279733622938, 0.037132625497352224, 0.037132418321389034, 0.037132240804791924, 0.03713193888308986, 0.037131703222246355, 0.037131519938836954, 0.0371315786780977, 0.03713135076473393, 0.03713105237248632, 0.037130938311146826, 0.03713076777301145, 0.037130452379838357, 0.03713009378546995, 0.03713008790920147, 0.037129956303789215, 0.03712978853092999, 0.0371295845553745, 0.0371294886018263, 0.03712912453212175, 0.0371290940457838, 0.037128875997886134, 0.03712871931822478, 0.03712835773520956, 0.03712831671332723, 0.03712815914031708, 0.037127988004208023, 0.03712765741601651, 0.0371274647080859, 0.037127193033432204, 0.03712684207776757, 0.03712651089602211, 0.037126240715923235, 0.03712593057218905, 0.03712578853680281, 0.03712563044773646, 0.037125515588908774, 0.03712535343596002, 0.03712520454908729, 0.037124934934728654, 0.03712490515780941, 0.03712480623041148, 0.037124610926995746, 0.03712438473412865, 0.03712425972719458, 0.03712414403036096, 0.0371239326566739, 0.03712379388884222, 0.037123502708206024, 0.03712319226843175, 0.03712308227566261, 0.03712293621742912, 0.037122681525452915, 0.037122530468190224, 0.037122276064644764, 0.03712196604857607, 0.03712171350544802, 0.037121606342733225, 0.03712147203385146, 0.03712138187969732, 0.037121210798038824, 0.03712090006478819, 0.03712089284128318, 0.037120763471481945, 0.03712066156941051, 0.037120518765783164, 0.03712024745019588, 0.037120015363104025, 0.03711997186879298, 0.03711964673163022, 0.0371195399568179, 0.037119426198123834, 0.03711919426119134, 0.03711900683271783, 0.037118893140544375, 0.03711887971633271, 0.03711874941803408, 0.03711869475747208, 0.0371185323576559, 0.03711840874996527, 0.03711829236852234, 0.03711815213452482, 0.03711800710259577, 0.03711767835231448, 0.0371174601902138, 0.037117242196483394, 0.03711710087023281, 0.037116889004182696, 0.03711666604226283, 0.03711648748152963, 0.03711638319739124, 0.03711627601504129, 0.03711611473847461, 0.037116123171361794, 0.037116174038368775, 0.037116026412909814, 0.03711590086699262, 0.037115724039909294, 0.03711556799971872, 0.037115464067901185, 0.03711536379500096, 0.0371151131709784, 0.037115016940017016, 0.03711477265775414, 0.03711464564880845, 0.037114500842889336, 0.03711435366076907, 0.03711417807068689, 0.037114021478475354, 0.0371137764827262, 0.0371136226228048, 0.037113542783517445, 0.03711329801051368, 0.03711313376835723, 0.03711287116365473, 0.03711265792527074, 0.03711260332837358, 0.03711259326780788, 0.03711237716562633, 0.037112401690136736, 0.03711234261414444, 0.037112207491515795, 0.03711207712680643, 0.037111955630024, 0.03711179635203765, 0.03711154330539362, 0.037111499540975534, 0.03711138395878961, 0.037111329657465726, 0.03711137735532347, 0.03711118263111984, 0.037111169846555776, 0.03711102033464933, 0.03711100391472404, 0.0371110514853615, 0.03711084209653829, 0.03711075618105957, 0.037110563255492196, 0.03711039015522257, 0.037110266994621645, 0.037110051547019844, 0.03711004773274531, 0.03710982314576256, 0.03710977016390623, 0.03710963421163979, 0.03710959406866291, 0.03710948611586754, 0.03710942812941781, 0.03710934694277366, 0.03710924253280725, 0.03710915971125223, 0.03710900354062808, 0.037108850211873864, 0.03710871896369551, 0.03710854803748197, 0.03710848897123334, 0.037108289641930456, 0.03710805260167833, 0.03710788985290217, 0.03710780819602452, 0.03710770291420499, 0.03710761832190051, 0.03710749502077987, 0.03710737942269805, 0.03710732666580876, 0.037107216026625194, 0.03710714908179019, 0.037107093785038986, 0.0371069763422092, 0.03710696962373035, 0.03710685086414247, 0.0371067091198506, 0.03710663035548823, 0.03710653775045506, 0.03710636384494582, 0.03710629141964499, 0.03710621603475577, 0.037106097524084745, 0.03710604466662158, 0.03710583306691564, 0.037105603580111494, 0.0371054961611846, 0.03710548342051129, 0.03710523242923912, 0.03710508473871025, 0.03710492030984098, 0.037104826073707804, 0.03710465467952367, 0.037104620737079326, 0.03710455041456254, 0.03710441478265502, 0.037104359746369636, 0.03710418766556222, 0.03710406028150474, 0.03710399588472634, 0.03710396717195089, 0.03710384742868814, 0.03710370418715373, 0.037103568336216135, 0.03710342130864482, 0.03710327411201964, 0.037103042194152586, 0.03710303798902426, 0.03710307880203035, 0.03710307617291391, 0.03710291632824907, 0.037102840081573905, 0.037102690090046415, 0.03710261635532425, 0.03710257405566492, 0.037102557447473816, 0.03710245577089653, 0.037102400450849025, 0.03710231264289005, 0.0371021678434329, 0.03710201858787029, 0.037101890883153334, 0.03710170334517136, 0.037101653539921446, 0.03710150264153294, 0.03710138700126412, 0.037101313264732046, 0.03710107098484945, 0.03710098499634056, 0.03710096751084155, 0.03710085324849649, 0.037100717969716646, 0.03710066159699025, 0.03710059469105223, 0.03710041366468058, 0.0371003004192289, 0.037100317364102166, 0.0371002090734718, 0.03710014067065654, 0.037100009598311785, 0.037099886978729485, 0.037099643611276566, 0.03709952438222767, 0.03709951410353526, 0.03709942497703705, 0.037099324459927176, 0.03709918916787231, 0.03709914730185793, 0.037099082992906124, 0.03709897295011507, 0.03709887720354721, 0.03709858139735477, 0.03709851720302959, 0.03709835117319095, 0.0370982197906907, 0.037098067979203224, 0.03709789969025995, 0.03709778695911569, 0.0370976605764195, 0.03709757304402326, 0.03709743602545944, 0.03709749772009115, 0.03709745811561945, 0.0370973836865462, 0.037097347494292, 0.03709729097026799, 0.03709711605358952, 0.03709706086073578, 0.03709693191060809, 0.03709671094122298, 0.037096634589318306, 0.03709661058432462, 0.03709650313780857, 0.037096499299994874, 0.037096439732117785, 0.03709634932722577, 0.03709624342109806, 0.03709611690284983, 0.037095999403517495, 0.037095907289991646, 0.037095824182961284, 0.03709570622320585, 0.03709545119378883, 0.037095348216885565, 0.03709527896996223, 0.03709505504008375, 0.0370949809415329, 0.03709495296651702, 0.037094868590358834, 0.03709484606696508, 0.03709477098041846, 0.03709472640024591, 0.03709466158394728, 0.03709465393751028, 0.03709461500071606, 0.03709456103769887, 0.037094527265098966, 0.03709439901473722, 0.03709428881033575, 0.03709425477482637, 0.03709422010107311, 0.037094176684896035, 0.037094158482636974, 0.03709409951205506, 0.03709400595669007, 0.037093956686908035, 0.037093890661403, 0.03709385875657193, 0.03709381710293017, 0.037093784954476815, 0.03709375876636467, 0.0370936943022924, 0.037093594698230536, 0.03709344460985208, 0.0370933913265033, 0.03709338959817939, 0.03709336733972387, 0.03709338959604119, 0.037093288377679556, 0.03709321485740156, 0.037093080574989966, 0.03709296424793456, 0.037092914617764074, 0.03709275576142207, 0.03709277244090926, 0.03709265222902147, 0.037092573300657926, 0.037092487772279054, 0.0370923959695043, 0.03709237129856555, 0.037092172342367995, 0.037092085496698644, 0.03709201872243612, 0.037091842413012936, 0.0370917850327406, 0.03709155742713771, 0.03709142002035374, 0.03709132135290221, 0.03709126257878583, 0.0370911788893603, 0.037091114347100586, 0.037091031944353724, 0.03709101498494229, 0.0370909314659962, 0.03709075836597831, 0.03709072632708022, 0.03709064141124984, 0.03709055668415384], 'test-MultiClass-std': [0.01637733586915443, 0.01371199659991434, 0.0064705470924947715, 0.006513511047421099, 0.006366706693806698, 0.0043876811588764355, 0.006597101359566266, 0.0076810430605797255, 0.006932831164202389, 0.005215902725611034, 0.0043561319626835605, 0.003974759587936513, 0.0036704348215869246, 0.00319382778178512, 0.00366558272325173, 0.0034731029250143586, 0.0034806379743667075, 0.0030328601232507157, 0.0029465002095028763, 0.0028107954814177646, 0.0021537883866096946, 0.002038223826644865, 0.0026255962605242662, 0.0025859130305731825, 0.0023132315704538255, 0.002282356356937359, 0.0016358715996348282, 0.0016273856218737377, 0.0019327889777417558, 0.0015001811564562928, 0.0009327285364755267, 0.0009987271022490077, 0.001203301584421454, 0.0013130837320396427, 0.0010102332072384603, 0.001007309010932891, 0.0009507565348064155, 0.0010250161463905003, 0.0011448267500639602, 0.0010374746086018961, 0.0012763347145053462, 0.0010889327337327854, 0.0010176784771866061, 0.0011303093339824451, 0.0011025572281249721, 0.0010415885416494998, 0.0011034032022313198, 0.001228585567413031, 0.0012891542236998167, 0.0012337116338477189, 0.0012786337479536933, 0.001283992539215654, 0.001341423617589743, 0.001251041584902872, 0.0012131280447527121, 0.0011280443451582244, 0.0011691661405996158, 0.001183839598492515, 0.001166271697199179, 0.0011644810215013315, 0.0012298736305664411, 0.0012836174465593495, 0.0012371824001350701, 0.001160115122182925, 0.0011876640444069757, 0.001034103550014442, 0.0010553414088185517, 0.0009652431458480783, 0.0009837536426627624, 0.0010086122284197773, 0.001013719411618368, 0.0010768818249106595, 0.001040235318499768, 0.0010079506274771521, 0.000978061283474497, 0.0009715798674228879, 0.0009392456978381936, 0.0009496134422273076, 0.0009638159051861693, 0.0009762957602677523, 0.0009646294373247033, 0.00096501419298322, 0.0009224462400505555, 0.0009280027166881824, 0.0009912909731514125, 0.0009892210843441159, 0.00103180672844761, 0.0010463073223520006, 0.0010106600051518258, 0.001031467833723226, 0.0010055597821908143, 0.0010425971395065401, 0.0010170769735962139, 0.001032377824889088, 0.0010747182695690289, 0.0010776604019715628, 0.0011014592663164807, 0.001118849180057052, 0.0011089432229512946, 0.001092284920286354, 0.0010820951028303071, 0.0010768642713210124, 0.001096998902961314, 0.0011106984367302264, 0.0011050681976424478, 0.0011166758720182698, 0.0011321339992933328, 0.001134956777393362, 0.0011562483365393547, 0.001142727324331883, 0.0011375442132192132, 0.001146002275372653, 0.0011349079618546897, 0.0011295384955432048, 0.001148328060350992, 0.0011459840548600773, 0.0011304105407799806, 0.0011328014410881611, 0.0011235720926529922, 0.0011330533653419175, 0.0011351336652077756, 0.001148293224709553, 0.0011396160346256417, 0.0011294673046047742, 0.0011377685234081258, 0.0011479233207101287, 0.001153522252878117, 0.0011477902607994529, 0.0011420970763946407, 0.0011538242949350331, 0.0011528152247745897, 0.0011658454055068726, 0.0011593316849939446, 0.0011596038856699145, 0.0011385103260344836, 0.001139916690101556, 0.0011444532005056695, 0.0011261189270386745, 0.0011189501478769707, 0.001126444234153386, 0.0011279176395607533, 0.0011132551658557064, 0.0011145425137895377, 0.0011224271457537472, 0.0011287924456894825, 0.001122523714524119, 0.0011091909098788612, 0.0011212048666986317, 0.0011230535434101055, 0.0011116100995117112, 0.0011053973976222968, 0.001095915124223763, 0.0010887588366181966, 0.001089596357457214, 0.0010982263718015614, 0.0010997814217394446, 0.0010981738412155868, 0.0010988889843993933, 0.001101606557641563, 0.0010953013959700616, 0.001102313092025041, 0.0011026203618800356, 0.00110441553291977, 0.0011118442835530995, 0.0011156036314339801, 0.001115561034306493, 0.0011231780685018297, 0.0011202646059765733, 0.0011190135457802433, 0.0011189654757383478, 0.0011196630251296576, 0.0011164553317507759, 0.0011127770068371872, 0.001107445943608231, 0.0011013666014677843, 0.001105152142672318, 0.001100911993490087, 0.0011035767686345902, 0.001110027298377597, 0.0011080279482187537, 0.001107297001957478, 0.001106278414679729, 0.0011033289038270278, 0.0010976588185164754, 0.0010959100621172656, 0.0010992713508776195, 0.0010990579736126645, 0.0011046637125013106, 0.0011040523798829722, 0.0011032563246011992, 0.001103936339304069, 0.0011047094481960653, 0.0011045047548778229, 0.001103902610237413, 0.0011061714182537333, 0.0011029056612840737, 0.0011044728698828001, 0.0011066153876320543, 0.0011082977115290452, 0.00110710721132682, 0.0011053449164367488, 0.0011083189883007234, 0.0011075974060788848, 0.0011063471935113158, 0.0011055075110027015, 0.0011076113685206189, 0.0011099780587525414, 0.001112035296191973, 0.0011137237077550731, 0.0011128825101035403, 0.0011088487350731683, 0.0011108565777612014, 0.0011128102294211354, 0.001109845885713993, 0.0011114862300123265, 0.0011140165379766682, 0.001115673664535571, 0.0011149226424125944, 0.0011155866977044238, 0.001118096588885282, 0.001119269762299705, 0.001120306449724712, 0.0011212981367334758, 0.001124186150311057, 0.0011224398537239516, 0.0011220524056475998, 0.0011201801657628537, 0.0011207955807067099, 0.001121498724772431, 0.0011238956297246316, 0.0011223185904601456, 0.0011220281777208996, 0.0011224667635802983, 0.00112370723196221, 0.0011249204536877807, 0.001126145340996776, 0.0011257573897923373, 0.0011239918673401595, 0.0011248089297789209, 0.0011240061632758179, 0.0011251154834792524, 0.0011253756942584113, 0.0011260979107997793, 0.0011250216588405335, 0.0011271775826503195, 0.0011286051118744775, 0.0011269193026816548, 0.001127512874829358, 0.0011278444896337559, 0.0011274073509844138, 0.001128715992613961, 0.001128960401266858, 0.0011289467150545176, 0.001129667739893178, 0.0011305379707572494, 0.0011312239331234691, 0.0011300696547201323, 0.001131080729309093, 0.0011320177531076317, 0.0011307885312533445, 0.0011306050216600269, 0.0011302799534296008, 0.00113025785050166, 0.0011306202316741123, 0.001131250236763791, 0.0011320870572993857, 0.0011319241171123364, 0.00113229370425785, 0.0011312884028594082, 0.0011313225482173202, 0.001131069144564907, 0.0011315113652845832, 0.0011320237815838038, 0.0011318431235784604, 0.0011308928069726286, 0.001132580697237071, 0.0011322350752753637, 0.0011324564268777781, 0.0011326704212772046, 0.0011318763572116166, 0.0011309610856258111, 0.001130495225344644, 0.0011288526546454427, 0.0011296815603160494, 0.0011289666659062003, 0.0011286671848535275, 0.0011295445417967168, 0.0011307252286018876, 0.00113133456631945, 0.0011310017860573735, 0.0011302134356511518, 0.0011305457450590476, 0.0011305371354633454, 0.0011302210182593405, 0.0011307875306550911, 0.001130187944177739, 0.0011304158495464122, 0.0011296676216181552, 0.0011293510540390822, 0.0011288669639944887, 0.0011291118421306683, 0.001127852746070435, 0.0011278435802009315, 0.0011292486799069916, 0.0011287791385288586, 0.0011292655673722316, 0.0011293291392383894, 0.001129653087862892, 0.0011299097167057936, 0.00112970409557025, 0.0011295737987506295, 0.0011291369742020044, 0.001129670820361249, 0.0011296309850919919, 0.0011287187434268983, 0.001128928933932798, 0.0011289387359664025, 0.0011290259805802896, 0.00112890536011548, 0.001128723452874496, 0.0011293216978136506, 0.0011300041731748616, 0.001130995721978983, 0.0011303529556542424, 0.0011313023797489617, 0.0011315724846086349, 0.001132070237630558, 0.0011318400680280286, 0.0011314754777681344, 0.0011308244264224547, 0.001130779269614366, 0.0011309240184170749, 0.0011306669967075496, 0.0011301106933622846, 0.0011302379403889367, 0.0011300603130529252, 0.0011300567956385936, 0.0011300640031854893, 0.0011303458047099428, 0.0011311589388937897, 0.0011313659557536453, 0.0011314318437228247, 0.0011313638544159725, 0.0011316345717287306, 0.0011312012426081368, 0.0011310274412493512, 0.0011308404627427176, 0.001131294613770055, 0.00113119252760926, 0.001131084014173022, 0.001131678067853984, 0.001132060053116235, 0.0011318857950084241, 0.0011314753932828209, 0.0011319187871175202, 0.0011320248867131677, 0.0011323102358496903, 0.0011322018038277461, 0.001132125314773183, 0.0011318076093627218, 0.0011312094865754045, 0.0011307839986765705, 0.0011307848405576538, 0.0011299531405894398, 0.0011302942257394896, 0.0011295825817935388, 0.0011303047794595184, 0.0011303739904548705, 0.0011306944750002661, 0.001131033979410604, 0.001131046728663103, 0.0011314758690067035, 0.0011312515364790045, 0.0011312826736726647, 0.0011312471023019318, 0.00113109708961948, 0.0011315660623489046, 0.0011316254667107222, 0.0011316121207011584, 0.0011316123878201754, 0.0011311059960906742, 0.0011308984351501485, 0.0011312973933157282, 0.0011310996849747761, 0.0011306682013964855, 0.0011312769964321612, 0.0011313312077761675, 0.0011316029125936513, 0.0011314318934432374, 0.001131348818635264, 0.0011310392274338566, 0.0011311401277300872, 0.0011308126869222748, 0.0011304233045148827, 0.0011309071153939325, 0.0011313821590468592, 0.0011314401911111712, 0.0011314091172954122, 0.0011318379104452052, 0.001131879598735948, 0.0011320697480147438, 0.0011321824306180113, 0.0011322932775722461, 0.0011320039344737305, 0.0011319881395439196, 0.0011318256026278343, 0.0011320019592268567, 0.001131947447440346, 0.0011319376978230513, 0.0011320057094128173, 0.0011320940365540883, 0.0011315403254217053, 0.00113156417678609, 0.0011319765780237702, 0.0011322903538568312, 0.0011322921231364177, 0.001132428308711187, 0.0011323649653550026, 0.0011319316208371318, 0.001131932456961623, 0.0011320234731012914, 0.0011317090450386124, 0.0011316335056804035, 0.0011315480717545737, 0.001131647146991234, 0.0011315492230982505, 0.001131539275871261, 0.0011317551692189708, 0.0011317793704712605, 0.0011315777106635952, 0.0011316575669953462, 0.0011315748054276811, 0.0011315367075339134, 0.0011315130046865994, 0.0011312094399432715, 0.0011310920053393706, 0.0011307627127328296, 0.0011304919385445472, 0.001130167950004803, 0.0011302195051235064, 0.001130029990308895, 0.0011302301584716868, 0.001130041739501371, 0.0011303175568216678, 0.0011298296532011075, 0.0011293444906420533, 0.0011294255796237861, 0.0011294469006821288, 0.0011292675553447498, 0.001129282344002983, 0.0011292627470616864, 0.0011290379544520038, 0.0011292129203364291, 0.0011293516878154127, 0.0011295754675307735, 0.001129898509323495, 0.0011297341131177624, 0.0011298367549468246, 0.0011299131317755735, 0.001129847328914459, 0.0011299934524149349, 0.0011300118320877069, 0.0011300722364172109, 0.0011297953538671785, 0.0011297584002775295, 0.001129593738612938, 0.0011295405787112476, 0.0011296596864088602, 0.0011297835740761613, 0.0011297026840371902, 0.0011298530945212523, 0.001130191628523091, 0.001130212100995864, 0.0011298164937341422, 0.0011295552288285013, 0.0011295758988436892, 0.0011294670335007173, 0.001129444530544178, 0.0011293829286568675, 0.0011295536463354226, 0.0011295375976135063, 0.0011294680731553707, 0.001129372130062649, 0.0011295284538094502, 0.001129365082111299, 0.0011290836829221593, 0.0011288273361231122, 0.0011289513949184055, 0.0011290053219428844, 0.0011288199700658075, 0.00112863260962494, 0.0011285005153576432, 0.0011287627245038393, 0.0011288457048494066, 0.0011289487378347278, 0.0011285535709959235, 0.001128491931865357, 0.0011283270959940963, 0.001128755503384351, 0.0011288112861276667, 0.0011288850906026886, 0.0011290594899432964, 0.0011290856121713585, 0.0011288044367615876, 0.0011285385086470066, 0.0011285212519791539, 0.0011285985258428016, 0.0011283409036344804, 0.001128483805349924, 0.0011284914535520897, 0.001128441014790548, 0.001128612588903981, 0.0011286761168247648, 0.0011285400810728584, 0.001128353238170379, 0.0011282939858557053, 0.00112841392677237, 0.00112855105732967, 0.0011285481635848118, 0.0011286646814681145, 0.001128574662937199, 0.0011282507129947194, 0.0011280781802963517, 0.0011280092932898533, 0.0011282091678979534, 0.0011279636963678493, 0.0011279103597497551, 0.0011275961865478395, 0.001127430272536356, 0.0011275309551166306, 0.0011277383954056291, 0.0011276636996338546, 0.0011276067179649674, 0.001127550543413129, 0.0011277706495990878, 0.0011278241077637473, 0.001127943344066352, 0.0011280447976638646, 0.0011283231862215955, 0.0011284153762615968, 0.0011286597393083207, 0.0011285835114207227, 0.001128515267672395, 0.001128721709046314, 0.0011291055670026376, 0.0011290661307307146, 0.0011288803414584109, 0.0011287475574731152, 0.0011286385927835644, 0.001128696350478796, 0.0011289153523022118, 0.0011289926562318144, 0.001128786674878577, 0.0011288141605618892, 0.0011289215096521542, 0.0011287306943602766, 0.0011286610165854884, 0.001128832239041686, 0.0011287418129262887, 0.0011289677146954094, 0.0011287736141046471, 0.0011288639923082789, 0.0011286471138634733, 0.0011287608328596173, 0.0011285613471431507, 0.0011284352780759775, 0.0011282859846480262, 0.0011283481771800481, 0.0011284666557710413, 0.0011283462071291497, 0.0011283835140344835, 0.001128353342701285, 0.0011282648039025436, 0.001128002562010441, 0.0011278516000704098, 0.0011279399588518476, 0.0011277738907892949, 0.0011276729773369739, 0.0011278236338797057, 0.001127920593126714, 0.0011280192877578784, 0.0011280502127702756, 0.0011279341195710184, 0.0011280260027521955, 0.0011280764765447804, 0.0011278842847001046, 0.0011277602451862138, 0.0011276935064732881, 0.0011276911133251473, 0.0011276066157691039, 0.0011273170068206188, 0.0011271821039325126, 0.0011272357191618755, 0.0011273088914287488, 0.0011271236441735325, 0.001127147596199143, 0.0011270607326821515, 0.0011270663851742347, 0.0011270946336138868, 0.0011271188024821173, 0.0011270631266681746, 0.0011268016823198122, 0.0011270958917769416, 0.0011268432791983612, 0.0011266788984173827, 0.0011265583055330537, 0.0011266144162851402, 0.0011265832839136172, 0.0011265356379876882, 0.0011264801280149819, 0.0011265729353748496, 0.001126676414348529, 0.0011265937491899702, 0.0011265959122121218, 0.0011265383475303422, 0.001126465607154292, 0.0011265435939316565, 0.0011266443824495742, 0.0011267661971444706, 0.0011266856380042118, 0.001126539667148899, 0.001126620860142364, 0.0011267695306114334, 0.001126793406102505, 0.0011266469898306012, 0.0011266953753457122, 0.0011265387859517757, 0.0011265354784016429, 0.001126568995262892, 0.0011264881008820967, 0.0011265272135545963, 0.0011265923984120896, 0.0011266560727965147, 0.001126530280849905, 0.0011265307207432478, 0.001126354379776343, 0.0011264056682267003, 0.0011263616911968246, 0.0011262610155630755, 0.0011262899253167659, 0.0011263371989633625, 0.0011263994591390874, 0.0011263999574151803, 0.0011263264314902107, 0.0011262980935478825, 0.0011263525381824478, 0.0011264930763626808, 0.0011266222594690553, 0.0011265474433536798, 0.0011265284156543659, 0.0011265143288360154, 0.0011263767828146283, 0.0011263293713345858, 0.001126324905290198, 0.001126279445133095, 0.0011261579039737227, 0.0011264211737263634, 0.0011264056400223516, 0.0011264516655756882, 0.001126455535888502, 0.0011264176935187752, 0.0011264650393869023, 0.0011263380892340426, 0.0011262411150249401, 0.0011263828289864997, 0.0011262661920491165, 0.0011261732045262404, 0.001126154117396344, 0.0011263033374600642, 0.0011260537308525208, 0.0011259988428680048, 0.001125822587277635, 0.001125781882118471, 0.0011257697584257226, 0.0011256204967524414, 0.0011257785456439415, 0.0011257080200810909, 0.0011255049893881494, 0.0011254666563610518, 0.0011254793964893932, 0.0011256281075780856, 0.001125572213032175, 0.0011256658317079524, 0.0011255660464382703, 0.0011254640523332652, 0.0011252203380750574, 0.0011253828022603398, 0.0011252451014960138, 0.0011251678334337509, 0.0011251505635916734, 0.0011249636014351336, 0.0011250936874249928, 0.0011250605573840259, 0.001125047116144158, 0.0011250734738194085, 0.0011251018757785169, 0.0011250655952254895, 0.0011250688833109525, 0.001125210459345422, 0.0011253182762892976, 0.0011252478744201244, 0.0011251978504768078, 0.00112535437939005, 0.001125568053284169, 0.0011256016034179258, 0.0011256233469172884, 0.0011255736590461282, 0.0011255598808439823, 0.0011256290359077914, 0.0011257131014520475, 0.0011255297783336568, 0.0011255444998583422, 0.001125739935879695, 0.0011258041277338235, 0.0011259517161219153, 0.001125950692709691, 0.0011259894559753941, 0.0011258695807246151, 0.0011257815323932182, 0.0011258994819642767, 0.0011258278007782135, 0.0011258374933725112, 0.0011259352809837035, 0.0011258927876350545, 0.0011258856365051009, 0.0011259584709850756, 0.0011257299343716924, 0.0011257979894308942, 0.0011258013731961264, 0.0011257368263241517, 0.0011257573514194326, 0.0011257770478622436, 0.0011257225277560763, 0.0011257192211333194, 0.001125610096418308, 0.0011256344141511738, 0.0011256323101064803, 0.0011255503976839224, 0.0011254516616315646, 0.0011253990396188555, 0.0011252764035801358, 0.001125418667235889, 0.0011255186926680176, 0.0011256427860325882, 0.0011257073708745634, 0.0011256192619189687, 0.0011256488922936102, 0.0011257431708979646, 0.0011258133925899202, 0.001125763614657204, 0.0011259271288882014, 0.0011259660289920815, 0.0011261348088062464, 0.001126096607521865, 0.0011259554286511722, 0.0011258641450833061, 0.0011258598488556727, 0.0011258620960819061, 0.0011257899360169559, 0.0011258608680507459, 0.0011257707849083361, 0.0011257416874080818, 0.001125829433823523, 0.0011258974228004514, 0.0011257880688164566, 0.001125831593259074, 0.0011259759501604965, 0.001126008919416436, 0.001125920321096771, 0.0011257761262809022, 0.001125816772455357, 0.001125730972702321, 0.0011257143007827534, 0.001125803543718799, 0.0011259075785717506, 0.0011257694391509292, 0.001125906570571896, 0.0011259488774705293, 0.0011260544007448774, 0.0011260733797682906, 0.0011261172728985467, 0.0011260488937314217, 0.0011259931574279071, 0.0011258628241257278, 0.0011257452577032065, 0.0011257802962309938, 0.0011258408039577627, 0.0011258053693911974, 0.0011258949141554708, 0.001125760810514561, 0.0011258445774836034, 0.001125784456998139, 0.0011256953988061896, 0.0011256527265332765, 0.00112566671242902, 0.0011256393275947587, 0.0011255415454799681, 0.0011255188989464449, 0.0011254596529099716, 0.0011253809350186637, 0.0011253222451141727, 0.0011252472306034795, 0.0011252750557991962, 0.0011253279071829516, 0.0011254088300501463, 0.0011256212086605286, 0.0011256180257550885, 0.0011255595416701368, 0.0011255097722977841, 0.0011255355412598926, 0.0011256016225833529, 0.0011256275249659949, 0.0011258241818767204, 0.0011256906986694673, 0.0011257572182910072, 0.0011256525012568598, 0.001125661996242318, 0.0011256036702389405, 0.0011256946273503422, 0.0011256711263123963, 0.001125726693078351, 0.0011257615480319413, 0.0011255968779763322, 0.0011255946612700528, 0.0011256220752167239, 0.0011256065155928097, 0.0011256305007442435, 0.0011256229138968696, 0.0011256717499434062, 0.001125648329855879, 0.0011256119962684155, 0.001125517004573891, 0.00112549139782547, 0.0011254338171388766, 0.001125428390165692, 0.001125473282013777, 0.0011253157363529645, 0.0011251800892076878, 0.0011252364060232096, 0.001125362342795757, 0.001125409728398087, 0.0011254489728443882, 0.00112532827991037, 0.0011253872284879734, 0.001125411289187987, 0.0011254049744099598, 0.0011253973860426358, 0.0011253289815327874, 0.0011254023896101386, 0.001125371299967943, 0.0011253774358921324, 0.001125272970806201, 0.0011252345723059296, 0.0011250405272635856, 0.0011250908305137286, 0.0011249798312242135, 0.0011250396898311278, 0.0011249229633270704, 0.0011249292813543246, 0.0011248593736692255, 0.0011250111037175111, 0.0011250916600834618, 0.0011250929191650785, 0.0011251830644346777, 0.0011250577276540484, 0.001125068008823367, 0.0011251558743923394, 0.001125192113905957, 0.0011252257173784894, 0.0011252580477988565, 0.0011253258646661292, 0.001125429377167395, 0.0011253521462133143, 0.0011253186349139358, 0.0011253977504142993, 0.0011253930970080248, 0.0011254282264167017, 0.0011253861621042004, 0.0011252839817779406, 0.0011253768576372065, 0.0011254772681486235, 0.0011253863601222824, 0.0011254086413793906, 0.0011252698047391472, 0.0011251509761918408, 0.001125106448706241, 0.0011251596573190966, 0.0011251197831852577, 0.0011251156559947603, 0.0011252181021139526, 0.0011252314326653395, 0.001125179342653622, 0.0011252340382295338, 0.0011252418488875845, 0.0011252773896364711, 0.0011252539075626519, 0.0011252295259073126, 0.0011252194616496497, 0.0011251567962899973, 0.0011251145760068158, 0.0011250775648967437, 0.0011250294730397376, 0.001124981470474701, 0.0011248883134204924, 0.001124782230078711, 0.001124757074661613, 0.0011248075676313554, 0.001124752283689631, 0.0011247721244319023, 0.0011247502685286042, 0.0011247367290529058, 0.0011247211628843898, 0.001124733510410294, 0.0011246536289019145, 0.0011245830716285956, 0.0011245401091089149, 0.001124593906056808, 0.0011246582491798377, 0.0011246107232543547, 0.0011246351405521074, 0.0011246530788402167, 0.001124730111355845, 0.0011247487727915847, 0.0011248114076391554, 0.0011248325047501658, 0.0011248872547350304, 0.0011249655877694565, 0.001124984391981325, 0.0011249943271231769, 0.0011249676646676564, 0.0011249330013978853, 0.0011248227123113646, 0.001124784053172662, 0.0011247095864314297, 0.0011247014008444053, 0.001124693210040151, 0.0011246591762311595, 0.0011246517796039248, 0.0011245911240127797, 0.001124619121976091, 0.001124582412036834, 0.00112457012081084, 0.0011245788007421941, 0.0011245462528009372, 0.0011245968016172392, 0.0011246542616686799, 0.0011246146452064578, 0.0011246727736844197, 0.0011247184025497082, 0.0011246503720595985, 0.0011245766557095788, 0.001124665183750032, 0.0011245920065523147, 0.0011246808553560454, 0.0011245182248610958, 0.0011245407343548756, 0.001124551824451361, 0.0011245608265246347, 0.0011245295432897756, 0.0011244755541597934, 0.0011245167964913764, 0.0011246057511443467, 0.0011244744669336906, 0.0011244778816200272, 0.0011244663384117782, 0.0011243448201534818, 0.0011244648843995258, 0.0011243933803043859, 0.0011244000152503086, 0.0011242959129664205, 0.001124219031883983, 0.001124302642253396, 0.0011242301054328408, 0.0011242096032345434, 0.0011242407488107474, 0.001124360976816267, 0.001124274253363615, 0.0011243070361839408, 0.001124283895235964, 0.0011242880445778405, 0.00112430475207275, 0.001124219067765015, 0.0011240866355071138, 0.001124042983085363, 0.0011240630589052167, 0.0011241515150811937, 0.0011241464403432926, 0.001124173838295438, 0.0011241250741967168, 0.0011240001436243745, 0.0011239311039483537, 0.0011238989178610466, 0.001123903344360011, 0.001123871389192969, 0.0011237900838911353, 0.0011238020261463094, 0.0011237528921087762, 0.0011237576145444845, 0.0011237986344502227, 0.0011237300436486922], 'train-MultiClass-mean': [1.5941220263543052, 1.2845804545467372, 1.0769267688869204, 0.9206889909125561, 0.8046709957756369, 0.7071109996492494, 0.6308373834408622, 0.5659168435416908, 0.5095064333578115, 0.4580143566140196, 0.4148865832809414, 0.3777058966205484, 0.3438149122326648, 0.3142700568623505, 0.2889927674593388, 0.26633446763185725, 0.2454154808641388, 0.22650350771505498, 0.2095696750548243, 0.19439320606026675, 0.18134961972192806, 0.1690403579506324, 0.158751705348451, 0.14928848098583328, 0.14042234446067645, 0.13241434266662788, 0.12555500737092096, 0.12019706017300918, 0.11390593143931393, 0.10856364143215237, 0.10357806611098454, 0.0986495471733687, 0.0944429015004195, 0.09041785001181839, 0.08708034960477902, 0.08385006990726644, 0.08074030155402946, 0.0785263604013081, 0.07590217663803844, 0.07347180616209131, 0.07156696373836273, 0.06969002286141023, 0.06784071384449374, 0.06613835217765185, 0.0644977234865512, 0.0630443501961238, 0.06158028349890545, 0.06034580654377999, 0.05913402162183664, 0.058173209925245835, 0.057157246458973875, 0.05612889841845508, 0.05525996943671558, 0.05443784550723507, 0.053748738650670626, 0.05297644304461852, 0.05226331989197544, 0.05163142858599884, 0.05094309291817734, 0.05035796699786823, 0.04979012263223025, 0.04922923199234442, 0.048728429774823695, 0.048243011501186976, 0.04780371727371232, 0.04740650426889695, 0.04708050115890532, 0.0467606273590078, 0.04643019923338673, 0.04610561725255483, 0.04576849024842738, 0.0454472844137258, 0.04515345734935163, 0.04490237350577051, 0.04466710401157243, 0.044422291314089764, 0.04415272555259925, 0.0439421991338648, 0.043706934420271966, 0.04347592364120741, 0.04329816230997894, 0.04314653249529695, 0.04297259298331937, 0.042814663352930875, 0.04260859038390019, 0.04241013795526306, 0.042266433662992424, 0.04210233394081412, 0.04198065490240999, 0.04184493693098266, 0.04168875305458839, 0.04156429331261383, 0.04144082048488056, 0.041326660490592165, 0.041222468853016475, 0.04110657579259244, 0.04100990714412319, 0.04091400583103287, 0.040832367478627805, 0.0407556784661298, 0.04068685156286874, 0.040612330974592624, 0.04052571023550453, 0.040443486522271345, 0.040364420261903323, 0.0402991715015123, 0.04023738623402242, 0.04017192234892095, 0.04009532678899382, 0.0400320946743273, 0.03997590800627617, 0.03992371092624844, 0.03987292267993365, 0.03979677881128615, 0.03972591049532042, 0.03964525244103132, 0.03958350624312649, 0.03951816728748229, 0.03944842352930735, 0.03940832224207397, 0.03936277955394334, 0.03932022548461713, 0.039253964400313424, 0.039196173186759005, 0.039159471978884, 0.03910132728373142, 0.03906641284395216, 0.03901831805111942, 0.0389701137764149, 0.038931424342616656, 0.03888641801887824, 0.03884528897307271, 0.03879675507582084, 0.038771834733258966, 0.038728565910794925, 0.03868564050836917, 0.03864596641125493, 0.03860839389129553, 0.03857837997584252, 0.03854640211925332, 0.03852018778001364, 0.038477776120644215, 0.03845400914816239, 0.038429439734785624, 0.03839992742459409, 0.03837316810054681, 0.038346413660432434, 0.03831949117519961, 0.03828839593381226, 0.03825375384620972, 0.03822538779403667, 0.03820283541014127, 0.03817299492436321, 0.03815201181320368, 0.03812324278876247, 0.03810447837321889, 0.0380851415772421, 0.03807011767369928, 0.038055388255760325, 0.03803792896113211, 0.03801806355458375, 0.038002637435433965, 0.03798107081610076, 0.037958410682263494, 0.03793935138382756, 0.037922277948570185, 0.03790675091544617, 0.037896766464579436, 0.03788297603098132, 0.0378693236959611, 0.03785582547487031, 0.03784419469287429, 0.03782667588776321, 0.03781247771121417, 0.037800218195860096, 0.037784567296469236, 0.03777416999190373, 0.037761272745503664, 0.037746281616635353, 0.037733173919551194, 0.03772097145702699, 0.03771048825429379, 0.03769750170559477, 0.03768859803269168, 0.03768058989314946, 0.03767263374480784, 0.037666579181157955, 0.03765856558156961, 0.03765183237897839, 0.03764263058426781, 0.037635536009768024, 0.03762827878532759, 0.037620980213056814, 0.03761262533928587, 0.03760493552190373, 0.03759880930856547, 0.03759050881470794, 0.037583255037798635, 0.037576091854039345, 0.037566462453118676, 0.03755833138504259, 0.03754850211121292, 0.03754282461609577, 0.03753645480061794, 0.037528716732288935, 0.03752292406457486, 0.03751636711480532, 0.03750950866615945, 0.03750374802425904, 0.03749747617810916, 0.037488891458735844, 0.03748418689756847, 0.037478772396516584, 0.03747290269735038, 0.037467951197858416, 0.037462227842725, 0.03745553277181978, 0.037449395691895315, 0.03744344837303287, 0.03743779828518968, 0.03743118423338184, 0.03742458360192955, 0.03742042186478602, 0.03741546615329707, 0.037411308649071835, 0.0374062985245922, 0.0374001597583247, 0.03739582119538704, 0.037391182158392704, 0.03738665792454491, 0.03738189243039636, 0.037378881161285106, 0.037374256399218306, 0.037369516535821275, 0.03736528018223032, 0.037360294346416946, 0.03735672935040321, 0.03735416937176708, 0.037351041957982174, 0.03734608934663707, 0.037342912206938694, 0.037340668430207564, 0.03733688423775015, 0.03733371572935146, 0.037330175970150196, 0.03732698771575691, 0.03732318178078294, 0.03731900043891986, 0.03731608558371403, 0.03731132187357547, 0.03730833727992674, 0.03730576308135764, 0.037302970146837255, 0.037300450361136585, 0.03729704203752113, 0.0372943752001945, 0.03729049660515372, 0.037288105445659775, 0.037284772008357135, 0.037282697237336425, 0.03727985264220223, 0.037276804290228734, 0.0372742243727805, 0.03727089712141013, 0.03726824204896637, 0.03726584815427934, 0.03726355096413389, 0.03726126950793771, 0.037258407750298955, 0.037256512519504854, 0.03725433438708003, 0.03725218784359105, 0.03724978108826088, 0.037248206161711366, 0.03724675172597225, 0.037243974770303, 0.0372420909532847, 0.03723983777778298, 0.037237773978793604, 0.03723549306658367, 0.037232908488960076, 0.03723082006257853, 0.03722864842246118, 0.037226498377532315, 0.03722473999194125, 0.03722243605424383, 0.037221182666420126, 0.03721953628688921, 0.03721799355047784, 0.037216135009989125, 0.03721454976835331, 0.03721309864158126, 0.03721162828241551, 0.037210196384286236, 0.037208870599396475, 0.03720599771836639, 0.037203961864568945, 0.037201740399857915, 0.03719977680198422, 0.03719802046330436, 0.03719611656723717, 0.03719480609333516, 0.037193697392260146, 0.03719169062409549, 0.03719015183035765, 0.03718851996386891, 0.037186995423348225, 0.037185579281208435, 0.03718395402968056, 0.03718249447120861, 0.037180852360526005, 0.037179362821277845, 0.03717789035572149, 0.037176600014763694, 0.03717527429217179, 0.03717429832940763, 0.037173282905611886, 0.037172278935263516, 0.037170587911048575, 0.03716899311189947, 0.03716762846712963, 0.037165695434095765, 0.03716438019503362, 0.037162894326832153, 0.037161613358206926, 0.03716033241361911, 0.03715919560491981, 0.03715831193366028, 0.03715752777258174, 0.03715639082659352, 0.03715549321845483, 0.03715420104283595, 0.037153385733756504, 0.03715196703442817, 0.037150823473341706, 0.03714997295814257, 0.03714892032850105, 0.03714780815418501, 0.03714683574354532, 0.03714509662029876, 0.03714426483945066, 0.037143014019447855, 0.03714229001561939, 0.037141467525727924, 0.03714047848482185, 0.03713959589066002, 0.03713888951387564, 0.03713744467111136, 0.03713640649606, 0.037134940713976196, 0.03713366222632119, 0.037132909773201515, 0.03713217175507095, 0.0371312683123415, 0.03713053967648492, 0.037129736970264, 0.03712893054620245, 0.03712784961598881, 0.037126740130700144, 0.03712586698862765, 0.03712466780523188, 0.03712387031266969, 0.03712306641452736, 0.037121884565345625, 0.03712111310877534, 0.03712008605058386, 0.03711875933895439, 0.03711813612861584, 0.03711738829715581, 0.03711627020220897, 0.0371152682482454, 0.03711446089009113, 0.03711388652158922, 0.03711295308400636, 0.0371124068969495, 0.037111901618993366, 0.03711096439952293, 0.037110338322397914, 0.0371094208455895, 0.037108707958677704, 0.037107913024905126, 0.037107194615429814, 0.037106344704072945, 0.03710569211084224, 0.03710484917327333, 0.03710373415198406, 0.037102844670608856, 0.037102122740126586, 0.037101446115402836, 0.03710103239690441, 0.03710056488577942, 0.03710008519979824, 0.037099121915926646, 0.03709864241340941, 0.03709799896298386, 0.03709704394948401, 0.03709630531938807, 0.03709575392302984, 0.0370950676856932, 0.037094590648260505, 0.03709407082932707, 0.037093494636625736, 0.037093083608168785, 0.03709278763071875, 0.037092076999169894, 0.037091476028812576, 0.03709076249720653, 0.037090293676170207, 0.03708969559691557, 0.03708917735610698, 0.03708868885398873, 0.037088129943654576, 0.037087478674539126, 0.03708695398550941, 0.03708619716018788, 0.037085751487668656, 0.037085197967605704, 0.03708443254556735, 0.037083753235560124, 0.03708306670086072, 0.03708233617342573, 0.03708178121276604, 0.03708127772276306, 0.03708057268358598, 0.03708013490673664, 0.037079607309153405, 0.03707915019919766, 0.037078678923899704, 0.03707814872813158, 0.037077505929182374, 0.03707705441885873, 0.037076624519107974, 0.03707604004060572, 0.037075388266958116, 0.03707507854563064, 0.037074656386325865, 0.037074163134195086, 0.03707376412511195, 0.03707328362940346, 0.037072928492179344, 0.03707254314511075, 0.037072159255414405, 0.037071713614313904, 0.037071015650222444, 0.03707046011551277, 0.03706989709291203, 0.03706952411514692, 0.03706893038626014, 0.03706867940576452, 0.03706828536146073, 0.03706790706250427, 0.037067588985669565, 0.037067170652836705, 0.03706675109196131, 0.037066185629848286, 0.037065583955333804, 0.03706527302861137, 0.03706492166441493, 0.03706453738716797, 0.03706412893938373, 0.037063795695910035, 0.03706353582418386, 0.03706315937717853, 0.03706288609522095, 0.0370624652794375, 0.03706211113820155, 0.037061653886695474, 0.03706131000600043, 0.037060763060133985, 0.03706046477910568, 0.037060063569098294, 0.037059630806068576, 0.03705929014364512, 0.03705897251380983, 0.03705874938600289, 0.03705835499183622, 0.0370579507469809, 0.0370576350196559, 0.03705726299914999, 0.037056739522956524, 0.03705638846767395, 0.03705609273797158, 0.03705592573260228, 0.037055549297606155, 0.03705514516200643, 0.03705484071459608, 0.03705440272575777, 0.03705417798528091, 0.03705381836926976, 0.03705345752880546, 0.0370531784363818, 0.037052895597946646, 0.037052481578102675, 0.037052125005808734, 0.03705172227849481, 0.037051174822354434, 0.03705083934232966, 0.03705053312218377, 0.037050177415138115, 0.03704992086731474, 0.037049684203216214, 0.037049333181690076, 0.03704898264805399, 0.03704876791256262, 0.03704844284706406, 0.037048159862785555, 0.037047871214602165, 0.03704757845780046, 0.03704731975182771, 0.037047041252734746, 0.03704675635014543, 0.037046498886834874, 0.03704624426936653, 0.037045944422357785, 0.037045635636172906, 0.037045364803606456, 0.037045054479332616, 0.03704480096598183, 0.03704447777014225, 0.03704411522299853, 0.03704379043007416, 0.03704351849531325, 0.03704332973021343, 0.037043159299573004, 0.03704291882219503, 0.0370427005504033, 0.03704237923068301, 0.03704205638123582, 0.03704180458840304, 0.037041591732478056, 0.0370413684816464, 0.03704123187454159, 0.037040995648463734, 0.03704076387645332, 0.037040386645102605, 0.03704018929061755, 0.03703988220248447, 0.03703971236853862, 0.03703952596911828, 0.03703931523382142, 0.03703903028212418, 0.037038900067848136, 0.0370386466772884, 0.037038421173289045, 0.03703807595227209, 0.03703790853865, 0.03703766871081545, 0.037037449165842416, 0.03703725402409705, 0.03703698338312431, 0.03703663426317072, 0.0370364111545125, 0.037036198469570854, 0.03703598425872305, 0.037035694970856264, 0.03703549611422428, 0.03703527624097652, 0.037035002717807, 0.03703469724484648, 0.03703449811111061, 0.0370343448076288, 0.03703411237476195, 0.03703382708974703, 0.037033584058273845, 0.037033332287599335, 0.037033125097068825, 0.037032957013921695, 0.037032681705709565, 0.03703246808316889, 0.03703229547757481, 0.03703203160190762, 0.037031819844575244, 0.03703167196256487, 0.037031402098373814, 0.037031195344638355, 0.037030927698002425, 0.03703068746831281, 0.03703046533433988, 0.03703023111436196, 0.037029950951176886, 0.03702976176532765, 0.03702956657227121, 0.03702930713669365, 0.037029083068287776, 0.0370288436266715, 0.037028613887515195, 0.037028410218363284, 0.03702810628248656, 0.03702792229216486, 0.03702771049368545, 0.03702751237237261, 0.03702731587104204, 0.037027112047313265, 0.03702690928715036, 0.03702675314014054, 0.03702651878040389, 0.037026320949700105, 0.037026107728954226, 0.037025937636069574, 0.0370257827208829, 0.037025662693981366, 0.03702549207549677, 0.037025260206810934, 0.03702507126383805, 0.037024956843645505, 0.03702482164195242, 0.037024653852246917, 0.03702452727923427, 0.03702428756425167, 0.03702418059862996, 0.03702400528250191, 0.03702382713933024, 0.03702367587386935, 0.03702350738162509, 0.03702330872990817, 0.037023167175735644, 0.037022918487910665, 0.03702274070187216, 0.03702262221764932, 0.037022464581699256, 0.03702221655070056, 0.03702212797434334, 0.03702193004367982, 0.03702177895919525, 0.03702161162924449, 0.03702149449588696, 0.03702131809808934, 0.03702114791865611, 0.03702096586136266, 0.037020804051207175, 0.03702066753430442, 0.037020457985949835, 0.03702025470977475, 0.03702008001280302, 0.0370199310117574, 0.03701978482305199, 0.037019603736361935, 0.037019486843895534, 0.0370193088724199, 0.0370191945533505, 0.03701904691259239, 0.03701892849743351, 0.0370187146725322, 0.037018616322951226, 0.037018455846970694, 0.037018239292104155, 0.03701806449628108, 0.03701795559818617, 0.03701781687386275, 0.03701768761486287, 0.03701758916499016, 0.03701743370176496, 0.037017287357173335, 0.037017065482798935, 0.037016933294053526, 0.037016772314061384, 0.037016615874204704, 0.03701646018391435, 0.03701632567398286, 0.03701617319774775, 0.03701598521328861, 0.03701585154760599, 0.037015734801969416, 0.03701557274250232, 0.037015419551301644, 0.03701527213180838, 0.03701513076891976, 0.03701500207359661, 0.03701486324399838, 0.0370146755746349, 0.03701454229353826, 0.03701443744843886, 0.037014372257760185, 0.037014283933856806, 0.03701414629598728, 0.03701403949476004, 0.03701387558634098, 0.03701378677713871, 0.03701366759416389, 0.037013587903564876, 0.03701339205780877, 0.03701324412206081, 0.037013122986316316, 0.03701292672285788, 0.03701272003154309, 0.0370125872542054, 0.037012449472754026, 0.037012319577073995, 0.03701215215450966, 0.03701199930036804, 0.03701182112294888, 0.03701165106705006, 0.03701152583259792, 0.0370114419668617, 0.037011322552397634, 0.0370112164836349, 0.03701104091949967, 0.03701093800126896, 0.03701084567716902, 0.037010737012171634, 0.03701063719491615, 0.037010512301885796, 0.03701038795228589, 0.037010219524582604, 0.03701011321478389, 0.03701004190353812, 0.037009892241378316, 0.03700980279007644, 0.037009695137783856, 0.037009496010471196, 0.03700941187064728, 0.0370092871208224, 0.03700914609162844, 0.03700902190044478, 0.03700885065890458, 0.03700869793824474, 0.03700859863335463, 0.037008483071265655, 0.03700834323082852, 0.037008215211714164, 0.037008097497685875, 0.037007995713017265, 0.037007897234886226, 0.037007752745181206, 0.037007628890539095, 0.037007534382135325, 0.03700742196321973, 0.03700733331673266, 0.03700720962239649, 0.03700710801976684, 0.037007019566546694, 0.03700692426059422, 0.037006813112490336, 0.037006737732547555, 0.03700662843567356, 0.03700649870123584, 0.03700638950138913, 0.03700623955323237, 0.03700609242183103, 0.03700603793954919, 0.037005895895956126, 0.037005780297085025, 0.037005660622290694, 0.03700557182234936, 0.037005428184386895, 0.03700532738064382, 0.03700515600704422, 0.037005052742613655, 0.037004965580838135, 0.037004872410449176, 0.03700476176981043, 0.03700465891502527, 0.03700451166569416, 0.037004419175779416, 0.037004334857115066, 0.03700424190211562, 0.03700417908065224, 0.03700403809228079, 0.037003938381054, 0.03700385356112277, 0.03700375876694876, 0.0370036571087515, 0.0370035566323182, 0.03700344074159826, 0.03700336029183226, 0.03700326715179719, 0.03700316821645607, 0.03700305727984728, 0.037002991396520045, 0.03700291308102529, 0.03700280233983124, 0.03700271227413129, 0.03700262937324247, 0.03700250361358673, 0.0370024177999514, 0.03700234288774127, 0.037002255895605136, 0.03700216318163218, 0.03700202295770781, 0.03700192536315556, 0.03700181966057725, 0.03700175382152258, 0.03700167702100226, 0.03700162037681737, 0.037001525418244266, 0.03700143362201146, 0.03700136323581227, 0.03700127903484113, 0.037001189542429695, 0.03700106538563137, 0.03700099126973586, 0.03700091219564313, 0.03700084035943423, 0.0370007703848618, 0.037000688835960645, 0.03700059180127634, 0.037000530079594675, 0.03700043141313905, 0.03700032261457047, 0.03700023608429722, 0.037000177634614856, 0.037000067098368415, 0.03699998491946039, 0.03699989807830568, 0.03699976794936969, 0.036999669384465105, 0.036999627792505714, 0.036999535238918274, 0.03699946453263012, 0.03699938896699961, 0.036999290696745346, 0.03699922022763913, 0.03699914714856772, 0.03699906998401682, 0.03699899028066162, 0.03699894136289442, 0.03699886185133957, 0.036998773812410984, 0.03699871325525563, 0.03699863929991035, 0.03699856535681751, 0.03699845261004617, 0.03699839951385179, 0.03699832564466948, 0.036998287207497794, 0.03699821341706436, 0.03699814963760307, 0.03699809216778508, 0.03699802771311913, 0.036997945149005236, 0.03699786917405861, 0.03699777320158951, 0.036997728147152735, 0.03699765221745389, 0.036997588849329265, 0.03699751830752104, 0.03699747889826601, 0.03699739030031655, 0.036997311003471346, 0.03699720742681838, 0.03699709843156106, 0.03699703100677767, 0.03699697245630058, 0.036996883444990795, 0.03699680284213241, 0.03699674440861431, 0.03699668050542488, 0.03699661437241106, 0.03699654381349349, 0.03699646310251343, 0.0369963949359335, 0.03699634663796319, 0.036996252670065295, 0.036996167831927544, 0.03699609924872833, 0.03699602795663599, 0.03699595307160406, 0.0369958934173985, 0.03699580632139583, 0.03699570606752725, 0.03699561710420244, 0.03699556260929137, 0.0369954892601497, 0.03699542015938457, 0.036995343913457886, 0.03699529095465326, 0.03699520047916072, 0.03699515641763815, 0.03699508750626362, 0.036995004222363875, 0.03699492390909904, 0.03699484301762256, 0.036994774613372854, 0.03699472383970565, 0.03699466953140338, 0.036994614445691985, 0.03699453802171889, 0.03699447002785398, 0.03699439528462315, 0.03699432983600884, 0.036994274027901536, 0.03699420206709772, 0.036994121704109095, 0.036994049718425764, 0.03699399353278393, 0.03699392899152285, 0.036993813400540615, 0.036993752917733554, 0.03699369587889308, 0.036993620908680774, 0.036993569212322046, 0.036993515358302406, 0.03699344912139351, 0.036993398612036966, 0.036993348580204574, 0.03699329975703989, 0.03699322294426279, 0.036993159790157853, 0.036993107737400804, 0.03699305387016519, 0.03699298973934311, 0.036992929669291415, 0.036992854517957606, 0.036992749540965826, 0.03699269564811885, 0.03699261784372044, 0.03699254310495793, 0.03699249123241721, 0.0369924075950313, 0.03699236058331473, 0.03699227398726535, 0.03699222372197295, 0.03699215701412268, 0.03699210656516635, 0.03699206784328581, 0.036992001254373776, 0.03699195608151431, 0.03699187254004955, 0.03699181766173346, 0.036991755562305294, 0.03699169320037043, 0.03699163538077576, 0.03699157464504946, 0.036991539360385746, 0.03699147422992657, 0.03699143222514504, 0.03699137583552089, 0.036991331028821635, 0.03699128276852138, 0.03699121792449819, 0.036991116584163826, 0.03699108156365586, 0.036991024564300944, 0.03699099004254657, 0.0369909237499924, 0.0369908829798482, 0.03699082750292885, 0.03699076393798767, 0.03699069404988165, 0.03699064308531754, 0.03699061527664741, 0.03699053584418194, 0.036990500685311865, 0.03699044898594572, 0.036990392622561735, 0.03699035563231079, 0.03699031071419783, 0.03699026649619316, 0.036990236733466866, 0.036990180861696864, 0.036990112573035715, 0.03699005456793967, 0.036990015782178025, 0.03698995751752541, 0.0369898962383658, 0.03698985538453922, 0.036989800146968795, 0.03698974906116811, 0.03698969857064319, 0.03698964078039425, 0.036989593253041596, 0.036989550420954156, 0.036989500080169095, 0.036989452846707495, 0.036989385088059254, 0.036989330965834614, 0.03698929107793084, 0.036989239275453334, 0.03698920454291448, 0.03698915055400797, 0.036989106579496805, 0.036989036458189926, 0.03698899156593538, 0.03698894210299738, 0.036988882648737495, 0.03698882868314094, 0.03698878907907791, 0.036988727342053565, 0.03698867967218966, 0.03698864429242054, 0.03698857745013883, 0.036988512923994556, 0.03698845087495488, 0.036988396748302725, 0.03698835733159969, 0.03698828705831813, 0.036988234411122894, 0.03698817661566367, 0.03698812913002073, 0.03698807150806891, 0.03698801316514445, 0.036987974867789354, 0.03698792430631129, 0.03698787014828984, 0.03698781481291843, 0.03698776811799912, 0.0369876870345016, 0.036987647316590024, 0.03698760529758651, 0.036987579362779494], 'train-MultiClass-std': [0.016192396654622875, 0.012652703331956143, 0.007015971694023256, 0.007278681393522926, 0.006702613564013476, 0.004957082669252437, 0.004872526217861369, 0.006195370601332945, 0.004997146184749476, 0.003229385166307189, 0.0025418439844869588, 0.00230638131441985, 0.002043414667615694, 0.001654748586640241, 0.0019035323736248835, 0.0016965719262425707, 0.0017548137116252854, 0.0013716520475491349, 0.0012141081864296768, 0.0010307040799224874, 0.00047888595074043063, 0.0005624635036357944, 0.0011497705745938367, 0.0016255747189513454, 0.0013288357023345278, 0.0015960776324283584, 0.0012455216459529433, 0.000872976087313742, 0.0008567805335535118, 0.00044818062012223173, 0.000941916183769723, 0.0008852166929706193, 0.0006611044258348053, 0.0006247043898553012, 0.0008110185770522817, 0.0009097530005623956, 0.0009760415813678643, 0.0010319064420723464, 0.0010836921087327527, 0.001083255169508231, 0.0011798053732255144, 0.0015056884759772223, 0.0015506485003864633, 0.0014353651992291827, 0.0011744549177358046, 0.0011466258172679468, 0.0012564012547244573, 0.0011977689135826211, 0.0011999732604283729, 0.0011544837291728712, 0.0011598171832209157, 0.0011185891193003091, 0.0012234746765051544, 0.0011337118016852914, 0.001104360013084938, 0.0010272649661558786, 0.0009692077014776524, 0.0010292797942092846, 0.0009536331795900588, 0.000940401994197931, 0.00091055324105956, 0.0008739955940984211, 0.0008599470039861981, 0.0008830400678076411, 0.0008524689402807678, 0.0008908752416796705, 0.000883350260681639, 0.0008825781409176018, 0.0008535322489717129, 0.0008379030687980404, 0.0008146574497933206, 0.0007354727757085182, 0.0007865192594569191, 0.0007861770730560144, 0.000788656442792259, 0.0007758833250809425, 0.0008452430266760078, 0.0008123407801208821, 0.0007782803871564801, 0.0007775776894931147, 0.0007884378397996628, 0.000775554899796682, 0.0008201853981681726, 0.000813516994424776, 0.0007754062361196909, 0.0007957389478448006, 0.0007535303008294102, 0.0007211086777871953, 0.0007560625090081159, 0.0007477797000413284, 0.0007695771066181185, 0.0007284472872009256, 0.0007588438224684145, 0.0007385857700274672, 0.0006912306543676937, 0.0006959057393257917, 0.0006873638438588493, 0.0006709825328231156, 0.0006783746889991759, 0.0007066154303964839, 0.0007077973898765695, 0.0007142769602497672, 0.0006957547980033453, 0.0006898342768797222, 0.000691720646699729, 0.0006797944652645696, 0.0006630081374670093, 0.000665848035725449, 0.0006493852374456453, 0.0006515103316511804, 0.0006520869744759067, 0.000642034140788022, 0.0006515891852313123, 0.0006627427697999009, 0.0006450816058668425, 0.0006384925827082722, 0.0006416232765695356, 0.0006401573143139559, 0.0006429043977427114, 0.0006392046343550445, 0.0006390587385777822, 0.0006238738686088743, 0.000622464359252347, 0.0006349508156825945, 0.0006328749908390714, 0.0006133579367613698, 0.0006091037526614369, 0.0006117408418020397, 0.0006130427412689349, 0.0005951566673741834, 0.0005964936977301458, 0.000582749696800697, 0.0005845603678079866, 0.0005831493505315956, 0.0005965686590021161, 0.0005953921338591578, 0.000589095693972636, 0.0006075615362321436, 0.0006178619470255192, 0.0006070536891306028, 0.0006067221152800001, 0.0006184935046294188, 0.0006175480550187626, 0.0006104090403292581, 0.0006022175448834879, 0.00061273641445186, 0.0006180240747853328, 0.0006076504058045668, 0.0006025105594804865, 0.0006131021803253501, 0.0006187432924978139, 0.0006272547819923199, 0.0006293828917090099, 0.0006262847927634074, 0.0006189040973123522, 0.0006171742128839904, 0.0006192519240823599, 0.0006178652480150714, 0.0006149339488708045, 0.0006192366162139472, 0.0006129692860476923, 0.0006129310314173644, 0.0006121740317246089, 0.0006035032759795303, 0.0006011749469109927, 0.0006017847782952923, 0.0005945660017774184, 0.0005977555637202202, 0.0005977636407852726, 0.0005983679681720021, 0.000597065665857038, 0.0006001384808597163, 0.0006046090642642096, 0.0006087481853440359, 0.0006126649378558916, 0.0006092457836884678, 0.0006138389948752221, 0.0006121608733580771, 0.0006064297884543598, 0.0006062261662746437, 0.0006043733601890181, 0.0006049244480150539, 0.0006058724332187308, 0.0006093054434060505, 0.0006119916757030255, 0.0006090766131252234, 0.0006093511431461031, 0.0006044032088997987, 0.000604797919542985, 0.0006067307463456979, 0.0006050973132927069, 0.000604396660470267, 0.0006048045501029354, 0.0006045098083844572, 0.0006029378783695701, 0.0006047930015710938, 0.0006014154342438293, 0.0006009718054235734, 0.0005998513942827507, 0.0006015859392761607, 0.000602588715991409, 0.0006003521926751856, 0.000600686598485666, 0.0006023101494196636, 0.0006022917380321051, 0.0006005045537177511, 0.0005981886112556768, 0.0005964143929310393, 0.0005953691585033489, 0.0005947573817693244, 0.0005980148658719072, 0.000597009246715839, 0.0005950559914331454, 0.0005960976349863886, 0.0005944751226454227, 0.0005922798657420631, 0.0005913019546575134, 0.0005919557900634359, 0.0005906883532573757, 0.000587715759032474, 0.0005850399037005975, 0.0005831065351920507, 0.0005818523732648615, 0.0005784604011299459, 0.0005793366329963947, 0.0005779448279981908, 0.0005796388733490796, 0.0005789408764255714, 0.0005790275440482433, 0.0005765998507212504, 0.0005774611203021656, 0.0005784038328573871, 0.0005779121693701503, 0.0005769249325842897, 0.0005759688738934183, 0.000575912143301243, 0.0005752708813571354, 0.0005760623302188325, 0.0005759630471011255, 0.0005765795486996284, 0.0005756213695970484, 0.0005750678684629943, 0.0005750020522956072, 0.0005751273688249903, 0.0005734059335027877, 0.0005726575262212737, 0.0005736950921114814, 0.0005731838704397244, 0.0005722131851051991, 0.0005723928703139413, 0.000570613971702057, 0.0005704290433350456, 0.0005702304899025921, 0.0005693395044126015, 0.000568875634459532, 0.0005677486012624906, 0.0005686771930135369, 0.0005673891972952738, 0.0005669461736048795, 0.0005674343153468387, 0.0005681702375462942, 0.0005681141758318297, 0.0005683697485350536, 0.0005684272835233609, 0.0005683701208130421, 0.0005675522119977619, 0.0005673157377850207, 0.0005676321784667445, 0.0005683303625429979, 0.0005682838501405085, 0.0005686635086722829, 0.0005686168051748174, 0.0005681200548525073, 0.0005679631977934845, 0.0005689741669869121, 0.0005678049351709808, 0.0005677009693957689, 0.0005673836095954837, 0.0005673794832444764, 0.0005678285314941001, 0.0005681260919482728, 0.0005685473941897491, 0.0005693581058503518, 0.0005683692378508755, 0.0005685411746969497, 0.0005682519214637921, 0.0005677091643326716, 0.0005671542373008836, 0.0005665251605020368, 0.0005665514391274967, 0.0005670764877292511, 0.000566827820191153, 0.0005669211164375742, 0.0005673431235814715, 0.00056687572047457, 0.0005673507143892047, 0.0005671422862950124, 0.000567784773474116, 0.0005680114749202926, 0.0005684670004348373, 0.000568811486096373, 0.0005694607841247862, 0.0005694691081180843, 0.000568057244394789, 0.000568499896182845, 0.0005681436459063268, 0.0005680705739782956, 0.0005680353944369234, 0.0005674541383129599, 0.0005673632200843445, 0.0005673621253689966, 0.0005676410568204698, 0.0005671672057530481, 0.0005668924906764486, 0.0005675945202721445, 0.0005674562645169799, 0.0005671263116941073, 0.0005667856712941237, 0.0005668786536734531, 0.0005669571953260625, 0.0005666406569200756, 0.0005658062214779954, 0.0005650514266657388, 0.0005654348391759258, 0.0005648597788047816, 0.0005644939851650476, 0.0005641125727398707, 0.0005640006370481145, 0.0005641244958309798, 0.0005642646697456968, 0.0005644091392877461, 0.0005642342071096015, 0.0005644731616857658, 0.0005650123950217881, 0.0005650988491208549, 0.0005651197483840415, 0.0005651331071370749, 0.0005649784939860362, 0.00056473759820849, 0.000564151334734879, 0.0005639911583205803, 0.0005638308498309048, 0.0005639767948619181, 0.000563752493402504, 0.000563646126003672, 0.0005635825738688771, 0.0005634150349003548, 0.0005632605512491615, 0.0005633314249939219, 0.000563371291873143, 0.0005630931761794986, 0.000562849488450377, 0.0005627992660577777, 0.0005630396004693408, 0.0005629404218830092, 0.0005627658305111832, 0.0005625788406220342, 0.0005627555736178824, 0.0005625538763855268, 0.0005626576667083473, 0.0005628031771658631, 0.0005623651975586245, 0.0005622198950899761, 0.0005627233466695736, 0.0005621858880321168, 0.0005624516853963343, 0.0005619845847576861, 0.0005620360038468833, 0.0005620115033674291, 0.0005619390778352755, 0.0005619015044835418, 0.000561574700687098, 0.0005616225861710881, 0.0005616248064335628, 0.00056152727737412, 0.0005614478077810203, 0.0005608180578425304, 0.0005607390256312281, 0.0005607535499380815, 0.0005607946924977486, 0.0005608306400794954, 0.0005607135935756474, 0.0005604901849810183, 0.0005605581952654326, 0.0005603676792377664, 0.0005599198165174172, 0.0005601262710382988, 0.0005600108669813793, 0.0005601132072957884, 0.0005601279633075271, 0.0005601975398153104, 0.0005601144646448624, 0.0005604542749064196, 0.0005603790585243306, 0.000560108549258295, 0.0005595104886658341, 0.0005592300862910991, 0.0005593114115187267, 0.00055908427370799, 0.0005592003908625866, 0.0005591526233558389, 0.0005592476146801227, 0.0005591856604858872, 0.0005592513221887522, 0.0005591692953277446, 0.0005591396864719089, 0.0005589893056991512, 0.0005590167568781671, 0.0005592239391868208, 0.0005590999403890573, 0.0005591024620736328, 0.0005592227712226108, 0.0005591836436755689, 0.0005589788518135191, 0.0005587835501528315, 0.0005586238977043795, 0.0005584381990981294, 0.0005583933195827338, 0.0005587782160191483, 0.0005586010016300693, 0.0005584359351912192, 0.0005587616465933884, 0.000558847979968081, 0.0005587813792158886, 0.0005587680905639878, 0.0005586100628508445, 0.0005587088154983159, 0.0005585436201052084, 0.0005582857345600639, 0.0005584589140248475, 0.0005585014313491071, 0.0005584807140184466, 0.0005582325155118211, 0.0005580789053130855, 0.0005581745096072864, 0.0005581706705593802, 0.0005581901011189713, 0.0005582732581662237, 0.0005582701536199691, 0.0005581065884542378, 0.0005581340572140223, 0.0005579904391283684, 0.0005579856571323948, 0.0005577415836234834, 0.0005578718682894804, 0.000558001506559075, 0.0005579698406448097, 0.0005580157177112166, 0.0005580596033165398, 0.0005579963662342052, 0.0005580977578633561, 0.0005581881638346578, 0.0005581878966382092, 0.0005582797756234417, 0.0005581456285548134, 0.0005578709618127714, 0.0005579046619822265, 0.0005578109553110458, 0.0005578021966872889, 0.000557662004164116, 0.0005576554169237082, 0.0005577782997373756, 0.0005578008673204695, 0.0005578683359551699, 0.0005579066390636397, 0.0005579800068988205, 0.0005579374596599133, 0.0005578747254745752, 0.0005579334748902454, 0.0005579659779436268, 0.000557981649664693, 0.0005577608785507762, 0.0005576621999408031, 0.0005578276495548214, 0.0005579240716146897, 0.0005578227661649784, 0.0005578086621175007, 0.0005577580473982779, 0.000557818182722927, 0.0005574958277987412, 0.0005574732610109097, 0.0005574720927051006, 0.0005574763180032461, 0.0005573383776201647, 0.0005573973809818109, 0.0005575769988652368, 0.0005577454227949213, 0.0005576825902549141, 0.0005575978171471981, 0.0005576012579825242, 0.0005577033914509286, 0.0005577120373454631, 0.0005575293412529233, 0.0005575502577746863, 0.0005575323792420547, 0.0005574864937287329, 0.0005575035985764974, 0.0005575916427366058, 0.0005574704473332138, 0.0005574088601189565, 0.0005573698137194553, 0.0005574034338031366, 0.0005574882925389521, 0.0005575565704490389, 0.0005577118253203952, 0.000557751553356749, 0.0005577306673373067, 0.0005578820046380751, 0.0005578760471356216, 0.0005579391618645656, 0.0005579591803431638, 0.0005579122978417089, 0.0005579451796451608, 0.0005579660586267534, 0.0005579925148923502, 0.000558008317204234, 0.0005578766838005133, 0.0005578507184987222, 0.0005579323248796033, 0.0005577091140914012, 0.0005576418219789856, 0.000557613322089362, 0.0005575952541611267, 0.0005576186917376039, 0.0005575918580227587, 0.0005575615480565858, 0.0005575543851585282, 0.0005575908797853277, 0.0005575865441468131, 0.0005575767114350197, 0.0005575360286784591, 0.0005575374483332329, 0.0005575988342903386, 0.000557601874023533, 0.0005576135722834071, 0.000557625639054015, 0.0005575873588104006, 0.0005575238331308104, 0.0005574765302889826, 0.0005574159506944497, 0.0005571978246200605, 0.0005571483808533332, 0.0005571960610522042, 0.0005571296555439595, 0.0005569617817031557, 0.0005569484808394961, 0.0005569888170444926, 0.0005570491678939362, 0.0005570298073046041, 0.0005568700646024557, 0.0005567911572332289, 0.0005567370001345929, 0.0005568115080307105, 0.0005567709508974905, 0.0005566580761893539, 0.0005566563050027235, 0.0005566511586154724, 0.0005564855148458507, 0.0005565579327937016, 0.0005565239509079184, 0.0005565169541533068, 0.0005564275769019595, 0.0005564497942787817, 0.0005564703895947456, 0.0005564883920674934, 0.0005565107159058687, 0.0005565144537180674, 0.0005564217890003356, 0.0005562907806904529, 0.0005562520308628391, 0.0005561113388774913, 0.000556038938571252, 0.0005560380073237468, 0.0005561067815100705, 0.0005560438659131561, 0.0005558611201988797, 0.000555994796021466, 0.0005558576678899902, 0.0005558317578452658, 0.0005558417100815276, 0.0005558212178456301, 0.0005557792876792359, 0.0005557904660643848, 0.0005558395268360134, 0.0005557895203722944, 0.0005558093596678265, 0.0005557331746463196, 0.0005557034163078417, 0.0005557507641650889, 0.0005557191654378036, 0.0005556957585543859, 0.0005556797678163626, 0.000555670287189927, 0.0005557084382070016, 0.0005557369866640783, 0.0005557804273452682, 0.0005557984477158526, 0.0005556977240333717, 0.0005557192007907344, 0.0005557353505889132, 0.0005557703579335179, 0.0005557847546195867, 0.0005556258815620013, 0.0005555574929544887, 0.0005555830372342245, 0.0005556217740247876, 0.0005555594354418115, 0.0005555610086943779, 0.0005554761042900457, 0.0005554737953325582, 0.0005554812151807627, 0.0005554810297321276, 0.0005554992134300387, 0.0005555196151583299, 0.0005555705481997943, 0.0005555851653599028, 0.0005556364564554132, 0.0005556324363930491, 0.0005556235104008225, 0.0005556498075643888, 0.000555727740805967, 0.0005557360101302142, 0.0005556771776701003, 0.0005555973581083506, 0.0005555770925583569, 0.0005555313230037672, 0.0005555742489018956, 0.0005555740492073694, 0.0005555512342005523, 0.0005556043278691746, 0.0005555088233343271, 0.0005554870244095867, 0.0005554825676180798, 0.0005554951147411762, 0.0005554272225449413, 0.0005554697779861166, 0.0005554702184800981, 0.0005554623020449513, 0.0005555001916969449, 0.0005554595184419648, 0.0005554179692566635, 0.0005553984869927103, 0.0005553654861622441, 0.0005553458425788326, 0.000555259920818722, 0.0005552282824274444, 0.0005552514418483501, 0.0005552597902935037, 0.0005552547525203182, 0.0005552410801172173, 0.0005552685531542292, 0.0005553281953274972, 0.000555353807384757, 0.0005552429419682183, 0.000555252598850041, 0.0005552398597675933, 0.0005551289826762342, 0.0005551744738221401, 0.0005551722356836429, 0.0005550917137236754, 0.0005551652150991321, 0.0005551673815359568, 0.0005551626295070974, 0.000555197716497485, 0.0005551836438162748, 0.0005552010091666931, 0.0005552087193821259, 0.0005551636793141493, 0.0005550300696109864, 0.0005551401263513595, 0.0005551920722313738, 0.0005551706276764451, 0.0005551931499139975, 0.0005551984274755333, 0.0005552649122155453, 0.0005552797934603794, 0.000555369636318557, 0.0005553650011584292, 0.0005553772513847115, 0.0005553921640306553, 0.0005552714872368174, 0.0005552804394979459, 0.000555309252820527, 0.000555286939729576, 0.0005553463532153811, 0.0005553428039889803, 0.0005553085696037109, 0.0005553149941731681, 0.0005553463101478818, 0.0005553106794705155, 0.0005553590861943407, 0.0005552694215149878, 0.0005552786209257955, 0.0005552795382030341, 0.0005552836669648239, 0.0005552932704820312, 0.0005552082626898712, 0.0005552485201465291, 0.0005552067450555109, 0.0005552122650655392, 0.0005552192253654463, 0.0005552449990856351, 0.0005551727492852357, 0.0005551389902566546, 0.0005551184791187987, 0.0005551241687597606, 0.0005551660042858531, 0.0005551467225690585, 0.0005551571022093677, 0.0005550989160360467, 0.0005551245746876737, 0.0005551410209527622, 0.0005551176009373279, 0.0005550500227800164, 0.0005549411131152742, 0.0005549610145313362, 0.0005549474940512205, 0.000555006110005083, 0.0005549750793517581, 0.0005549572119824626, 0.0005549248303426552, 0.000554924533870236, 0.0005549106773029697, 0.0005548252762820074, 0.0005548289386310695, 0.0005548405720558953, 0.0005548697515942663, 0.0005548732613448221, 0.0005548102188303945, 0.0005548119356505437, 0.0005548058779149622, 0.0005548147207860002, 0.0005548457039053863, 0.0005548642769092245, 0.0005549015151994322, 0.0005548319127243557, 0.0005548696293079384, 0.0005548636910322897, 0.0005547961799378899, 0.0005548112980987669, 0.0005548281027021607, 0.0005548156947596118, 0.0005547553527488715, 0.0005547683421227766, 0.0005546907765454128, 0.0005546639125794123, 0.0005545599876309252, 0.0005545148852952952, 0.0005544809704737129, 0.000554442835264738, 0.0005543917227652587, 0.0005544029500567556, 0.0005543311136531353, 0.0005543466601747881, 0.0005543509173798099, 0.0005543337456579808, 0.0005543295684038644, 0.0005543525517138424, 0.0005543384608320655, 0.0005543255482228658, 0.0005543360483766905, 0.000554346590144734, 0.0005543630540941492, 0.000554372426923844, 0.0005543668597594904, 0.0005543564676329613, 0.0005543653908788633, 0.0005543117825826308, 0.0005543217416724123, 0.000554328103934926, 0.000554338061701941, 0.0005543438639288547, 0.0005543549145113476, 0.0005543996192760834, 0.0005543886632797815, 0.0005544144460442789, 0.0005544141518046112, 0.0005543891342183935, 0.0005543314786906578, 0.0005543254522984956, 0.0005542668406598169, 0.0005542643017080532, 0.0005542565062501109, 0.0005542445152754911, 0.0005542446141510783, 0.0005542179210276402, 0.0005542104735536933, 0.0005542067646041712, 0.0005542401685536238, 0.000554252689179011, 0.0005542150270072151, 0.0005542342940567553, 0.00055424972820919, 0.0005542546675062342, 0.0005542454126918321, 0.0005542595062078199, 0.0005542633479862426, 0.0005542317738832099, 0.0005542232102829163, 0.0005542164576395925, 0.0005542224596142719, 0.0005542545745579636, 0.0005542546370170224, 0.0005542427973607063, 0.0005541736297985806, 0.0005541659288901108, 0.0005541747170683243, 0.0005541828330105073, 0.0005541999683117073, 0.000554175779307087, 0.0005541916845536967, 0.0005541782029710173, 0.0005541591977327112, 0.0005541472909426198, 0.0005541365509585528, 0.0005541467986020511, 0.0005541528988990936, 0.0005541420542257543, 0.0005541640633716116, 0.0005541687090390462, 0.0005541821331088403, 0.0005541510568991968, 0.0005541569201011587, 0.0005541418941227732, 0.0005541460650462634, 0.000554123385352138, 0.0005541127124420553, 0.0005541186478024393, 0.0005541090912969069, 0.0005541014092416333, 0.0005540917703654299, 0.0005540965371410851, 0.0005540882317233973, 0.0005541215490348468, 0.0005541491736236193, 0.0005541299895256781, 0.000554138321330634, 0.0005541508867670899, 0.0005541183021102657, 0.0005540886849937595, 0.0005540387934727558, 0.0005540092797174869, 0.0005540158278786985, 0.0005540492633162312, 0.0005540229158321334, 0.0005540223962062248, 0.0005540466207835827, 0.0005540617619678153, 0.0005540836968033165, 0.0005540965144635434, 0.0005540847820159547, 0.000554067303390359, 0.0005540861152424283, 0.0005540676315358808, 0.0005540907439773732, 0.0005540975470935282, 0.0005540882053454103, 0.000554096625952908, 0.0005540976791355653, 0.0005540512168224577, 0.0005540067690333821, 0.0005540128010438158, 0.00055400423996188, 0.0005540080546286696, 0.0005540240904831136, 0.0005540414428956045, 0.0005540782630468141, 0.0005540690523844513, 0.0005540886402163527, 0.0005541280590231029, 0.0005541097921296674, 0.0005540896753865194, 0.0005541113911595085, 0.0005541156888171733, 0.0005541028768870406, 0.0005541194299896871, 0.0005540747941953462, 0.0005541038399359799, 0.0005540992228349576, 0.000554084131199912, 0.0005540886986125556, 0.000554069989338776, 0.0005540913542000296, 0.0005540999179841848, 0.0005541019929556576, 0.0005541244009298987, 0.0005540887264794652, 0.0005540976856341958, 0.0005540880575793944, 0.0005540694529194719, 0.0005540692998069452, 0.0005540488849586461, 0.0005540445541340218, 0.0005540548742739818, 0.0005540497288854809, 0.0005540595495950817, 0.0005540454576452745, 0.0005540720728912747, 0.0005540563398112002, 0.0005540687671325015, 0.0005540835304194946, 0.0005541241592361774, 0.0005541253546390645, 0.0005541071370324874, 0.000554108128838668, 0.0005541015152099916, 0.0005540579938265297, 0.0005540771092048484, 0.0005540573056772994, 0.0005540610464980125, 0.0005540358197918841, 0.0005540443940530489, 0.000554051210080894, 0.0005540160371050075, 0.0005540231483190405, 0.000554054920444649, 0.0005540720383434016, 0.000554076868577782, 0.0005540573594333939, 0.0005540802288528896, 0.0005540841692000604, 0.0005540720855042518, 0.0005540728385687702, 0.0005540783995311447, 0.0005540779915057907, 0.0005540677678407987, 0.0005540838661879124, 0.0005540812940851611, 0.0005540838737951838, 0.0005541086563616166, 0.0005540969345593927, 0.0005541081521697137, 0.0005540727622376768, 0.0005540743125747517, 0.0005540804068263153, 0.00055409810387231, 0.0005541021558546484, 0.0005540920721012585, 0.0005541242825584073, 0.0005541316798673168, 0.0005541278651151699, 0.0005540824051256568, 0.0005540785123395623, 0.00055410437419028, 0.0005541042518815069, 0.0005541183573281167, 0.0005541164259949732, 0.0005540936338264124, 0.0005540924409503582, 0.0005540713329599987, 0.0005540417430346818, 0.0005540165599343035, 0.0005540311327612834, 0.0005540068682934369, 0.0005540253347609452, 0.0005540296401369793, 0.0005540309712241092, 0.0005540358369474816, 0.0005540209341740838, 0.0005540519160267158, 0.0005540331887076801, 0.0005540091452133401, 0.0005540287186883604, 0.0005540062666159684, 0.0005540098133294832, 0.0005540124239128762, 0.0005539989808063773, 0.0005540202064638515, 0.0005540236767612777, 0.000554046213046874, 0.0005540419905950741, 0.000554037614097432, 0.0005540347099752395, 0.0005540195352504001, 0.0005540304459392255, 0.0005540352076475311, 0.0005540314757226613, 0.0005540558732565406, 0.0005540511039588385, 0.0005540541551596981, 0.0005540443328861847, 0.000554039375554863, 0.0005540540492476905, 0.000554060741702502, 0.0005540670129421491, 0.0005540247835562767, 0.0005540062786325905, 0.0005539950193352148, 0.0005539917781200485, 0.0005539985884662057, 0.0005540103648762776, 0.0005540361542723358, 0.0005540433088544928, 0.0005540507912968774, 0.000554066295120791, 0.0005540682039989894, 0.0005540662489931363, 0.0005540719632257413, 0.0005540637041889346, 0.000554067168557842]})])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcCM6mwRLjGB"
      },
      "source": [
        "#bestTest = 0.03697023278\n",
        "#bestIteration = 999\n",
        "#'depth': 10, 'l2_leaf_reg': 1, 'learning_rate': 0.1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKL3sQ3xsnwC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYyO5rznmekX"
      },
      "source": [
        "#Ref : https://setscholars.net/wp-content/uploads/2019/02/find-optimal-parameters-for-CatBoost-using-GridSearchCV-for-Classification-in-Python.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnzNPaaSsnjW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkLHKWlE-CJy"
      },
      "source": [
        "#### 10.4 Light + Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HHHkFfSmegb"
      },
      "source": [
        "from scipy.stats import uniform as sp_randFloat\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import uniform as sp_uniform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezHm5dkEcvOd"
      },
      "source": [
        "param_test ={'num_leaves': sp_randint(6, 50), \n",
        "             'min_child_samples': sp_randint(100, 500), \n",
        "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
        "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
        "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
        "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
        "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVjAmvIhdhTt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4gAbgb3-KvC"
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "# n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
        "# clf = lgb.LGBMClassifier(max_depth = -1, random_state = 314, silent = True, metric = 'None', n_jobs = 4, n_estimators = 5000)\n",
        "clf = lgb.LGBMClassifier()\n",
        "\n",
        "\n",
        "light_search = RandomizedSearchCV( \n",
        "    estimator = clf, param_distributions = param_test, \n",
        "    n_iter = 100,\n",
        "    refit = True,\n",
        "    random_state = 314,\n",
        "    verbose = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zufdhWHzcLDj",
        "outputId": "738b9a6d-9e08-400b-ea4c-7badae370a82"
      },
      "source": [
        "light_search.fit(X_train5, y_train5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 44.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=None, error_score=nan,\n",
              "                   estimator=LGBMClassifier(boosting_type='gbdt',\n",
              "                                            class_weight=None,\n",
              "                                            colsample_bytree=1.0,\n",
              "                                            importance_type='split',\n",
              "                                            learning_rate=0.1, max_depth=-1,\n",
              "                                            min_child_samples=20,\n",
              "                                            min_child_weight=0.001,\n",
              "                                            min_split_gain=0.0,\n",
              "                                            n_estimators=100, n_jobs=-1,\n",
              "                                            num_leaves=31, objective=None,\n",
              "                                            random_state=None, reg_alpha=0.0,\n",
              "                                            reg_lambda=0.0, s...\n",
              "                                                             10000.0],\n",
              "                                        'num_leaves': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fba32556850>,\n",
              "                                        'reg_alpha': [0, 0.1, 1, 2, 5, 7, 10,\n",
              "                                                      50, 100],\n",
              "                                        'reg_lambda': [0, 0.1, 1, 5, 10, 20, 50,\n",
              "                                                       100],\n",
              "                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fba32556c10>},\n",
              "                   pre_dispatch='2*n_jobs', random_state=314, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVIJJ3zXcLAL",
        "outputId": "2e55c221-cc38-44a0-fb0a-25e00aa765be"
      },
      "source": [
        "light_search.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'colsample_bytree': 0.6810492267417421,\n",
              " 'min_child_samples': 272,\n",
              " 'min_child_weight': 1,\n",
              " 'num_leaves': 26,\n",
              " 'reg_alpha': 0.1,\n",
              " 'reg_lambda': 0.1,\n",
              " 'subsample': 0.48511105572491486}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DbGXdDccK9K",
        "outputId": "cc1099ce-9a59-46f5-8d96-96b38fe180d8"
      },
      "source": [
        "print (\"Best Estimator : \" , light_search.best_estimator_)\n",
        "print (\"Best Params : \" , light_search.best_params_)\n",
        "print (\"Best Score : \" , light_search.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Estimator :  LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
            "               colsample_bytree=0.6810492267417421, importance_type='split',\n",
            "               learning_rate=0.1, max_depth=-1, min_child_samples=272,\n",
            "               min_child_weight=1, min_split_gain=0.0, n_estimators=100,\n",
            "               n_jobs=-1, num_leaves=26, objective=None, random_state=None,\n",
            "               reg_alpha=0.1, reg_lambda=0.1, silent=True,\n",
            "               subsample=0.48511105572491486, subsample_for_bin=200000,\n",
            "               subsample_freq=0)\n",
            "Best Params :  {'colsample_bytree': 0.6810492267417421, 'min_child_samples': 272, 'min_child_weight': 1, 'num_leaves': 26, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.48511105572491486}\n",
            "Best Score :  0.9999926416482708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RObcb5zy8vz7"
      },
      "source": [
        "light = lgb.LGBMClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAmDpwoScK6j",
        "outputId": "f42b0a00-955b-47d4-b606-f75cdb4e9972"
      },
      "source": [
        "light = lgb.LGBMClassifier (colsample_bytree = 0.6810492267417421,\n",
        "                      min_child_samples = 272,\n",
        "                      min_child_weight = 1,\n",
        "                      num_leaves = 26,\n",
        "                      reg_alpha = 0.1,\n",
        "                      reg_lambda = 0.1,\n",
        "                      subsample = 0.48511105572491486 )\n",
        "\n",
        "build_model_new (light, X_train5 , y_train5 , X_test5 , y_test5)\n",
        "light_train_param , light_test_param = save_model_new(light,'light_model_para', X_train5 , y_train5 , X_test5 , y_test5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
            "               colsample_bytree=0.6810492267417421, importance_type='split',\n",
            "               learning_rate=0.1, max_depth=-1, min_child_samples=272,\n",
            "               min_child_weight=1, min_split_gain=0.0, n_estimators=100,\n",
            "               n_jobs=-1, num_leaves=26, objective=None, random_state=None,\n",
            "               reg_alpha=0.1, reg_lambda=0.1, silent=True,\n",
            "               subsample=0.48511105572491486, subsample_for_bin=200000,\n",
            "               subsample_freq=0)\n",
            "Accuracy of Model: 0.9999411331861663\n",
            "Precision of Model: 0.999941152801899\n",
            "Recall of Model: 0.9999411331861663\n",
            "F1-score of Model: 0.9999411309225273\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       1.00      1.00      1.00      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      1.00      1.00      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           1.00     33975\n",
            "   macro avg       1.00      1.00      1.00     33975\n",
            "weighted avg       1.00      1.00      1.00     33975\n",
            "\n",
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  light_model_para.sav\n",
            "Accuracy Model:  0.9999411331861663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl_trHMscK3I"
      },
      "source": [
        "## REF : https://www.kaggle.com/rtatman/lightgbm-hyperparameter-optimisation-lb-0-761#Model-fitting-with-HyperParameter-optimisation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzIn9W9Q9j3T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26oueYEM9Kwh"
      },
      "source": [
        "### 11. Ensemble Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRcWpTc0FcHa"
      },
      "source": [
        "#### 11.1 Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4jNeTGCA-np",
        "outputId": "4393fab2-0133-4eb8-fb19-10e77a888e22"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Newi/2021Project3/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "01_data_cleanup.ipynb\t\t      catboost_info  models5\n",
            "02_data_analysis.ipynb\t\t      Data\t     processed\n",
            "03_ml_classifier.ipynb\t\t      Dataupdate     result_model\n",
            "04_dl_anomaly_detection.ipynb\t      mlids\t     tmp\n",
            "05_dl_classifier.ipynb\t\t      models\n",
            "6_binary_classifier_comparison.ipynb  models4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzmh4v6MA48w"
      },
      "source": [
        " def load_model(file,X_train5,X_test5,y_train5,y_test5): \n",
        "  loaded_model = pickle.load(open('./result_model/' + file , 'rb'))\n",
        "  result = loaded_model.score(X_test5, y_test5)\n",
        "  print(\"Accuracy Model \", result)\n",
        "  #-------------------------------------------------------------------------------\n",
        "  #name_model = name_model\n",
        "  #train = name_model + 'train'\n",
        "  train = loaded_model.predict(X_train5)\n",
        "  test = loaded_model.predict(X_test5)\n",
        "\n",
        "  return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9c9H79nEBi5",
        "outputId": "2313ec72-5727-4bd2-c7ef-43cc0e016240"
      },
      "source": [
        "rf_train_param , rf_test_param = load_model('rf_model_para.sav')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Model  0.9860485651214128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9q0PsblA40m",
        "outputId": "8c038a9d-9c3c-450a-85f1-5e49ca3fed9e"
      },
      "source": [
        "xg_train_param , xg_test_param = load_model('xg_model_para.sav')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Model  0.9859896983075791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfaNEvLBEBWH",
        "outputId": "5981ed58-c3cf-4acb-e626-6787dc2ec1f9"
      },
      "source": [
        "cat_train_param , cat_test_param = load_model('cat_model_para.sav')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Model  0.986019131714496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmLBfjpfFFBa",
        "outputId": "a3e5de2c-6936-4dcc-e6b4-a9794f1e84df"
      },
      "source": [
        "light_train_param , light_test_param = load_model('light_model_para.sav')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Model  0.9999411331861663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZW1uqdhFE5Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKFd28BLFm5R"
      },
      "source": [
        "#### 11.2 Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "J-xALz3Y-Kng",
        "outputId": "e00a8eca-9a7e-4ec4-a3d8-69e97a9fdc9b"
      },
      "source": [
        "# Use the outputs of 4 base models to construct a new ensemble model\n",
        "base_predictions_train = pd.DataFrame( {\n",
        "    'RandomForest': light_train_param.ravel(),\n",
        "    'XgBoost': xg_train_param.ravel(),\n",
        "    'CatBoost': cat_train_param.ravel(), \n",
        "    'Lightgbm': light_train_param.ravel()\n",
        "    })\n",
        "base_predictions_train.head(9) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RandomForest</th>\n",
              "      <th>XgBoost</th>\n",
              "      <th>CatBoost</th>\n",
              "      <th>Lightgbm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   RandomForest  XgBoost  CatBoost  Lightgbm\n",
              "0             8        8         8         8\n",
              "1             9        9         9         9\n",
              "2             0        0         0         0\n",
              "3            14       14        14        14\n",
              "4            14       14        14        14\n",
              "5            14       14        14        14\n",
              "6             4        4         4         4\n",
              "7             7        7         7         7\n",
              "8             0        0         0         0"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc1XAjnK-KgL"
      },
      "source": [
        "rf_train_param = rf_train_param.reshape(-1, 1)\n",
        "xg_train_param = xg_train_param.reshape(-1, 1)\n",
        "cat_train_param = cat_train_param.reshape(-1, 1)\n",
        "light_train_param = light_train_param.reshape(-1, 1)\n",
        "\n",
        "rf_test_param = rf_test_param.reshape(-1, 1)\n",
        "xg_test_param = xg_test_param.reshape(-1, 1)\n",
        "cat_test_param = cat_test_param.reshape(-1, 1)\n",
        "light_test_param = light_test_param.reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgfZ8qaemebi"
      },
      "source": [
        "X_train_stack = np.concatenate(( rf_train_param, xg_train_param, cat_train_param, light_train_param), axis=1)\n",
        "X_test_stack = np.concatenate((  rf_test_param, xg_test_param, cat_test_param, light_test_param), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUgwbzArJT5X",
        "outputId": "3d02aa0f-f131-47ce-dc69-665ae2ebebe0"
      },
      "source": [
        "build_model_new (rf, X_train_stack , y_train5 , X_test_stack , y_test5)\n",
        "rf_train_stack , rf_test_stack = save_model_new(rf,'rf_model_stack', X_train_stack , y_train5 , X_test_stack , y_test5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
            "                       warm_start=False)\n",
            "Accuracy of Model: 1.0\n",
            "Precision of Model: 1.0\n",
            "Recall of Model: 1.0\n",
            "F1-score of Model: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       1.00      1.00      1.00      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      1.00      1.00      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           1.00     33975\n",
            "   macro avg       1.00      1.00      1.00     33975\n",
            "weighted avg       1.00      1.00      1.00     33975\n",
            "\n",
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  rf_model_stack.sav\n",
            "Accuracy Model:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP2ObMifHNSy",
        "outputId": "838e146b-1bbe-41b2-98c5-5893635d423a"
      },
      "source": [
        "build_model_new (xg, X_train_stack , y_train5 , X_test_stack , y_test5)\n",
        "xg_train_stack , xg_test_stack = save_model_new(xg,'xg_model_stack', X_train_stack , y_train5 , X_test_stack , y_test5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
            "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
            "              nthread=None, objective='multi:softprob', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n",
            "Accuracy of Model: 0.9999705665930831\n",
            "Precision of Model: 0.9999705764009493\n",
            "Recall of Model: 0.9999705665930831\n",
            "F1-score of Model: 0.999970567253579\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       1.00      1.00      1.00      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      1.00      1.00      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           1.00     33975\n",
            "   macro avg       1.00      1.00      1.00     33975\n",
            "weighted avg       1.00      1.00      1.00     33975\n",
            "\n",
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  xg_model_stack.sav\n",
            "Accuracy Model:  0.9999705665930831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccZXejcrJf71",
        "outputId": "4b52884b-1bc1-4128-cd3e-05b98557fa32"
      },
      "source": [
        "build_model_new (cat, X_train_stack , y_train5 , X_test_stack , y_test5)\n",
        "cat_train_stack , cat_test_cat = save_model_new(cat,'cat_model_stack', X_train_stack , y_train5 , X_test_stack , y_test5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<catboost.core.CatBoostClassifier object at 0x7f7e1e46a150>\n",
            "Learning rate set to 0.101652\n",
            "0:\tlearn: 1.8151132\ttotal: 111ms\tremaining: 1m 50s\n",
            "1:\tlearn: 1.4722676\ttotal: 156ms\tremaining: 1m 17s\n",
            "2:\tlearn: 1.2234231\ttotal: 206ms\tremaining: 1m 8s\n",
            "3:\tlearn: 1.0524439\ttotal: 260ms\tremaining: 1m 4s\n",
            "4:\tlearn: 0.9276098\ttotal: 319ms\tremaining: 1m 3s\n",
            "5:\tlearn: 0.8161370\ttotal: 365ms\tremaining: 1m\n",
            "6:\tlearn: 0.7278537\ttotal: 422ms\tremaining: 59.8s\n",
            "7:\tlearn: 0.6481370\ttotal: 481ms\tremaining: 59.6s\n",
            "8:\tlearn: 0.5820416\ttotal: 534ms\tremaining: 58.8s\n",
            "9:\tlearn: 0.5303777\ttotal: 584ms\tremaining: 57.8s\n",
            "10:\tlearn: 0.4752879\ttotal: 646ms\tremaining: 58.1s\n",
            "11:\tlearn: 0.4282923\ttotal: 698ms\tremaining: 57.4s\n",
            "12:\tlearn: 0.3877855\ttotal: 743ms\tremaining: 56.4s\n",
            "13:\tlearn: 0.3555689\ttotal: 797ms\tremaining: 56.1s\n",
            "14:\tlearn: 0.3231652\ttotal: 840ms\tremaining: 55.2s\n",
            "15:\tlearn: 0.2972040\ttotal: 894ms\tremaining: 55s\n",
            "16:\tlearn: 0.2707233\ttotal: 940ms\tremaining: 54.3s\n",
            "17:\tlearn: 0.2497477\ttotal: 993ms\tremaining: 54.2s\n",
            "18:\tlearn: 0.2303544\ttotal: 1.04s\tremaining: 53.7s\n",
            "19:\tlearn: 0.2105471\ttotal: 1.09s\tremaining: 53.6s\n",
            "20:\tlearn: 0.1927915\ttotal: 1.14s\tremaining: 53.3s\n",
            "21:\tlearn: 0.1771610\ttotal: 1.2s\tremaining: 53.2s\n",
            "22:\tlearn: 0.1634676\ttotal: 1.24s\tremaining: 52.8s\n",
            "23:\tlearn: 0.1516833\ttotal: 1.29s\tremaining: 52.4s\n",
            "24:\tlearn: 0.1393574\ttotal: 1.33s\tremaining: 52s\n",
            "25:\tlearn: 0.1294444\ttotal: 1.39s\tremaining: 52.1s\n",
            "26:\tlearn: 0.1190719\ttotal: 1.44s\tremaining: 52s\n",
            "27:\tlearn: 0.1108100\ttotal: 1.5s\tremaining: 51.9s\n",
            "28:\tlearn: 0.1022135\ttotal: 1.55s\tremaining: 51.8s\n",
            "29:\tlearn: 0.0941506\ttotal: 1.6s\tremaining: 51.6s\n",
            "30:\tlearn: 0.0873418\ttotal: 1.64s\tremaining: 51.4s\n",
            "31:\tlearn: 0.0805498\ttotal: 1.7s\tremaining: 51.4s\n",
            "32:\tlearn: 0.0750905\ttotal: 1.74s\tremaining: 51.1s\n",
            "33:\tlearn: 0.0700460\ttotal: 1.79s\tremaining: 50.9s\n",
            "34:\tlearn: 0.0649859\ttotal: 1.83s\tremaining: 50.6s\n",
            "35:\tlearn: 0.0601032\ttotal: 1.88s\tremaining: 50.3s\n",
            "36:\tlearn: 0.0560354\ttotal: 1.92s\tremaining: 50s\n",
            "37:\tlearn: 0.0516789\ttotal: 1.97s\tremaining: 49.8s\n",
            "38:\tlearn: 0.0482004\ttotal: 2.02s\tremaining: 49.7s\n",
            "39:\tlearn: 0.0451603\ttotal: 2.07s\tremaining: 49.6s\n",
            "40:\tlearn: 0.0419558\ttotal: 2.13s\tremaining: 49.8s\n",
            "41:\tlearn: 0.0388214\ttotal: 2.18s\tremaining: 49.8s\n",
            "42:\tlearn: 0.0360139\ttotal: 2.23s\tremaining: 49.6s\n",
            "43:\tlearn: 0.0336983\ttotal: 2.27s\tremaining: 49.4s\n",
            "44:\tlearn: 0.0313409\ttotal: 2.32s\tremaining: 49.3s\n",
            "45:\tlearn: 0.0291401\ttotal: 2.37s\tremaining: 49.2s\n",
            "46:\tlearn: 0.0272804\ttotal: 2.42s\tremaining: 49.1s\n",
            "47:\tlearn: 0.0255212\ttotal: 2.47s\tremaining: 49s\n",
            "48:\tlearn: 0.0239935\ttotal: 2.53s\tremaining: 49.1s\n",
            "49:\tlearn: 0.0222621\ttotal: 2.57s\tremaining: 48.9s\n",
            "50:\tlearn: 0.0207150\ttotal: 2.62s\tremaining: 48.8s\n",
            "51:\tlearn: 0.0192945\ttotal: 2.66s\tremaining: 48.5s\n",
            "52:\tlearn: 0.0178984\ttotal: 2.7s\tremaining: 48.3s\n",
            "53:\tlearn: 0.0165897\ttotal: 2.75s\tremaining: 48.3s\n",
            "54:\tlearn: 0.0155916\ttotal: 2.79s\tremaining: 48s\n",
            "55:\tlearn: 0.0145670\ttotal: 2.84s\tremaining: 47.9s\n",
            "56:\tlearn: 0.0135528\ttotal: 2.89s\tremaining: 47.8s\n",
            "57:\tlearn: 0.0127748\ttotal: 2.93s\tremaining: 47.5s\n",
            "58:\tlearn: 0.0118482\ttotal: 2.97s\tremaining: 47.4s\n",
            "59:\tlearn: 0.0110743\ttotal: 3.02s\tremaining: 47.3s\n",
            "60:\tlearn: 0.0104189\ttotal: 3.07s\tremaining: 47.2s\n",
            "61:\tlearn: 0.0098103\ttotal: 3.11s\tremaining: 47.1s\n",
            "62:\tlearn: 0.0091380\ttotal: 3.15s\tremaining: 46.9s\n",
            "63:\tlearn: 0.0085641\ttotal: 3.19s\tremaining: 46.7s\n",
            "64:\tlearn: 0.0079877\ttotal: 3.24s\tremaining: 46.6s\n",
            "65:\tlearn: 0.0074444\ttotal: 3.29s\tremaining: 46.6s\n",
            "66:\tlearn: 0.0069732\ttotal: 3.35s\tremaining: 46.6s\n",
            "67:\tlearn: 0.0065612\ttotal: 3.39s\tremaining: 46.5s\n",
            "68:\tlearn: 0.0061273\ttotal: 3.43s\tremaining: 46.3s\n",
            "69:\tlearn: 0.0057261\ttotal: 3.49s\tremaining: 46.4s\n",
            "70:\tlearn: 0.0053705\ttotal: 3.56s\tremaining: 46.5s\n",
            "71:\tlearn: 0.0050629\ttotal: 3.62s\tremaining: 46.7s\n",
            "72:\tlearn: 0.0047353\ttotal: 3.68s\tremaining: 46.8s\n",
            "73:\tlearn: 0.0044586\ttotal: 3.74s\tremaining: 46.9s\n",
            "74:\tlearn: 0.0041808\ttotal: 3.79s\tremaining: 46.8s\n",
            "75:\tlearn: 0.0039560\ttotal: 3.83s\tremaining: 46.6s\n",
            "76:\tlearn: 0.0036854\ttotal: 3.88s\tremaining: 46.5s\n",
            "77:\tlearn: 0.0034638\ttotal: 3.92s\tremaining: 46.3s\n",
            "78:\tlearn: 0.0032378\ttotal: 3.97s\tremaining: 46.2s\n",
            "79:\tlearn: 0.0030306\ttotal: 4.01s\tremaining: 46.1s\n",
            "80:\tlearn: 0.0028709\ttotal: 4.05s\tremaining: 45.9s\n",
            "81:\tlearn: 0.0027127\ttotal: 4.1s\tremaining: 45.9s\n",
            "82:\tlearn: 0.0025524\ttotal: 4.15s\tremaining: 45.8s\n",
            "83:\tlearn: 0.0023920\ttotal: 4.2s\tremaining: 45.8s\n",
            "84:\tlearn: 0.0022465\ttotal: 4.24s\tremaining: 45.6s\n",
            "85:\tlearn: 0.0021191\ttotal: 4.28s\tremaining: 45.5s\n",
            "86:\tlearn: 0.0019919\ttotal: 4.33s\tremaining: 45.4s\n",
            "87:\tlearn: 0.0018723\ttotal: 4.37s\tremaining: 45.3s\n",
            "88:\tlearn: 0.0017776\ttotal: 4.42s\tremaining: 45.2s\n",
            "89:\tlearn: 0.0016769\ttotal: 4.47s\tremaining: 45.2s\n",
            "90:\tlearn: 0.0015916\ttotal: 4.51s\tremaining: 45.1s\n",
            "91:\tlearn: 0.0014976\ttotal: 4.57s\tremaining: 45.1s\n",
            "92:\tlearn: 0.0014178\ttotal: 4.62s\tremaining: 45s\n",
            "93:\tlearn: 0.0013475\ttotal: 4.67s\tremaining: 45s\n",
            "94:\tlearn: 0.0012815\ttotal: 4.72s\tremaining: 44.9s\n",
            "95:\tlearn: 0.0012132\ttotal: 4.78s\tremaining: 45s\n",
            "96:\tlearn: 0.0011563\ttotal: 4.83s\tremaining: 44.9s\n",
            "97:\tlearn: 0.0010925\ttotal: 4.88s\tremaining: 44.9s\n",
            "98:\tlearn: 0.0010346\ttotal: 4.92s\tremaining: 44.8s\n",
            "99:\tlearn: 0.0009789\ttotal: 4.97s\tremaining: 44.8s\n",
            "100:\tlearn: 0.0009297\ttotal: 5.02s\tremaining: 44.7s\n",
            "101:\tlearn: 0.0008939\ttotal: 5.06s\tremaining: 44.6s\n",
            "102:\tlearn: 0.0008491\ttotal: 5.12s\tremaining: 44.6s\n",
            "103:\tlearn: 0.0008015\ttotal: 5.17s\tremaining: 44.5s\n",
            "104:\tlearn: 0.0007600\ttotal: 5.21s\tremaining: 44.5s\n",
            "105:\tlearn: 0.0007193\ttotal: 5.27s\tremaining: 44.4s\n",
            "106:\tlearn: 0.0006886\ttotal: 5.32s\tremaining: 44.4s\n",
            "107:\tlearn: 0.0006578\ttotal: 5.38s\tremaining: 44.5s\n",
            "108:\tlearn: 0.0006251\ttotal: 5.43s\tremaining: 44.4s\n",
            "109:\tlearn: 0.0005946\ttotal: 5.48s\tremaining: 44.4s\n",
            "110:\tlearn: 0.0005686\ttotal: 5.53s\tremaining: 44.3s\n",
            "111:\tlearn: 0.0005441\ttotal: 5.59s\tremaining: 44.3s\n",
            "112:\tlearn: 0.0005206\ttotal: 5.66s\tremaining: 44.4s\n",
            "113:\tlearn: 0.0005162\ttotal: 5.69s\tremaining: 44.2s\n",
            "114:\tlearn: 0.0004933\ttotal: 5.73s\tremaining: 44.1s\n",
            "115:\tlearn: 0.0004721\ttotal: 5.78s\tremaining: 44s\n",
            "116:\tlearn: 0.0004549\ttotal: 5.84s\tremaining: 44.1s\n",
            "117:\tlearn: 0.0004513\ttotal: 5.87s\tremaining: 43.9s\n",
            "118:\tlearn: 0.0004361\ttotal: 5.91s\tremaining: 43.8s\n",
            "119:\tlearn: 0.0004205\ttotal: 5.95s\tremaining: 43.7s\n",
            "120:\tlearn: 0.0004033\ttotal: 6s\tremaining: 43.6s\n",
            "121:\tlearn: 0.0003881\ttotal: 6.05s\tremaining: 43.6s\n",
            "122:\tlearn: 0.0003721\ttotal: 6.11s\tremaining: 43.6s\n",
            "123:\tlearn: 0.0003600\ttotal: 6.16s\tremaining: 43.5s\n",
            "124:\tlearn: 0.0003482\ttotal: 6.2s\tremaining: 43.4s\n",
            "125:\tlearn: 0.0003336\ttotal: 6.24s\tremaining: 43.3s\n",
            "126:\tlearn: 0.0003229\ttotal: 6.3s\tremaining: 43.3s\n",
            "127:\tlearn: 0.0003110\ttotal: 6.35s\tremaining: 43.3s\n",
            "128:\tlearn: 0.0003088\ttotal: 6.38s\tremaining: 43.1s\n",
            "129:\tlearn: 0.0002951\ttotal: 6.44s\tremaining: 43.1s\n",
            "130:\tlearn: 0.0002863\ttotal: 6.49s\tremaining: 43.1s\n",
            "131:\tlearn: 0.0002771\ttotal: 6.55s\tremaining: 43.1s\n",
            "132:\tlearn: 0.0002753\ttotal: 6.58s\tremaining: 42.9s\n",
            "133:\tlearn: 0.0002665\ttotal: 6.63s\tremaining: 42.9s\n",
            "134:\tlearn: 0.0002639\ttotal: 6.66s\tremaining: 42.6s\n",
            "135:\tlearn: 0.0002623\ttotal: 6.68s\tremaining: 42.5s\n",
            "136:\tlearn: 0.0002529\ttotal: 6.73s\tremaining: 42.4s\n",
            "137:\tlearn: 0.0002459\ttotal: 6.78s\tremaining: 42.4s\n",
            "138:\tlearn: 0.0002380\ttotal: 6.84s\tremaining: 42.4s\n",
            "139:\tlearn: 0.0002366\ttotal: 6.88s\tremaining: 42.2s\n",
            "140:\tlearn: 0.0002296\ttotal: 6.93s\tremaining: 42.2s\n",
            "141:\tlearn: 0.0002216\ttotal: 6.99s\tremaining: 42.2s\n",
            "142:\tlearn: 0.0002154\ttotal: 7.05s\tremaining: 42.2s\n",
            "143:\tlearn: 0.0002102\ttotal: 7.09s\tremaining: 42.2s\n",
            "144:\tlearn: 0.0002091\ttotal: 7.12s\tremaining: 42s\n",
            "145:\tlearn: 0.0002073\ttotal: 7.16s\tremaining: 41.9s\n",
            "146:\tlearn: 0.0002003\ttotal: 7.21s\tremaining: 41.8s\n",
            "147:\tlearn: 0.0001993\ttotal: 7.24s\tremaining: 41.7s\n",
            "148:\tlearn: 0.0001928\ttotal: 7.29s\tremaining: 41.6s\n",
            "149:\tlearn: 0.0001884\ttotal: 7.33s\tremaining: 41.6s\n",
            "150:\tlearn: 0.0001874\ttotal: 7.36s\tremaining: 41.4s\n",
            "151:\tlearn: 0.0001860\ttotal: 7.38s\tremaining: 41.2s\n",
            "152:\tlearn: 0.0001802\ttotal: 7.43s\tremaining: 41.1s\n",
            "153:\tlearn: 0.0001763\ttotal: 7.47s\tremaining: 41s\n",
            "154:\tlearn: 0.0001754\ttotal: 7.5s\tremaining: 40.9s\n",
            "155:\tlearn: 0.0001698\ttotal: 7.55s\tremaining: 40.9s\n",
            "156:\tlearn: 0.0001690\ttotal: 7.58s\tremaining: 40.7s\n",
            "157:\tlearn: 0.0001655\ttotal: 7.64s\tremaining: 40.7s\n",
            "158:\tlearn: 0.0001643\ttotal: 7.67s\tremaining: 40.5s\n",
            "159:\tlearn: 0.0001595\ttotal: 7.72s\tremaining: 40.5s\n",
            "160:\tlearn: 0.0001588\ttotal: 7.75s\tremaining: 40.4s\n",
            "161:\tlearn: 0.0001556\ttotal: 7.79s\tremaining: 40.3s\n",
            "162:\tlearn: 0.0001545\ttotal: 7.82s\tremaining: 40.2s\n",
            "163:\tlearn: 0.0001502\ttotal: 7.87s\tremaining: 40.1s\n",
            "164:\tlearn: 0.0001495\ttotal: 7.89s\tremaining: 39.9s\n",
            "165:\tlearn: 0.0001490\ttotal: 7.92s\tremaining: 39.8s\n",
            "166:\tlearn: 0.0001449\ttotal: 7.96s\tremaining: 39.7s\n",
            "167:\tlearn: 0.0001422\ttotal: 8.01s\tremaining: 39.7s\n",
            "168:\tlearn: 0.0001416\ttotal: 8.04s\tremaining: 39.6s\n",
            "169:\tlearn: 0.0001411\ttotal: 8.07s\tremaining: 39.4s\n",
            "170:\tlearn: 0.0001370\ttotal: 8.13s\tremaining: 39.4s\n",
            "171:\tlearn: 0.0001345\ttotal: 8.17s\tremaining: 39.4s\n",
            "172:\tlearn: 0.0001310\ttotal: 8.22s\tremaining: 39.3s\n",
            "173:\tlearn: 0.0001302\ttotal: 8.26s\tremaining: 39.2s\n",
            "174:\tlearn: 0.0001297\ttotal: 8.28s\tremaining: 39.1s\n",
            "175:\tlearn: 0.0001262\ttotal: 8.35s\tremaining: 39.1s\n",
            "176:\tlearn: 0.0001239\ttotal: 8.39s\tremaining: 39s\n",
            "177:\tlearn: 0.0001235\ttotal: 8.42s\tremaining: 38.9s\n",
            "178:\tlearn: 0.0001214\ttotal: 8.46s\tremaining: 38.8s\n",
            "179:\tlearn: 0.0001210\ttotal: 8.49s\tremaining: 38.7s\n",
            "180:\tlearn: 0.0001203\ttotal: 8.52s\tremaining: 38.6s\n",
            "181:\tlearn: 0.0001174\ttotal: 8.58s\tremaining: 38.6s\n",
            "182:\tlearn: 0.0001155\ttotal: 8.63s\tremaining: 38.5s\n",
            "183:\tlearn: 0.0001148\ttotal: 8.65s\tremaining: 38.4s\n",
            "184:\tlearn: 0.0001121\ttotal: 8.7s\tremaining: 38.3s\n",
            "185:\tlearn: 0.0001118\ttotal: 8.72s\tremaining: 38.2s\n",
            "186:\tlearn: 0.0001101\ttotal: 8.77s\tremaining: 38.1s\n",
            "187:\tlearn: 0.0001072\ttotal: 8.81s\tremaining: 38.1s\n",
            "188:\tlearn: 0.0001056\ttotal: 8.86s\tremaining: 38s\n",
            "189:\tlearn: 0.0001030\ttotal: 8.89s\tremaining: 37.9s\n",
            "190:\tlearn: 0.0001011\ttotal: 8.93s\tremaining: 37.8s\n",
            "191:\tlearn: 0.0001006\ttotal: 8.96s\tremaining: 37.7s\n",
            "192:\tlearn: 0.0000992\ttotal: 9s\tremaining: 37.6s\n",
            "193:\tlearn: 0.0000971\ttotal: 9.04s\tremaining: 37.6s\n",
            "194:\tlearn: 0.0000951\ttotal: 9.09s\tremaining: 37.5s\n",
            "195:\tlearn: 0.0000929\ttotal: 9.13s\tremaining: 37.5s\n",
            "196:\tlearn: 0.0000916\ttotal: 9.18s\tremaining: 37.4s\n",
            "197:\tlearn: 0.0000904\ttotal: 9.23s\tremaining: 37.4s\n",
            "198:\tlearn: 0.0000901\ttotal: 9.26s\tremaining: 37.3s\n",
            "199:\tlearn: 0.0000898\ttotal: 9.29s\tremaining: 37.2s\n",
            "200:\tlearn: 0.0000878\ttotal: 9.33s\tremaining: 37.1s\n",
            "201:\tlearn: 0.0000864\ttotal: 9.38s\tremaining: 37.1s\n",
            "202:\tlearn: 0.0000862\ttotal: 9.4s\tremaining: 36.9s\n",
            "203:\tlearn: 0.0000851\ttotal: 9.45s\tremaining: 36.9s\n",
            "204:\tlearn: 0.0000847\ttotal: 9.48s\tremaining: 36.8s\n",
            "205:\tlearn: 0.0000845\ttotal: 9.51s\tremaining: 36.7s\n",
            "206:\tlearn: 0.0000829\ttotal: 9.56s\tremaining: 36.6s\n",
            "207:\tlearn: 0.0000827\ttotal: 9.58s\tremaining: 36.5s\n",
            "208:\tlearn: 0.0000824\ttotal: 9.61s\tremaining: 36.4s\n",
            "209:\tlearn: 0.0000822\ttotal: 9.64s\tremaining: 36.3s\n",
            "210:\tlearn: 0.0000807\ttotal: 9.7s\tremaining: 36.3s\n",
            "211:\tlearn: 0.0000805\ttotal: 9.73s\tremaining: 36.2s\n",
            "212:\tlearn: 0.0000795\ttotal: 9.78s\tremaining: 36.1s\n",
            "213:\tlearn: 0.0000786\ttotal: 9.84s\tremaining: 36.1s\n",
            "214:\tlearn: 0.0000784\ttotal: 9.86s\tremaining: 36s\n",
            "215:\tlearn: 0.0000781\ttotal: 9.89s\tremaining: 35.9s\n",
            "216:\tlearn: 0.0000777\ttotal: 9.92s\tremaining: 35.8s\n",
            "217:\tlearn: 0.0000763\ttotal: 9.97s\tremaining: 35.8s\n",
            "218:\tlearn: 0.0000750\ttotal: 10s\tremaining: 35.7s\n",
            "219:\tlearn: 0.0000747\ttotal: 10s\tremaining: 35.6s\n",
            "220:\tlearn: 0.0000732\ttotal: 10.1s\tremaining: 35.6s\n",
            "221:\tlearn: 0.0000730\ttotal: 10.1s\tremaining: 35.5s\n",
            "222:\tlearn: 0.0000722\ttotal: 10.2s\tremaining: 35.4s\n",
            "223:\tlearn: 0.0000714\ttotal: 10.2s\tremaining: 35.4s\n",
            "224:\tlearn: 0.0000702\ttotal: 10.3s\tremaining: 35.3s\n",
            "225:\tlearn: 0.0000694\ttotal: 10.3s\tremaining: 35.3s\n",
            "226:\tlearn: 0.0000692\ttotal: 10.3s\tremaining: 35.1s\n",
            "227:\tlearn: 0.0000690\ttotal: 10.3s\tremaining: 35s\n",
            "228:\tlearn: 0.0000677\ttotal: 10.4s\tremaining: 35s\n",
            "229:\tlearn: 0.0000670\ttotal: 10.5s\tremaining: 35s\n",
            "230:\tlearn: 0.0000668\ttotal: 10.5s\tremaining: 34.9s\n",
            "231:\tlearn: 0.0000667\ttotal: 10.5s\tremaining: 34.8s\n",
            "232:\tlearn: 0.0000665\ttotal: 10.5s\tremaining: 34.7s\n",
            "233:\tlearn: 0.0000655\ttotal: 10.6s\tremaining: 34.7s\n",
            "234:\tlearn: 0.0000648\ttotal: 10.6s\tremaining: 34.7s\n",
            "235:\tlearn: 0.0000636\ttotal: 10.7s\tremaining: 34.6s\n",
            "236:\tlearn: 0.0000630\ttotal: 10.8s\tremaining: 34.6s\n",
            "237:\tlearn: 0.0000628\ttotal: 10.8s\tremaining: 34.5s\n",
            "238:\tlearn: 0.0000616\ttotal: 10.8s\tremaining: 34.5s\n",
            "239:\tlearn: 0.0000615\ttotal: 10.9s\tremaining: 34.4s\n",
            "240:\tlearn: 0.0000608\ttotal: 10.9s\tremaining: 34.4s\n",
            "241:\tlearn: 0.0000602\ttotal: 11s\tremaining: 34.3s\n",
            "242:\tlearn: 0.0000601\ttotal: 11s\tremaining: 34.2s\n",
            "243:\tlearn: 0.0000592\ttotal: 11s\tremaining: 34.2s\n",
            "244:\tlearn: 0.0000586\ttotal: 11.1s\tremaining: 34.1s\n",
            "245:\tlearn: 0.0000585\ttotal: 11.1s\tremaining: 34s\n",
            "246:\tlearn: 0.0000577\ttotal: 11.2s\tremaining: 34s\n",
            "247:\tlearn: 0.0000572\ttotal: 11.2s\tremaining: 34s\n",
            "248:\tlearn: 0.0000571\ttotal: 11.2s\tremaining: 33.9s\n",
            "249:\tlearn: 0.0000569\ttotal: 11.3s\tremaining: 33.8s\n",
            "250:\tlearn: 0.0000568\ttotal: 11.3s\tremaining: 33.7s\n",
            "251:\tlearn: 0.0000566\ttotal: 11.3s\tremaining: 33.6s\n",
            "252:\tlearn: 0.0000558\ttotal: 11.4s\tremaining: 33.7s\n",
            "253:\tlearn: 0.0000557\ttotal: 11.4s\tremaining: 33.6s\n",
            "254:\tlearn: 0.0000552\ttotal: 11.5s\tremaining: 33.6s\n",
            "255:\tlearn: 0.0000551\ttotal: 11.5s\tremaining: 33.5s\n",
            "256:\tlearn: 0.0000543\ttotal: 11.6s\tremaining: 33.5s\n",
            "257:\tlearn: 0.0000539\ttotal: 11.7s\tremaining: 33.5s\n",
            "258:\tlearn: 0.0000537\ttotal: 11.7s\tremaining: 33.4s\n",
            "259:\tlearn: 0.0000535\ttotal: 11.7s\tremaining: 33.3s\n",
            "260:\tlearn: 0.0000528\ttotal: 11.8s\tremaining: 33.3s\n",
            "261:\tlearn: 0.0000527\ttotal: 11.8s\tremaining: 33.2s\n",
            "262:\tlearn: 0.0000519\ttotal: 11.8s\tremaining: 33.1s\n",
            "263:\tlearn: 0.0000514\ttotal: 11.9s\tremaining: 33.1s\n",
            "264:\tlearn: 0.0000506\ttotal: 11.9s\tremaining: 33.1s\n",
            "265:\tlearn: 0.0000502\ttotal: 12s\tremaining: 33s\n",
            "266:\tlearn: 0.0000498\ttotal: 12s\tremaining: 33s\n",
            "267:\tlearn: 0.0000497\ttotal: 12s\tremaining: 32.9s\n",
            "268:\tlearn: 0.0000491\ttotal: 12.1s\tremaining: 32.9s\n",
            "269:\tlearn: 0.0000490\ttotal: 12.1s\tremaining: 32.8s\n",
            "270:\tlearn: 0.0000486\ttotal: 12.2s\tremaining: 32.7s\n",
            "271:\tlearn: 0.0000485\ttotal: 12.2s\tremaining: 32.7s\n",
            "272:\tlearn: 0.0000479\ttotal: 12.3s\tremaining: 32.6s\n",
            "273:\tlearn: 0.0000478\ttotal: 12.3s\tremaining: 32.6s\n",
            "274:\tlearn: 0.0000474\ttotal: 12.3s\tremaining: 32.5s\n",
            "275:\tlearn: 0.0000468\ttotal: 12.4s\tremaining: 32.5s\n",
            "276:\tlearn: 0.0000461\ttotal: 12.4s\tremaining: 32.5s\n",
            "277:\tlearn: 0.0000457\ttotal: 12.5s\tremaining: 32.4s\n",
            "278:\tlearn: 0.0000454\ttotal: 12.5s\tremaining: 32.4s\n",
            "279:\tlearn: 0.0000453\ttotal: 12.6s\tremaining: 32.3s\n",
            "280:\tlearn: 0.0000452\ttotal: 12.6s\tremaining: 32.2s\n",
            "281:\tlearn: 0.0000451\ttotal: 12.6s\tremaining: 32.1s\n",
            "282:\tlearn: 0.0000447\ttotal: 12.7s\tremaining: 32.1s\n",
            "283:\tlearn: 0.0000446\ttotal: 12.7s\tremaining: 32s\n",
            "284:\tlearn: 0.0000442\ttotal: 12.7s\tremaining: 32s\n",
            "285:\tlearn: 0.0000437\ttotal: 12.8s\tremaining: 31.9s\n",
            "286:\tlearn: 0.0000433\ttotal: 12.8s\tremaining: 31.9s\n",
            "287:\tlearn: 0.0000432\ttotal: 12.9s\tremaining: 31.8s\n",
            "288:\tlearn: 0.0000432\ttotal: 12.9s\tremaining: 31.7s\n",
            "289:\tlearn: 0.0000431\ttotal: 12.9s\tremaining: 31.7s\n",
            "290:\tlearn: 0.0000430\ttotal: 13s\tremaining: 31.6s\n",
            "291:\tlearn: 0.0000430\ttotal: 13s\tremaining: 31.5s\n",
            "292:\tlearn: 0.0000426\ttotal: 13s\tremaining: 31.4s\n",
            "293:\tlearn: 0.0000425\ttotal: 13.1s\tremaining: 31.4s\n",
            "294:\tlearn: 0.0000422\ttotal: 13.1s\tremaining: 31.4s\n",
            "295:\tlearn: 0.0000421\ttotal: 13.1s\tremaining: 31.3s\n",
            "296:\tlearn: 0.0000416\ttotal: 13.2s\tremaining: 31.2s\n",
            "297:\tlearn: 0.0000412\ttotal: 13.2s\tremaining: 31.2s\n",
            "298:\tlearn: 0.0000408\ttotal: 13.3s\tremaining: 31.2s\n",
            "299:\tlearn: 0.0000405\ttotal: 13.4s\tremaining: 31.2s\n",
            "300:\tlearn: 0.0000404\ttotal: 13.4s\tremaining: 31.1s\n",
            "301:\tlearn: 0.0000400\ttotal: 13.5s\tremaining: 31.1s\n",
            "302:\tlearn: 0.0000399\ttotal: 13.5s\tremaining: 31s\n",
            "303:\tlearn: 0.0000396\ttotal: 13.5s\tremaining: 31s\n",
            "304:\tlearn: 0.0000396\ttotal: 13.6s\tremaining: 30.9s\n",
            "305:\tlearn: 0.0000395\ttotal: 13.6s\tremaining: 30.8s\n",
            "306:\tlearn: 0.0000394\ttotal: 13.6s\tremaining: 30.7s\n",
            "307:\tlearn: 0.0000394\ttotal: 13.6s\tremaining: 30.6s\n",
            "308:\tlearn: 0.0000390\ttotal: 13.7s\tremaining: 30.6s\n",
            "309:\tlearn: 0.0000389\ttotal: 13.7s\tremaining: 30.5s\n",
            "310:\tlearn: 0.0000387\ttotal: 13.8s\tremaining: 30.5s\n",
            "311:\tlearn: 0.0000382\ttotal: 13.8s\tremaining: 30.4s\n",
            "312:\tlearn: 0.0000377\ttotal: 13.9s\tremaining: 30.4s\n",
            "313:\tlearn: 0.0000375\ttotal: 13.9s\tremaining: 30.4s\n",
            "314:\tlearn: 0.0000372\ttotal: 13.9s\tremaining: 30.3s\n",
            "315:\tlearn: 0.0000372\ttotal: 14s\tremaining: 30.2s\n",
            "316:\tlearn: 0.0000370\ttotal: 14s\tremaining: 30.2s\n",
            "317:\tlearn: 0.0000365\ttotal: 14.1s\tremaining: 30.2s\n",
            "318:\tlearn: 0.0000363\ttotal: 14.1s\tremaining: 30.2s\n",
            "319:\tlearn: 0.0000359\ttotal: 14.2s\tremaining: 30.1s\n",
            "320:\tlearn: 0.0000359\ttotal: 14.2s\tremaining: 30.1s\n",
            "321:\tlearn: 0.0000358\ttotal: 14.2s\tremaining: 30s\n",
            "322:\tlearn: 0.0000356\ttotal: 14.3s\tremaining: 29.9s\n",
            "323:\tlearn: 0.0000355\ttotal: 14.3s\tremaining: 29.9s\n",
            "324:\tlearn: 0.0000352\ttotal: 14.4s\tremaining: 29.9s\n",
            "325:\tlearn: 0.0000351\ttotal: 14.4s\tremaining: 29.8s\n",
            "326:\tlearn: 0.0000351\ttotal: 14.4s\tremaining: 29.7s\n",
            "327:\tlearn: 0.0000347\ttotal: 14.5s\tremaining: 29.7s\n",
            "328:\tlearn: 0.0000343\ttotal: 14.6s\tremaining: 29.7s\n",
            "329:\tlearn: 0.0000341\ttotal: 14.6s\tremaining: 29.7s\n",
            "330:\tlearn: 0.0000340\ttotal: 14.6s\tremaining: 29.6s\n",
            "331:\tlearn: 0.0000340\ttotal: 14.7s\tremaining: 29.6s\n",
            "332:\tlearn: 0.0000338\ttotal: 14.8s\tremaining: 29.5s\n",
            "333:\tlearn: 0.0000335\ttotal: 14.8s\tremaining: 29.6s\n",
            "334:\tlearn: 0.0000335\ttotal: 14.9s\tremaining: 29.5s\n",
            "335:\tlearn: 0.0000335\ttotal: 14.9s\tremaining: 29.4s\n",
            "336:\tlearn: 0.0000334\ttotal: 14.9s\tremaining: 29.3s\n",
            "337:\tlearn: 0.0000334\ttotal: 14.9s\tremaining: 29.3s\n",
            "338:\tlearn: 0.0000332\ttotal: 15s\tremaining: 29.3s\n",
            "339:\tlearn: 0.0000329\ttotal: 15.1s\tremaining: 29.2s\n",
            "340:\tlearn: 0.0000329\ttotal: 15.1s\tremaining: 29.2s\n",
            "341:\tlearn: 0.0000327\ttotal: 15.1s\tremaining: 29.1s\n",
            "342:\tlearn: 0.0000326\ttotal: 15.2s\tremaining: 29s\n",
            "343:\tlearn: 0.0000326\ttotal: 15.2s\tremaining: 29s\n",
            "344:\tlearn: 0.0000323\ttotal: 15.2s\tremaining: 28.9s\n",
            "345:\tlearn: 0.0000323\ttotal: 15.3s\tremaining: 28.9s\n",
            "346:\tlearn: 0.0000320\ttotal: 15.3s\tremaining: 28.8s\n",
            "347:\tlearn: 0.0000318\ttotal: 15.4s\tremaining: 28.8s\n",
            "348:\tlearn: 0.0000317\ttotal: 15.4s\tremaining: 28.7s\n",
            "349:\tlearn: 0.0000313\ttotal: 15.4s\tremaining: 28.7s\n",
            "350:\tlearn: 0.0000313\ttotal: 15.5s\tremaining: 28.6s\n",
            "351:\tlearn: 0.0000312\ttotal: 15.5s\tremaining: 28.5s\n",
            "352:\tlearn: 0.0000312\ttotal: 15.5s\tremaining: 28.5s\n",
            "353:\tlearn: 0.0000309\ttotal: 15.6s\tremaining: 28.4s\n",
            "354:\tlearn: 0.0000307\ttotal: 15.6s\tremaining: 28.4s\n",
            "355:\tlearn: 0.0000306\ttotal: 15.6s\tremaining: 28.3s\n",
            "356:\tlearn: 0.0000305\ttotal: 15.7s\tremaining: 28.3s\n",
            "357:\tlearn: 0.0000303\ttotal: 15.7s\tremaining: 28.2s\n",
            "358:\tlearn: 0.0000303\ttotal: 15.8s\tremaining: 28.1s\n",
            "359:\tlearn: 0.0000302\ttotal: 15.8s\tremaining: 28.1s\n",
            "360:\tlearn: 0.0000302\ttotal: 15.8s\tremaining: 28s\n",
            "361:\tlearn: 0.0000302\ttotal: 15.8s\tremaining: 27.9s\n",
            "362:\tlearn: 0.0000299\ttotal: 15.9s\tremaining: 27.9s\n",
            "363:\tlearn: 0.0000296\ttotal: 15.9s\tremaining: 27.8s\n",
            "364:\tlearn: 0.0000295\ttotal: 16s\tremaining: 27.8s\n",
            "365:\tlearn: 0.0000292\ttotal: 16s\tremaining: 27.8s\n",
            "366:\tlearn: 0.0000290\ttotal: 16.1s\tremaining: 27.7s\n",
            "367:\tlearn: 0.0000289\ttotal: 16.1s\tremaining: 27.7s\n",
            "368:\tlearn: 0.0000287\ttotal: 16.2s\tremaining: 27.7s\n",
            "369:\tlearn: 0.0000287\ttotal: 16.2s\tremaining: 27.6s\n",
            "370:\tlearn: 0.0000286\ttotal: 16.2s\tremaining: 27.5s\n",
            "371:\tlearn: 0.0000285\ttotal: 16.3s\tremaining: 27.5s\n",
            "372:\tlearn: 0.0000282\ttotal: 16.3s\tremaining: 27.4s\n",
            "373:\tlearn: 0.0000281\ttotal: 16.4s\tremaining: 27.4s\n",
            "374:\tlearn: 0.0000278\ttotal: 16.4s\tremaining: 27.4s\n",
            "375:\tlearn: 0.0000278\ttotal: 16.5s\tremaining: 27.3s\n",
            "376:\tlearn: 0.0000278\ttotal: 16.5s\tremaining: 27.2s\n",
            "377:\tlearn: 0.0000276\ttotal: 16.5s\tremaining: 27.2s\n",
            "378:\tlearn: 0.0000274\ttotal: 16.6s\tremaining: 27.2s\n",
            "379:\tlearn: 0.0000274\ttotal: 16.6s\tremaining: 27.1s\n",
            "380:\tlearn: 0.0000272\ttotal: 16.7s\tremaining: 27.1s\n",
            "381:\tlearn: 0.0000269\ttotal: 16.7s\tremaining: 27.1s\n",
            "382:\tlearn: 0.0000268\ttotal: 16.8s\tremaining: 27s\n",
            "383:\tlearn: 0.0000268\ttotal: 16.8s\tremaining: 26.9s\n",
            "384:\tlearn: 0.0000266\ttotal: 16.8s\tremaining: 26.9s\n",
            "385:\tlearn: 0.0000266\ttotal: 16.9s\tremaining: 26.8s\n",
            "386:\tlearn: 0.0000266\ttotal: 16.9s\tremaining: 26.8s\n",
            "387:\tlearn: 0.0000264\ttotal: 16.9s\tremaining: 26.7s\n",
            "388:\tlearn: 0.0000264\ttotal: 17s\tremaining: 26.7s\n",
            "389:\tlearn: 0.0000263\ttotal: 17s\tremaining: 26.6s\n",
            "390:\tlearn: 0.0000262\ttotal: 17.1s\tremaining: 26.6s\n",
            "391:\tlearn: 0.0000260\ttotal: 17.1s\tremaining: 26.6s\n",
            "392:\tlearn: 0.0000260\ttotal: 17.2s\tremaining: 26.5s\n",
            "393:\tlearn: 0.0000259\ttotal: 17.2s\tremaining: 26.5s\n",
            "394:\tlearn: 0.0000258\ttotal: 17.2s\tremaining: 26.4s\n",
            "395:\tlearn: 0.0000257\ttotal: 17.3s\tremaining: 26.4s\n",
            "396:\tlearn: 0.0000255\ttotal: 17.3s\tremaining: 26.4s\n",
            "397:\tlearn: 0.0000254\ttotal: 17.4s\tremaining: 26.3s\n",
            "398:\tlearn: 0.0000253\ttotal: 17.4s\tremaining: 26.2s\n",
            "399:\tlearn: 0.0000253\ttotal: 17.4s\tremaining: 26.2s\n",
            "400:\tlearn: 0.0000253\ttotal: 17.5s\tremaining: 26.1s\n",
            "401:\tlearn: 0.0000251\ttotal: 17.5s\tremaining: 26.1s\n",
            "402:\tlearn: 0.0000251\ttotal: 17.5s\tremaining: 26s\n",
            "403:\tlearn: 0.0000249\ttotal: 17.6s\tremaining: 26s\n",
            "404:\tlearn: 0.0000247\ttotal: 17.6s\tremaining: 25.9s\n",
            "405:\tlearn: 0.0000246\ttotal: 17.7s\tremaining: 25.9s\n",
            "406:\tlearn: 0.0000246\ttotal: 17.7s\tremaining: 25.8s\n",
            "407:\tlearn: 0.0000246\ttotal: 17.7s\tremaining: 25.7s\n",
            "408:\tlearn: 0.0000244\ttotal: 17.8s\tremaining: 25.7s\n",
            "409:\tlearn: 0.0000244\ttotal: 17.8s\tremaining: 25.6s\n",
            "410:\tlearn: 0.0000243\ttotal: 17.9s\tremaining: 25.6s\n",
            "411:\tlearn: 0.0000243\ttotal: 17.9s\tremaining: 25.5s\n",
            "412:\tlearn: 0.0000242\ttotal: 17.9s\tremaining: 25.5s\n",
            "413:\tlearn: 0.0000242\ttotal: 17.9s\tremaining: 25.4s\n",
            "414:\tlearn: 0.0000241\ttotal: 18s\tremaining: 25.3s\n",
            "415:\tlearn: 0.0000241\ttotal: 18s\tremaining: 25.3s\n",
            "416:\tlearn: 0.0000240\ttotal: 18.1s\tremaining: 25.2s\n",
            "417:\tlearn: 0.0000240\ttotal: 18.1s\tremaining: 25.2s\n",
            "418:\tlearn: 0.0000240\ttotal: 18.1s\tremaining: 25.1s\n",
            "419:\tlearn: 0.0000238\ttotal: 18.2s\tremaining: 25.1s\n",
            "420:\tlearn: 0.0000236\ttotal: 18.2s\tremaining: 25s\n",
            "421:\tlearn: 0.0000236\ttotal: 18.2s\tremaining: 25s\n",
            "422:\tlearn: 0.0000235\ttotal: 18.3s\tremaining: 24.9s\n",
            "423:\tlearn: 0.0000235\ttotal: 18.3s\tremaining: 24.9s\n",
            "424:\tlearn: 0.0000234\ttotal: 18.4s\tremaining: 24.8s\n",
            "425:\tlearn: 0.0000233\ttotal: 18.4s\tremaining: 24.8s\n",
            "426:\tlearn: 0.0000233\ttotal: 18.4s\tremaining: 24.8s\n",
            "427:\tlearn: 0.0000232\ttotal: 18.5s\tremaining: 24.7s\n",
            "428:\tlearn: 0.0000232\ttotal: 18.5s\tremaining: 24.7s\n",
            "429:\tlearn: 0.0000232\ttotal: 18.6s\tremaining: 24.6s\n",
            "430:\tlearn: 0.0000232\ttotal: 18.6s\tremaining: 24.5s\n",
            "431:\tlearn: 0.0000231\ttotal: 18.6s\tremaining: 24.5s\n",
            "432:\tlearn: 0.0000230\ttotal: 18.7s\tremaining: 24.5s\n",
            "433:\tlearn: 0.0000229\ttotal: 18.7s\tremaining: 24.4s\n",
            "434:\tlearn: 0.0000228\ttotal: 18.8s\tremaining: 24.4s\n",
            "435:\tlearn: 0.0000228\ttotal: 18.8s\tremaining: 24.3s\n",
            "436:\tlearn: 0.0000227\ttotal: 18.8s\tremaining: 24.3s\n",
            "437:\tlearn: 0.0000226\ttotal: 18.9s\tremaining: 24.2s\n",
            "438:\tlearn: 0.0000226\ttotal: 18.9s\tremaining: 24.1s\n",
            "439:\tlearn: 0.0000226\ttotal: 18.9s\tremaining: 24.1s\n",
            "440:\tlearn: 0.0000225\ttotal: 19s\tremaining: 24s\n",
            "441:\tlearn: 0.0000224\ttotal: 19s\tremaining: 24s\n",
            "442:\tlearn: 0.0000223\ttotal: 19s\tremaining: 23.9s\n",
            "443:\tlearn: 0.0000222\ttotal: 19.1s\tremaining: 23.9s\n",
            "444:\tlearn: 0.0000222\ttotal: 19.1s\tremaining: 23.9s\n",
            "445:\tlearn: 0.0000221\ttotal: 19.2s\tremaining: 23.8s\n",
            "446:\tlearn: 0.0000220\ttotal: 19.2s\tremaining: 23.8s\n",
            "447:\tlearn: 0.0000219\ttotal: 19.3s\tremaining: 23.7s\n",
            "448:\tlearn: 0.0000218\ttotal: 19.3s\tremaining: 23.7s\n",
            "449:\tlearn: 0.0000218\ttotal: 19.3s\tremaining: 23.6s\n",
            "450:\tlearn: 0.0000217\ttotal: 19.4s\tremaining: 23.6s\n",
            "451:\tlearn: 0.0000216\ttotal: 19.4s\tremaining: 23.5s\n",
            "452:\tlearn: 0.0000216\ttotal: 19.4s\tremaining: 23.5s\n",
            "453:\tlearn: 0.0000216\ttotal: 19.5s\tremaining: 23.4s\n",
            "454:\tlearn: 0.0000216\ttotal: 19.5s\tremaining: 23.4s\n",
            "455:\tlearn: 0.0000215\ttotal: 19.5s\tremaining: 23.3s\n",
            "456:\tlearn: 0.0000215\ttotal: 19.6s\tremaining: 23.3s\n",
            "457:\tlearn: 0.0000214\ttotal: 19.6s\tremaining: 23.2s\n",
            "458:\tlearn: 0.0000213\ttotal: 19.7s\tremaining: 23.2s\n",
            "459:\tlearn: 0.0000212\ttotal: 19.7s\tremaining: 23.1s\n",
            "460:\tlearn: 0.0000211\ttotal: 19.8s\tremaining: 23.1s\n",
            "461:\tlearn: 0.0000210\ttotal: 19.8s\tremaining: 23.1s\n",
            "462:\tlearn: 0.0000210\ttotal: 19.8s\tremaining: 23s\n",
            "463:\tlearn: 0.0000210\ttotal: 19.9s\tremaining: 22.9s\n",
            "464:\tlearn: 0.0000209\ttotal: 19.9s\tremaining: 22.9s\n",
            "465:\tlearn: 0.0000209\ttotal: 19.9s\tremaining: 22.8s\n",
            "466:\tlearn: 0.0000209\ttotal: 20s\tremaining: 22.8s\n",
            "467:\tlearn: 0.0000209\ttotal: 20s\tremaining: 22.7s\n",
            "468:\tlearn: 0.0000209\ttotal: 20s\tremaining: 22.7s\n",
            "469:\tlearn: 0.0000207\ttotal: 20.1s\tremaining: 22.6s\n",
            "470:\tlearn: 0.0000207\ttotal: 20.1s\tremaining: 22.6s\n",
            "471:\tlearn: 0.0000206\ttotal: 20.2s\tremaining: 22.5s\n",
            "472:\tlearn: 0.0000205\ttotal: 20.2s\tremaining: 22.5s\n",
            "473:\tlearn: 0.0000205\ttotal: 20.2s\tremaining: 22.5s\n",
            "474:\tlearn: 0.0000204\ttotal: 20.3s\tremaining: 22.4s\n",
            "475:\tlearn: 0.0000204\ttotal: 20.3s\tremaining: 22.3s\n",
            "476:\tlearn: 0.0000203\ttotal: 20.3s\tremaining: 22.3s\n",
            "477:\tlearn: 0.0000203\ttotal: 20.4s\tremaining: 22.2s\n",
            "478:\tlearn: 0.0000203\ttotal: 20.4s\tremaining: 22.2s\n",
            "479:\tlearn: 0.0000202\ttotal: 20.4s\tremaining: 22.1s\n",
            "480:\tlearn: 0.0000202\ttotal: 20.5s\tremaining: 22.1s\n",
            "481:\tlearn: 0.0000202\ttotal: 20.5s\tremaining: 22s\n",
            "482:\tlearn: 0.0000200\ttotal: 20.6s\tremaining: 22s\n",
            "483:\tlearn: 0.0000200\ttotal: 20.6s\tremaining: 22s\n",
            "484:\tlearn: 0.0000199\ttotal: 20.6s\tremaining: 21.9s\n",
            "485:\tlearn: 0.0000199\ttotal: 20.7s\tremaining: 21.9s\n",
            "486:\tlearn: 0.0000198\ttotal: 20.7s\tremaining: 21.8s\n",
            "487:\tlearn: 0.0000198\ttotal: 20.8s\tremaining: 21.8s\n",
            "488:\tlearn: 0.0000198\ttotal: 20.8s\tremaining: 21.7s\n",
            "489:\tlearn: 0.0000198\ttotal: 20.8s\tremaining: 21.7s\n",
            "490:\tlearn: 0.0000197\ttotal: 20.9s\tremaining: 21.6s\n",
            "491:\tlearn: 0.0000196\ttotal: 20.9s\tremaining: 21.6s\n",
            "492:\tlearn: 0.0000196\ttotal: 20.9s\tremaining: 21.5s\n",
            "493:\tlearn: 0.0000195\ttotal: 21s\tremaining: 21.5s\n",
            "494:\tlearn: 0.0000195\ttotal: 21s\tremaining: 21.4s\n",
            "495:\tlearn: 0.0000194\ttotal: 21.1s\tremaining: 21.4s\n",
            "496:\tlearn: 0.0000194\ttotal: 21.1s\tremaining: 21.4s\n",
            "497:\tlearn: 0.0000193\ttotal: 21.1s\tremaining: 21.3s\n",
            "498:\tlearn: 0.0000193\ttotal: 21.2s\tremaining: 21.2s\n",
            "499:\tlearn: 0.0000193\ttotal: 21.2s\tremaining: 21.2s\n",
            "500:\tlearn: 0.0000193\ttotal: 21.2s\tremaining: 21.1s\n",
            "501:\tlearn: 0.0000192\ttotal: 21.3s\tremaining: 21.1s\n",
            "502:\tlearn: 0.0000191\ttotal: 21.3s\tremaining: 21.1s\n",
            "503:\tlearn: 0.0000191\ttotal: 21.3s\tremaining: 21s\n",
            "504:\tlearn: 0.0000191\ttotal: 21.4s\tremaining: 20.9s\n",
            "505:\tlearn: 0.0000191\ttotal: 21.4s\tremaining: 20.9s\n",
            "506:\tlearn: 0.0000190\ttotal: 21.4s\tremaining: 20.9s\n",
            "507:\tlearn: 0.0000190\ttotal: 21.5s\tremaining: 20.8s\n",
            "508:\tlearn: 0.0000189\ttotal: 21.5s\tremaining: 20.7s\n",
            "509:\tlearn: 0.0000189\ttotal: 21.5s\tremaining: 20.7s\n",
            "510:\tlearn: 0.0000188\ttotal: 21.6s\tremaining: 20.6s\n",
            "511:\tlearn: 0.0000188\ttotal: 21.6s\tremaining: 20.6s\n",
            "512:\tlearn: 0.0000188\ttotal: 21.6s\tremaining: 20.5s\n",
            "513:\tlearn: 0.0000188\ttotal: 21.7s\tremaining: 20.5s\n",
            "514:\tlearn: 0.0000187\ttotal: 21.7s\tremaining: 20.4s\n",
            "515:\tlearn: 0.0000186\ttotal: 21.7s\tremaining: 20.4s\n",
            "516:\tlearn: 0.0000185\ttotal: 21.8s\tremaining: 20.3s\n",
            "517:\tlearn: 0.0000185\ttotal: 21.8s\tremaining: 20.3s\n",
            "518:\tlearn: 0.0000184\ttotal: 21.8s\tremaining: 20.2s\n",
            "519:\tlearn: 0.0000184\ttotal: 21.9s\tremaining: 20.2s\n",
            "520:\tlearn: 0.0000183\ttotal: 21.9s\tremaining: 20.1s\n",
            "521:\tlearn: 0.0000183\ttotal: 21.9s\tremaining: 20.1s\n",
            "522:\tlearn: 0.0000183\ttotal: 22s\tremaining: 20s\n",
            "523:\tlearn: 0.0000182\ttotal: 22s\tremaining: 20s\n",
            "524:\tlearn: 0.0000182\ttotal: 22s\tremaining: 19.9s\n",
            "525:\tlearn: 0.0000181\ttotal: 22.1s\tremaining: 19.9s\n",
            "526:\tlearn: 0.0000180\ttotal: 22.1s\tremaining: 19.8s\n",
            "527:\tlearn: 0.0000180\ttotal: 22.1s\tremaining: 19.8s\n",
            "528:\tlearn: 0.0000179\ttotal: 22.2s\tremaining: 19.8s\n",
            "529:\tlearn: 0.0000178\ttotal: 22.2s\tremaining: 19.7s\n",
            "530:\tlearn: 0.0000178\ttotal: 22.3s\tremaining: 19.7s\n",
            "531:\tlearn: 0.0000177\ttotal: 22.3s\tremaining: 19.6s\n",
            "532:\tlearn: 0.0000176\ttotal: 22.3s\tremaining: 19.6s\n",
            "533:\tlearn: 0.0000176\ttotal: 22.4s\tremaining: 19.5s\n",
            "534:\tlearn: 0.0000176\ttotal: 22.4s\tremaining: 19.5s\n",
            "535:\tlearn: 0.0000176\ttotal: 22.4s\tremaining: 19.4s\n",
            "536:\tlearn: 0.0000176\ttotal: 22.5s\tremaining: 19.4s\n",
            "537:\tlearn: 0.0000175\ttotal: 22.5s\tremaining: 19.3s\n",
            "538:\tlearn: 0.0000175\ttotal: 22.6s\tremaining: 19.3s\n",
            "539:\tlearn: 0.0000175\ttotal: 22.6s\tremaining: 19.2s\n",
            "540:\tlearn: 0.0000175\ttotal: 22.6s\tremaining: 19.2s\n",
            "541:\tlearn: 0.0000174\ttotal: 22.7s\tremaining: 19.1s\n",
            "542:\tlearn: 0.0000174\ttotal: 22.7s\tremaining: 19.1s\n",
            "543:\tlearn: 0.0000173\ttotal: 22.8s\tremaining: 19.1s\n",
            "544:\tlearn: 0.0000173\ttotal: 22.8s\tremaining: 19s\n",
            "545:\tlearn: 0.0000172\ttotal: 22.8s\tremaining: 19s\n",
            "546:\tlearn: 0.0000172\ttotal: 22.9s\tremaining: 18.9s\n",
            "547:\tlearn: 0.0000172\ttotal: 22.9s\tremaining: 18.9s\n",
            "548:\tlearn: 0.0000172\ttotal: 22.9s\tremaining: 18.8s\n",
            "549:\tlearn: 0.0000172\ttotal: 23s\tremaining: 18.8s\n",
            "550:\tlearn: 0.0000171\ttotal: 23s\tremaining: 18.8s\n",
            "551:\tlearn: 0.0000171\ttotal: 23.1s\tremaining: 18.7s\n",
            "552:\tlearn: 0.0000170\ttotal: 23.1s\tremaining: 18.7s\n",
            "553:\tlearn: 0.0000170\ttotal: 23.1s\tremaining: 18.6s\n",
            "554:\tlearn: 0.0000169\ttotal: 23.2s\tremaining: 18.6s\n",
            "555:\tlearn: 0.0000168\ttotal: 23.2s\tremaining: 18.6s\n",
            "556:\tlearn: 0.0000168\ttotal: 23.3s\tremaining: 18.5s\n",
            "557:\tlearn: 0.0000168\ttotal: 23.3s\tremaining: 18.5s\n",
            "558:\tlearn: 0.0000167\ttotal: 23.4s\tremaining: 18.4s\n",
            "559:\tlearn: 0.0000167\ttotal: 23.4s\tremaining: 18.4s\n",
            "560:\tlearn: 0.0000167\ttotal: 23.4s\tremaining: 18.3s\n",
            "561:\tlearn: 0.0000167\ttotal: 23.5s\tremaining: 18.3s\n",
            "562:\tlearn: 0.0000166\ttotal: 23.5s\tremaining: 18.2s\n",
            "563:\tlearn: 0.0000166\ttotal: 23.5s\tremaining: 18.2s\n",
            "564:\tlearn: 0.0000165\ttotal: 23.6s\tremaining: 18.2s\n",
            "565:\tlearn: 0.0000165\ttotal: 23.6s\tremaining: 18.1s\n",
            "566:\tlearn: 0.0000165\ttotal: 23.7s\tremaining: 18.1s\n",
            "567:\tlearn: 0.0000165\ttotal: 23.7s\tremaining: 18s\n",
            "568:\tlearn: 0.0000164\ttotal: 23.7s\tremaining: 18s\n",
            "569:\tlearn: 0.0000164\ttotal: 23.8s\tremaining: 17.9s\n",
            "570:\tlearn: 0.0000163\ttotal: 23.8s\tremaining: 17.9s\n",
            "571:\tlearn: 0.0000162\ttotal: 23.8s\tremaining: 17.8s\n",
            "572:\tlearn: 0.0000162\ttotal: 23.9s\tremaining: 17.8s\n",
            "573:\tlearn: 0.0000161\ttotal: 23.9s\tremaining: 17.8s\n",
            "574:\tlearn: 0.0000160\ttotal: 24s\tremaining: 17.7s\n",
            "575:\tlearn: 0.0000160\ttotal: 24s\tremaining: 17.7s\n",
            "576:\tlearn: 0.0000160\ttotal: 24s\tremaining: 17.6s\n",
            "577:\tlearn: 0.0000160\ttotal: 24.1s\tremaining: 17.6s\n",
            "578:\tlearn: 0.0000160\ttotal: 24.1s\tremaining: 17.5s\n",
            "579:\tlearn: 0.0000160\ttotal: 24.1s\tremaining: 17.5s\n",
            "580:\tlearn: 0.0000159\ttotal: 24.2s\tremaining: 17.4s\n",
            "581:\tlearn: 0.0000159\ttotal: 24.2s\tremaining: 17.4s\n",
            "582:\tlearn: 0.0000159\ttotal: 24.2s\tremaining: 17.3s\n",
            "583:\tlearn: 0.0000158\ttotal: 24.3s\tremaining: 17.3s\n",
            "584:\tlearn: 0.0000158\ttotal: 24.3s\tremaining: 17.3s\n",
            "585:\tlearn: 0.0000158\ttotal: 24.4s\tremaining: 17.2s\n",
            "586:\tlearn: 0.0000157\ttotal: 24.4s\tremaining: 17.2s\n",
            "587:\tlearn: 0.0000157\ttotal: 24.4s\tremaining: 17.1s\n",
            "588:\tlearn: 0.0000157\ttotal: 24.4s\tremaining: 17.1s\n",
            "589:\tlearn: 0.0000156\ttotal: 24.5s\tremaining: 17s\n",
            "590:\tlearn: 0.0000156\ttotal: 24.5s\tremaining: 17s\n",
            "591:\tlearn: 0.0000156\ttotal: 24.6s\tremaining: 16.9s\n",
            "592:\tlearn: 0.0000156\ttotal: 24.6s\tremaining: 16.9s\n",
            "593:\tlearn: 0.0000155\ttotal: 24.6s\tremaining: 16.8s\n",
            "594:\tlearn: 0.0000155\ttotal: 24.7s\tremaining: 16.8s\n",
            "595:\tlearn: 0.0000154\ttotal: 24.7s\tremaining: 16.8s\n",
            "596:\tlearn: 0.0000154\ttotal: 24.8s\tremaining: 16.7s\n",
            "597:\tlearn: 0.0000153\ttotal: 24.8s\tremaining: 16.7s\n",
            "598:\tlearn: 0.0000153\ttotal: 24.8s\tremaining: 16.6s\n",
            "599:\tlearn: 0.0000153\ttotal: 24.9s\tremaining: 16.6s\n",
            "600:\tlearn: 0.0000153\ttotal: 24.9s\tremaining: 16.5s\n",
            "601:\tlearn: 0.0000153\ttotal: 24.9s\tremaining: 16.5s\n",
            "602:\tlearn: 0.0000152\ttotal: 25s\tremaining: 16.4s\n",
            "603:\tlearn: 0.0000152\ttotal: 25s\tremaining: 16.4s\n",
            "604:\tlearn: 0.0000152\ttotal: 25s\tremaining: 16.3s\n",
            "605:\tlearn: 0.0000151\ttotal: 25.1s\tremaining: 16.3s\n",
            "606:\tlearn: 0.0000151\ttotal: 25.1s\tremaining: 16.2s\n",
            "607:\tlearn: 0.0000151\ttotal: 25.1s\tremaining: 16.2s\n",
            "608:\tlearn: 0.0000150\ttotal: 25.2s\tremaining: 16.2s\n",
            "609:\tlearn: 0.0000150\ttotal: 25.2s\tremaining: 16.1s\n",
            "610:\tlearn: 0.0000150\ttotal: 25.3s\tremaining: 16.1s\n",
            "611:\tlearn: 0.0000150\ttotal: 25.3s\tremaining: 16s\n",
            "612:\tlearn: 0.0000149\ttotal: 25.4s\tremaining: 16s\n",
            "613:\tlearn: 0.0000148\ttotal: 25.4s\tremaining: 16s\n",
            "614:\tlearn: 0.0000148\ttotal: 25.4s\tremaining: 15.9s\n",
            "615:\tlearn: 0.0000147\ttotal: 25.5s\tremaining: 15.9s\n",
            "616:\tlearn: 0.0000147\ttotal: 25.5s\tremaining: 15.8s\n",
            "617:\tlearn: 0.0000147\ttotal: 25.5s\tremaining: 15.8s\n",
            "618:\tlearn: 0.0000147\ttotal: 25.6s\tremaining: 15.7s\n",
            "619:\tlearn: 0.0000147\ttotal: 25.6s\tremaining: 15.7s\n",
            "620:\tlearn: 0.0000147\ttotal: 25.6s\tremaining: 15.7s\n",
            "621:\tlearn: 0.0000146\ttotal: 25.7s\tremaining: 15.6s\n",
            "622:\tlearn: 0.0000146\ttotal: 25.8s\tremaining: 15.6s\n",
            "623:\tlearn: 0.0000146\ttotal: 25.8s\tremaining: 15.5s\n",
            "624:\tlearn: 0.0000146\ttotal: 25.8s\tremaining: 15.5s\n",
            "625:\tlearn: 0.0000145\ttotal: 25.9s\tremaining: 15.4s\n",
            "626:\tlearn: 0.0000145\ttotal: 25.9s\tremaining: 15.4s\n",
            "627:\tlearn: 0.0000145\ttotal: 25.9s\tremaining: 15.4s\n",
            "628:\tlearn: 0.0000144\ttotal: 26s\tremaining: 15.3s\n",
            "629:\tlearn: 0.0000144\ttotal: 26s\tremaining: 15.3s\n",
            "630:\tlearn: 0.0000143\ttotal: 26.1s\tremaining: 15.2s\n",
            "631:\tlearn: 0.0000143\ttotal: 26.1s\tremaining: 15.2s\n",
            "632:\tlearn: 0.0000143\ttotal: 26.1s\tremaining: 15.2s\n",
            "633:\tlearn: 0.0000142\ttotal: 26.2s\tremaining: 15.1s\n",
            "634:\tlearn: 0.0000142\ttotal: 26.2s\tremaining: 15.1s\n",
            "635:\tlearn: 0.0000142\ttotal: 26.3s\tremaining: 15s\n",
            "636:\tlearn: 0.0000142\ttotal: 26.3s\tremaining: 15s\n",
            "637:\tlearn: 0.0000141\ttotal: 26.4s\tremaining: 15s\n",
            "638:\tlearn: 0.0000140\ttotal: 26.4s\tremaining: 14.9s\n",
            "639:\tlearn: 0.0000140\ttotal: 26.4s\tremaining: 14.9s\n",
            "640:\tlearn: 0.0000140\ttotal: 26.5s\tremaining: 14.8s\n",
            "641:\tlearn: 0.0000140\ttotal: 26.5s\tremaining: 14.8s\n",
            "642:\tlearn: 0.0000140\ttotal: 26.6s\tremaining: 14.7s\n",
            "643:\tlearn: 0.0000140\ttotal: 26.6s\tremaining: 14.7s\n",
            "644:\tlearn: 0.0000139\ttotal: 26.6s\tremaining: 14.7s\n",
            "645:\tlearn: 0.0000138\ttotal: 26.7s\tremaining: 14.6s\n",
            "646:\tlearn: 0.0000138\ttotal: 26.7s\tremaining: 14.6s\n",
            "647:\tlearn: 0.0000138\ttotal: 26.7s\tremaining: 14.5s\n",
            "648:\tlearn: 0.0000138\ttotal: 26.8s\tremaining: 14.5s\n",
            "649:\tlearn: 0.0000138\ttotal: 26.8s\tremaining: 14.4s\n",
            "650:\tlearn: 0.0000137\ttotal: 26.9s\tremaining: 14.4s\n",
            "651:\tlearn: 0.0000137\ttotal: 26.9s\tremaining: 14.4s\n",
            "652:\tlearn: 0.0000137\ttotal: 26.9s\tremaining: 14.3s\n",
            "653:\tlearn: 0.0000137\ttotal: 27s\tremaining: 14.3s\n",
            "654:\tlearn: 0.0000137\ttotal: 27s\tremaining: 14.2s\n",
            "655:\tlearn: 0.0000137\ttotal: 27s\tremaining: 14.2s\n",
            "656:\tlearn: 0.0000136\ttotal: 27.1s\tremaining: 14.1s\n",
            "657:\tlearn: 0.0000136\ttotal: 27.1s\tremaining: 14.1s\n",
            "658:\tlearn: 0.0000136\ttotal: 27.2s\tremaining: 14.1s\n",
            "659:\tlearn: 0.0000136\ttotal: 27.2s\tremaining: 14s\n",
            "660:\tlearn: 0.0000135\ttotal: 27.2s\tremaining: 14s\n",
            "661:\tlearn: 0.0000135\ttotal: 27.3s\tremaining: 13.9s\n",
            "662:\tlearn: 0.0000134\ttotal: 27.3s\tremaining: 13.9s\n",
            "663:\tlearn: 0.0000134\ttotal: 27.3s\tremaining: 13.8s\n",
            "664:\tlearn: 0.0000134\ttotal: 27.4s\tremaining: 13.8s\n",
            "665:\tlearn: 0.0000134\ttotal: 27.4s\tremaining: 13.7s\n",
            "666:\tlearn: 0.0000134\ttotal: 27.4s\tremaining: 13.7s\n",
            "667:\tlearn: 0.0000134\ttotal: 27.5s\tremaining: 13.7s\n",
            "668:\tlearn: 0.0000134\ttotal: 27.5s\tremaining: 13.6s\n",
            "669:\tlearn: 0.0000133\ttotal: 27.6s\tremaining: 13.6s\n",
            "670:\tlearn: 0.0000133\ttotal: 27.6s\tremaining: 13.5s\n",
            "671:\tlearn: 0.0000133\ttotal: 27.6s\tremaining: 13.5s\n",
            "672:\tlearn: 0.0000132\ttotal: 27.7s\tremaining: 13.5s\n",
            "673:\tlearn: 0.0000132\ttotal: 27.7s\tremaining: 13.4s\n",
            "674:\tlearn: 0.0000131\ttotal: 27.8s\tremaining: 13.4s\n",
            "675:\tlearn: 0.0000131\ttotal: 27.8s\tremaining: 13.3s\n",
            "676:\tlearn: 0.0000131\ttotal: 27.9s\tremaining: 13.3s\n",
            "677:\tlearn: 0.0000130\ttotal: 27.9s\tremaining: 13.3s\n",
            "678:\tlearn: 0.0000130\ttotal: 28s\tremaining: 13.2s\n",
            "679:\tlearn: 0.0000130\ttotal: 28.1s\tremaining: 13.2s\n",
            "680:\tlearn: 0.0000129\ttotal: 28.1s\tremaining: 13.2s\n",
            "681:\tlearn: 0.0000129\ttotal: 28.2s\tremaining: 13.1s\n",
            "682:\tlearn: 0.0000129\ttotal: 28.2s\tremaining: 13.1s\n",
            "683:\tlearn: 0.0000128\ttotal: 28.3s\tremaining: 13.1s\n",
            "684:\tlearn: 0.0000128\ttotal: 28.3s\tremaining: 13s\n",
            "685:\tlearn: 0.0000128\ttotal: 28.4s\tremaining: 13s\n",
            "686:\tlearn: 0.0000128\ttotal: 28.4s\tremaining: 12.9s\n",
            "687:\tlearn: 0.0000128\ttotal: 28.4s\tremaining: 12.9s\n",
            "688:\tlearn: 0.0000128\ttotal: 28.5s\tremaining: 12.8s\n",
            "689:\tlearn: 0.0000127\ttotal: 28.5s\tremaining: 12.8s\n",
            "690:\tlearn: 0.0000127\ttotal: 28.5s\tremaining: 12.8s\n",
            "691:\tlearn: 0.0000127\ttotal: 28.6s\tremaining: 12.7s\n",
            "692:\tlearn: 0.0000127\ttotal: 28.6s\tremaining: 12.7s\n",
            "693:\tlearn: 0.0000127\ttotal: 28.7s\tremaining: 12.6s\n",
            "694:\tlearn: 0.0000127\ttotal: 28.7s\tremaining: 12.6s\n",
            "695:\tlearn: 0.0000127\ttotal: 28.7s\tremaining: 12.6s\n",
            "696:\tlearn: 0.0000126\ttotal: 28.8s\tremaining: 12.5s\n",
            "697:\tlearn: 0.0000126\ttotal: 28.8s\tremaining: 12.5s\n",
            "698:\tlearn: 0.0000126\ttotal: 28.9s\tremaining: 12.4s\n",
            "699:\tlearn: 0.0000125\ttotal: 28.9s\tremaining: 12.4s\n",
            "700:\tlearn: 0.0000125\ttotal: 29s\tremaining: 12.4s\n",
            "701:\tlearn: 0.0000125\ttotal: 29.1s\tremaining: 12.3s\n",
            "702:\tlearn: 0.0000124\ttotal: 29.1s\tremaining: 12.3s\n",
            "703:\tlearn: 0.0000124\ttotal: 29.1s\tremaining: 12.3s\n",
            "704:\tlearn: 0.0000124\ttotal: 29.2s\tremaining: 12.2s\n",
            "705:\tlearn: 0.0000124\ttotal: 29.2s\tremaining: 12.2s\n",
            "706:\tlearn: 0.0000124\ttotal: 29.3s\tremaining: 12.1s\n",
            "707:\tlearn: 0.0000124\ttotal: 29.3s\tremaining: 12.1s\n",
            "708:\tlearn: 0.0000123\ttotal: 29.4s\tremaining: 12.1s\n",
            "709:\tlearn: 0.0000123\ttotal: 29.4s\tremaining: 12s\n",
            "710:\tlearn: 0.0000122\ttotal: 29.5s\tremaining: 12s\n",
            "711:\tlearn: 0.0000122\ttotal: 29.5s\tremaining: 11.9s\n",
            "712:\tlearn: 0.0000122\ttotal: 29.6s\tremaining: 11.9s\n",
            "713:\tlearn: 0.0000121\ttotal: 29.6s\tremaining: 11.9s\n",
            "714:\tlearn: 0.0000121\ttotal: 29.7s\tremaining: 11.8s\n",
            "715:\tlearn: 0.0000121\ttotal: 29.7s\tremaining: 11.8s\n",
            "716:\tlearn: 0.0000120\ttotal: 29.8s\tremaining: 11.7s\n",
            "717:\tlearn: 0.0000120\ttotal: 29.8s\tremaining: 11.7s\n",
            "718:\tlearn: 0.0000120\ttotal: 29.8s\tremaining: 11.7s\n",
            "719:\tlearn: 0.0000120\ttotal: 29.9s\tremaining: 11.6s\n",
            "720:\tlearn: 0.0000120\ttotal: 29.9s\tremaining: 11.6s\n",
            "721:\tlearn: 0.0000119\ttotal: 29.9s\tremaining: 11.5s\n",
            "722:\tlearn: 0.0000119\ttotal: 30s\tremaining: 11.5s\n",
            "723:\tlearn: 0.0000119\ttotal: 30s\tremaining: 11.4s\n",
            "724:\tlearn: 0.0000119\ttotal: 30.1s\tremaining: 11.4s\n",
            "725:\tlearn: 0.0000118\ttotal: 30.1s\tremaining: 11.4s\n",
            "726:\tlearn: 0.0000118\ttotal: 30.1s\tremaining: 11.3s\n",
            "727:\tlearn: 0.0000118\ttotal: 30.2s\tremaining: 11.3s\n",
            "728:\tlearn: 0.0000118\ttotal: 30.2s\tremaining: 11.2s\n",
            "729:\tlearn: 0.0000118\ttotal: 30.2s\tremaining: 11.2s\n",
            "730:\tlearn: 0.0000118\ttotal: 30.3s\tremaining: 11.1s\n",
            "731:\tlearn: 0.0000118\ttotal: 30.3s\tremaining: 11.1s\n",
            "732:\tlearn: 0.0000118\ttotal: 30.3s\tremaining: 11s\n",
            "733:\tlearn: 0.0000118\ttotal: 30.4s\tremaining: 11s\n",
            "734:\tlearn: 0.0000118\ttotal: 30.4s\tremaining: 11s\n",
            "735:\tlearn: 0.0000118\ttotal: 30.4s\tremaining: 10.9s\n",
            "736:\tlearn: 0.0000117\ttotal: 30.4s\tremaining: 10.9s\n",
            "737:\tlearn: 0.0000117\ttotal: 30.5s\tremaining: 10.8s\n",
            "738:\tlearn: 0.0000117\ttotal: 30.6s\tremaining: 10.8s\n",
            "739:\tlearn: 0.0000117\ttotal: 30.6s\tremaining: 10.7s\n",
            "740:\tlearn: 0.0000116\ttotal: 30.6s\tremaining: 10.7s\n",
            "741:\tlearn: 0.0000116\ttotal: 30.7s\tremaining: 10.7s\n",
            "742:\tlearn: 0.0000116\ttotal: 30.7s\tremaining: 10.6s\n",
            "743:\tlearn: 0.0000116\ttotal: 30.7s\tremaining: 10.6s\n",
            "744:\tlearn: 0.0000116\ttotal: 30.8s\tremaining: 10.5s\n",
            "745:\tlearn: 0.0000115\ttotal: 30.8s\tremaining: 10.5s\n",
            "746:\tlearn: 0.0000115\ttotal: 30.9s\tremaining: 10.5s\n",
            "747:\tlearn: 0.0000115\ttotal: 30.9s\tremaining: 10.4s\n",
            "748:\tlearn: 0.0000114\ttotal: 30.9s\tremaining: 10.4s\n",
            "749:\tlearn: 0.0000114\ttotal: 31s\tremaining: 10.3s\n",
            "750:\tlearn: 0.0000114\ttotal: 31s\tremaining: 10.3s\n",
            "751:\tlearn: 0.0000114\ttotal: 31.1s\tremaining: 10.3s\n",
            "752:\tlearn: 0.0000113\ttotal: 31.2s\tremaining: 10.2s\n",
            "753:\tlearn: 0.0000113\ttotal: 31.2s\tremaining: 10.2s\n",
            "754:\tlearn: 0.0000113\ttotal: 31.3s\tremaining: 10.2s\n",
            "755:\tlearn: 0.0000113\ttotal: 31.3s\tremaining: 10.1s\n",
            "756:\tlearn: 0.0000113\ttotal: 31.4s\tremaining: 10.1s\n",
            "757:\tlearn: 0.0000113\ttotal: 31.4s\tremaining: 10s\n",
            "758:\tlearn: 0.0000113\ttotal: 31.4s\tremaining: 9.97s\n",
            "759:\tlearn: 0.0000113\ttotal: 31.5s\tremaining: 9.93s\n",
            "760:\tlearn: 0.0000112\ttotal: 31.5s\tremaining: 9.89s\n",
            "761:\tlearn: 0.0000112\ttotal: 31.5s\tremaining: 9.85s\n",
            "762:\tlearn: 0.0000112\ttotal: 31.6s\tremaining: 9.81s\n",
            "763:\tlearn: 0.0000111\ttotal: 31.6s\tremaining: 9.77s\n",
            "764:\tlearn: 0.0000111\ttotal: 31.6s\tremaining: 9.72s\n",
            "765:\tlearn: 0.0000111\ttotal: 31.7s\tremaining: 9.68s\n",
            "766:\tlearn: 0.0000111\ttotal: 31.7s\tremaining: 9.64s\n",
            "767:\tlearn: 0.0000111\ttotal: 31.8s\tremaining: 9.6s\n",
            "768:\tlearn: 0.0000110\ttotal: 31.8s\tremaining: 9.56s\n",
            "769:\tlearn: 0.0000110\ttotal: 31.9s\tremaining: 9.52s\n",
            "770:\tlearn: 0.0000110\ttotal: 31.9s\tremaining: 9.47s\n",
            "771:\tlearn: 0.0000110\ttotal: 31.9s\tremaining: 9.42s\n",
            "772:\tlearn: 0.0000110\ttotal: 32s\tremaining: 9.38s\n",
            "773:\tlearn: 0.0000109\ttotal: 32s\tremaining: 9.34s\n",
            "774:\tlearn: 0.0000109\ttotal: 32s\tremaining: 9.3s\n",
            "775:\tlearn: 0.0000109\ttotal: 32.1s\tremaining: 9.26s\n",
            "776:\tlearn: 0.0000109\ttotal: 32.1s\tremaining: 9.22s\n",
            "777:\tlearn: 0.0000108\ttotal: 32.2s\tremaining: 9.18s\n",
            "778:\tlearn: 0.0000108\ttotal: 32.2s\tremaining: 9.15s\n",
            "779:\tlearn: 0.0000108\ttotal: 32.3s\tremaining: 9.11s\n",
            "780:\tlearn: 0.0000108\ttotal: 32.3s\tremaining: 9.07s\n",
            "781:\tlearn: 0.0000107\ttotal: 32.4s\tremaining: 9.03s\n",
            "782:\tlearn: 0.0000107\ttotal: 32.4s\tremaining: 8.98s\n",
            "783:\tlearn: 0.0000107\ttotal: 32.5s\tremaining: 8.94s\n",
            "784:\tlearn: 0.0000107\ttotal: 32.5s\tremaining: 8.9s\n",
            "785:\tlearn: 0.0000107\ttotal: 32.5s\tremaining: 8.86s\n",
            "786:\tlearn: 0.0000106\ttotal: 32.6s\tremaining: 8.82s\n",
            "787:\tlearn: 0.0000106\ttotal: 32.6s\tremaining: 8.78s\n",
            "788:\tlearn: 0.0000106\ttotal: 32.7s\tremaining: 8.73s\n",
            "789:\tlearn: 0.0000106\ttotal: 32.7s\tremaining: 8.69s\n",
            "790:\tlearn: 0.0000106\ttotal: 32.7s\tremaining: 8.65s\n",
            "791:\tlearn: 0.0000105\ttotal: 32.8s\tremaining: 8.61s\n",
            "792:\tlearn: 0.0000105\ttotal: 32.8s\tremaining: 8.57s\n",
            "793:\tlearn: 0.0000105\ttotal: 32.9s\tremaining: 8.53s\n",
            "794:\tlearn: 0.0000104\ttotal: 32.9s\tremaining: 8.49s\n",
            "795:\tlearn: 0.0000104\ttotal: 33s\tremaining: 8.45s\n",
            "796:\tlearn: 0.0000104\ttotal: 33s\tremaining: 8.4s\n",
            "797:\tlearn: 0.0000104\ttotal: 33s\tremaining: 8.36s\n",
            "798:\tlearn: 0.0000103\ttotal: 33.1s\tremaining: 8.32s\n",
            "799:\tlearn: 0.0000103\ttotal: 33.1s\tremaining: 8.28s\n",
            "800:\tlearn: 0.0000103\ttotal: 33.2s\tremaining: 8.24s\n",
            "801:\tlearn: 0.0000103\ttotal: 33.2s\tremaining: 8.2s\n",
            "802:\tlearn: 0.0000103\ttotal: 33.2s\tremaining: 8.15s\n",
            "803:\tlearn: 0.0000102\ttotal: 33.3s\tremaining: 8.11s\n",
            "804:\tlearn: 0.0000102\ttotal: 33.3s\tremaining: 8.07s\n",
            "805:\tlearn: 0.0000102\ttotal: 33.4s\tremaining: 8.03s\n",
            "806:\tlearn: 0.0000102\ttotal: 33.4s\tremaining: 7.99s\n",
            "807:\tlearn: 0.0000102\ttotal: 33.4s\tremaining: 7.95s\n",
            "808:\tlearn: 0.0000102\ttotal: 33.5s\tremaining: 7.9s\n",
            "809:\tlearn: 0.0000102\ttotal: 33.5s\tremaining: 7.86s\n",
            "810:\tlearn: 0.0000101\ttotal: 33.6s\tremaining: 7.82s\n",
            "811:\tlearn: 0.0000101\ttotal: 33.6s\tremaining: 7.78s\n",
            "812:\tlearn: 0.0000101\ttotal: 33.6s\tremaining: 7.74s\n",
            "813:\tlearn: 0.0000101\ttotal: 33.7s\tremaining: 7.69s\n",
            "814:\tlearn: 0.0000101\ttotal: 33.7s\tremaining: 7.65s\n",
            "815:\tlearn: 0.0000101\ttotal: 33.7s\tremaining: 7.61s\n",
            "816:\tlearn: 0.0000100\ttotal: 33.8s\tremaining: 7.56s\n",
            "817:\tlearn: 0.0000100\ttotal: 33.8s\tremaining: 7.52s\n",
            "818:\tlearn: 0.0000100\ttotal: 33.9s\tremaining: 7.48s\n",
            "819:\tlearn: 0.0000100\ttotal: 33.9s\tremaining: 7.44s\n",
            "820:\tlearn: 0.0000100\ttotal: 33.9s\tremaining: 7.4s\n",
            "821:\tlearn: 0.0000100\ttotal: 34s\tremaining: 7.36s\n",
            "822:\tlearn: 0.0000100\ttotal: 34s\tremaining: 7.31s\n",
            "823:\tlearn: 0.0000100\ttotal: 34s\tremaining: 7.27s\n",
            "824:\tlearn: 0.0000100\ttotal: 34.1s\tremaining: 7.22s\n",
            "825:\tlearn: 0.0000099\ttotal: 34.1s\tremaining: 7.18s\n",
            "826:\tlearn: 0.0000099\ttotal: 34.1s\tremaining: 7.14s\n",
            "827:\tlearn: 0.0000099\ttotal: 34.2s\tremaining: 7.1s\n",
            "828:\tlearn: 0.0000099\ttotal: 34.2s\tremaining: 7.06s\n",
            "829:\tlearn: 0.0000099\ttotal: 34.3s\tremaining: 7.02s\n",
            "830:\tlearn: 0.0000099\ttotal: 34.3s\tremaining: 6.97s\n",
            "831:\tlearn: 0.0000098\ttotal: 34.3s\tremaining: 6.93s\n",
            "832:\tlearn: 0.0000098\ttotal: 34.4s\tremaining: 6.89s\n",
            "833:\tlearn: 0.0000098\ttotal: 34.4s\tremaining: 6.85s\n",
            "834:\tlearn: 0.0000098\ttotal: 34.5s\tremaining: 6.81s\n",
            "835:\tlearn: 0.0000097\ttotal: 34.5s\tremaining: 6.77s\n",
            "836:\tlearn: 0.0000097\ttotal: 34.6s\tremaining: 6.73s\n",
            "837:\tlearn: 0.0000097\ttotal: 34.6s\tremaining: 6.69s\n",
            "838:\tlearn: 0.0000097\ttotal: 34.6s\tremaining: 6.65s\n",
            "839:\tlearn: 0.0000097\ttotal: 34.7s\tremaining: 6.6s\n",
            "840:\tlearn: 0.0000097\ttotal: 34.7s\tremaining: 6.56s\n",
            "841:\tlearn: 0.0000097\ttotal: 34.7s\tremaining: 6.52s\n",
            "842:\tlearn: 0.0000097\ttotal: 34.8s\tremaining: 6.47s\n",
            "843:\tlearn: 0.0000097\ttotal: 34.8s\tremaining: 6.43s\n",
            "844:\tlearn: 0.0000096\ttotal: 34.9s\tremaining: 6.39s\n",
            "845:\tlearn: 0.0000096\ttotal: 34.9s\tremaining: 6.35s\n",
            "846:\tlearn: 0.0000096\ttotal: 34.9s\tremaining: 6.31s\n",
            "847:\tlearn: 0.0000096\ttotal: 35s\tremaining: 6.27s\n",
            "848:\tlearn: 0.0000096\ttotal: 35s\tremaining: 6.23s\n",
            "849:\tlearn: 0.0000096\ttotal: 35.1s\tremaining: 6.19s\n",
            "850:\tlearn: 0.0000096\ttotal: 35.1s\tremaining: 6.14s\n",
            "851:\tlearn: 0.0000096\ttotal: 35.1s\tremaining: 6.1s\n",
            "852:\tlearn: 0.0000096\ttotal: 35.2s\tremaining: 6.06s\n",
            "853:\tlearn: 0.0000096\ttotal: 35.2s\tremaining: 6.01s\n",
            "854:\tlearn: 0.0000096\ttotal: 35.2s\tremaining: 5.97s\n",
            "855:\tlearn: 0.0000095\ttotal: 35.3s\tremaining: 5.93s\n",
            "856:\tlearn: 0.0000095\ttotal: 35.3s\tremaining: 5.89s\n",
            "857:\tlearn: 0.0000095\ttotal: 35.4s\tremaining: 5.85s\n",
            "858:\tlearn: 0.0000095\ttotal: 35.4s\tremaining: 5.81s\n",
            "859:\tlearn: 0.0000095\ttotal: 35.4s\tremaining: 5.77s\n",
            "860:\tlearn: 0.0000095\ttotal: 35.5s\tremaining: 5.73s\n",
            "861:\tlearn: 0.0000095\ttotal: 35.6s\tremaining: 5.69s\n",
            "862:\tlearn: 0.0000094\ttotal: 35.6s\tremaining: 5.65s\n",
            "863:\tlearn: 0.0000094\ttotal: 35.6s\tremaining: 5.61s\n",
            "864:\tlearn: 0.0000094\ttotal: 35.7s\tremaining: 5.57s\n",
            "865:\tlearn: 0.0000094\ttotal: 35.7s\tremaining: 5.52s\n",
            "866:\tlearn: 0.0000094\ttotal: 35.7s\tremaining: 5.48s\n",
            "867:\tlearn: 0.0000094\ttotal: 35.8s\tremaining: 5.44s\n",
            "868:\tlearn: 0.0000094\ttotal: 35.8s\tremaining: 5.4s\n",
            "869:\tlearn: 0.0000094\ttotal: 35.9s\tremaining: 5.36s\n",
            "870:\tlearn: 0.0000093\ttotal: 35.9s\tremaining: 5.32s\n",
            "871:\tlearn: 0.0000093\ttotal: 35.9s\tremaining: 5.28s\n",
            "872:\tlearn: 0.0000093\ttotal: 36s\tremaining: 5.23s\n",
            "873:\tlearn: 0.0000093\ttotal: 36s\tremaining: 5.19s\n",
            "874:\tlearn: 0.0000093\ttotal: 36.1s\tremaining: 5.15s\n",
            "875:\tlearn: 0.0000092\ttotal: 36.1s\tremaining: 5.11s\n",
            "876:\tlearn: 0.0000092\ttotal: 36.2s\tremaining: 5.07s\n",
            "877:\tlearn: 0.0000092\ttotal: 36.2s\tremaining: 5.03s\n",
            "878:\tlearn: 0.0000092\ttotal: 36.3s\tremaining: 4.99s\n",
            "879:\tlearn: 0.0000092\ttotal: 36.3s\tremaining: 4.95s\n",
            "880:\tlearn: 0.0000092\ttotal: 36.4s\tremaining: 4.91s\n",
            "881:\tlearn: 0.0000092\ttotal: 36.4s\tremaining: 4.87s\n",
            "882:\tlearn: 0.0000091\ttotal: 36.5s\tremaining: 4.83s\n",
            "883:\tlearn: 0.0000091\ttotal: 36.5s\tremaining: 4.79s\n",
            "884:\tlearn: 0.0000091\ttotal: 36.6s\tremaining: 4.75s\n",
            "885:\tlearn: 0.0000091\ttotal: 36.6s\tremaining: 4.71s\n",
            "886:\tlearn: 0.0000091\ttotal: 36.7s\tremaining: 4.67s\n",
            "887:\tlearn: 0.0000091\ttotal: 36.7s\tremaining: 4.63s\n",
            "888:\tlearn: 0.0000090\ttotal: 36.7s\tremaining: 4.59s\n",
            "889:\tlearn: 0.0000090\ttotal: 36.8s\tremaining: 4.54s\n",
            "890:\tlearn: 0.0000090\ttotal: 36.8s\tremaining: 4.5s\n",
            "891:\tlearn: 0.0000090\ttotal: 36.8s\tremaining: 4.46s\n",
            "892:\tlearn: 0.0000090\ttotal: 36.9s\tremaining: 4.42s\n",
            "893:\tlearn: 0.0000090\ttotal: 36.9s\tremaining: 4.38s\n",
            "894:\tlearn: 0.0000090\ttotal: 37s\tremaining: 4.34s\n",
            "895:\tlearn: 0.0000089\ttotal: 37s\tremaining: 4.3s\n",
            "896:\tlearn: 0.0000089\ttotal: 37.1s\tremaining: 4.25s\n",
            "897:\tlearn: 0.0000089\ttotal: 37.1s\tremaining: 4.21s\n",
            "898:\tlearn: 0.0000089\ttotal: 37.2s\tremaining: 4.17s\n",
            "899:\tlearn: 0.0000089\ttotal: 37.2s\tremaining: 4.13s\n",
            "900:\tlearn: 0.0000089\ttotal: 37.2s\tremaining: 4.09s\n",
            "901:\tlearn: 0.0000089\ttotal: 37.3s\tremaining: 4.05s\n",
            "902:\tlearn: 0.0000089\ttotal: 37.3s\tremaining: 4.01s\n",
            "903:\tlearn: 0.0000089\ttotal: 37.4s\tremaining: 3.97s\n",
            "904:\tlearn: 0.0000088\ttotal: 37.4s\tremaining: 3.93s\n",
            "905:\tlearn: 0.0000088\ttotal: 37.5s\tremaining: 3.89s\n",
            "906:\tlearn: 0.0000088\ttotal: 37.5s\tremaining: 3.84s\n",
            "907:\tlearn: 0.0000088\ttotal: 37.5s\tremaining: 3.8s\n",
            "908:\tlearn: 0.0000088\ttotal: 37.6s\tremaining: 3.76s\n",
            "909:\tlearn: 0.0000088\ttotal: 37.6s\tremaining: 3.72s\n",
            "910:\tlearn: 0.0000088\ttotal: 37.6s\tremaining: 3.68s\n",
            "911:\tlearn: 0.0000088\ttotal: 37.7s\tremaining: 3.63s\n",
            "912:\tlearn: 0.0000088\ttotal: 37.7s\tremaining: 3.59s\n",
            "913:\tlearn: 0.0000088\ttotal: 37.7s\tremaining: 3.55s\n",
            "914:\tlearn: 0.0000088\ttotal: 37.8s\tremaining: 3.51s\n",
            "915:\tlearn: 0.0000087\ttotal: 37.8s\tremaining: 3.47s\n",
            "916:\tlearn: 0.0000087\ttotal: 37.9s\tremaining: 3.43s\n",
            "917:\tlearn: 0.0000087\ttotal: 37.9s\tremaining: 3.39s\n",
            "918:\tlearn: 0.0000087\ttotal: 38s\tremaining: 3.35s\n",
            "919:\tlearn: 0.0000087\ttotal: 38s\tremaining: 3.3s\n",
            "920:\tlearn: 0.0000087\ttotal: 38s\tremaining: 3.26s\n",
            "921:\tlearn: 0.0000087\ttotal: 38.1s\tremaining: 3.22s\n",
            "922:\tlearn: 0.0000087\ttotal: 38.1s\tremaining: 3.18s\n",
            "923:\tlearn: 0.0000086\ttotal: 38.2s\tremaining: 3.14s\n",
            "924:\tlearn: 0.0000086\ttotal: 38.2s\tremaining: 3.1s\n",
            "925:\tlearn: 0.0000086\ttotal: 38.3s\tremaining: 3.06s\n",
            "926:\tlearn: 0.0000086\ttotal: 38.3s\tremaining: 3.02s\n",
            "927:\tlearn: 0.0000086\ttotal: 38.3s\tremaining: 2.97s\n",
            "928:\tlearn: 0.0000086\ttotal: 38.4s\tremaining: 2.93s\n",
            "929:\tlearn: 0.0000086\ttotal: 38.4s\tremaining: 2.89s\n",
            "930:\tlearn: 0.0000086\ttotal: 38.5s\tremaining: 2.85s\n",
            "931:\tlearn: 0.0000086\ttotal: 38.5s\tremaining: 2.81s\n",
            "932:\tlearn: 0.0000085\ttotal: 38.6s\tremaining: 2.77s\n",
            "933:\tlearn: 0.0000085\ttotal: 38.6s\tremaining: 2.73s\n",
            "934:\tlearn: 0.0000085\ttotal: 38.7s\tremaining: 2.69s\n",
            "935:\tlearn: 0.0000085\ttotal: 38.7s\tremaining: 2.64s\n",
            "936:\tlearn: 0.0000085\ttotal: 38.7s\tremaining: 2.6s\n",
            "937:\tlearn: 0.0000085\ttotal: 38.8s\tremaining: 2.56s\n",
            "938:\tlearn: 0.0000085\ttotal: 38.8s\tremaining: 2.52s\n",
            "939:\tlearn: 0.0000085\ttotal: 38.8s\tremaining: 2.48s\n",
            "940:\tlearn: 0.0000085\ttotal: 38.8s\tremaining: 2.43s\n",
            "941:\tlearn: 0.0000085\ttotal: 38.9s\tremaining: 2.39s\n",
            "942:\tlearn: 0.0000085\ttotal: 38.9s\tremaining: 2.35s\n",
            "943:\tlearn: 0.0000085\ttotal: 38.9s\tremaining: 2.31s\n",
            "944:\tlearn: 0.0000084\ttotal: 39s\tremaining: 2.27s\n",
            "945:\tlearn: 0.0000084\ttotal: 39s\tremaining: 2.23s\n",
            "946:\tlearn: 0.0000084\ttotal: 39.1s\tremaining: 2.19s\n",
            "947:\tlearn: 0.0000084\ttotal: 39.1s\tremaining: 2.15s\n",
            "948:\tlearn: 0.0000084\ttotal: 39.1s\tremaining: 2.1s\n",
            "949:\tlearn: 0.0000084\ttotal: 39.2s\tremaining: 2.06s\n",
            "950:\tlearn: 0.0000084\ttotal: 39.2s\tremaining: 2.02s\n",
            "951:\tlearn: 0.0000084\ttotal: 39.2s\tremaining: 1.98s\n",
            "952:\tlearn: 0.0000084\ttotal: 39.3s\tremaining: 1.94s\n",
            "953:\tlearn: 0.0000083\ttotal: 39.3s\tremaining: 1.9s\n",
            "954:\tlearn: 0.0000083\ttotal: 39.4s\tremaining: 1.85s\n",
            "955:\tlearn: 0.0000083\ttotal: 39.4s\tremaining: 1.81s\n",
            "956:\tlearn: 0.0000083\ttotal: 39.4s\tremaining: 1.77s\n",
            "957:\tlearn: 0.0000083\ttotal: 39.5s\tremaining: 1.73s\n",
            "958:\tlearn: 0.0000083\ttotal: 39.5s\tremaining: 1.69s\n",
            "959:\tlearn: 0.0000083\ttotal: 39.6s\tremaining: 1.65s\n",
            "960:\tlearn: 0.0000082\ttotal: 39.6s\tremaining: 1.61s\n",
            "961:\tlearn: 0.0000082\ttotal: 39.7s\tremaining: 1.57s\n",
            "962:\tlearn: 0.0000082\ttotal: 39.7s\tremaining: 1.52s\n",
            "963:\tlearn: 0.0000082\ttotal: 39.7s\tremaining: 1.48s\n",
            "964:\tlearn: 0.0000082\ttotal: 39.8s\tremaining: 1.44s\n",
            "965:\tlearn: 0.0000082\ttotal: 39.8s\tremaining: 1.4s\n",
            "966:\tlearn: 0.0000082\ttotal: 39.8s\tremaining: 1.36s\n",
            "967:\tlearn: 0.0000082\ttotal: 39.9s\tremaining: 1.32s\n",
            "968:\tlearn: 0.0000082\ttotal: 39.9s\tremaining: 1.28s\n",
            "969:\tlearn: 0.0000082\ttotal: 40s\tremaining: 1.24s\n",
            "970:\tlearn: 0.0000081\ttotal: 40s\tremaining: 1.2s\n",
            "971:\tlearn: 0.0000081\ttotal: 40.1s\tremaining: 1.15s\n",
            "972:\tlearn: 0.0000081\ttotal: 40.1s\tremaining: 1.11s\n",
            "973:\tlearn: 0.0000081\ttotal: 40.1s\tremaining: 1.07s\n",
            "974:\tlearn: 0.0000081\ttotal: 40.2s\tremaining: 1.03s\n",
            "975:\tlearn: 0.0000081\ttotal: 40.2s\tremaining: 989ms\n",
            "976:\tlearn: 0.0000081\ttotal: 40.3s\tremaining: 948ms\n",
            "977:\tlearn: 0.0000081\ttotal: 40.3s\tremaining: 906ms\n",
            "978:\tlearn: 0.0000081\ttotal: 40.3s\tremaining: 865ms\n",
            "979:\tlearn: 0.0000081\ttotal: 40.4s\tremaining: 823ms\n",
            "980:\tlearn: 0.0000080\ttotal: 40.4s\tremaining: 782ms\n",
            "981:\tlearn: 0.0000080\ttotal: 40.4s\tremaining: 741ms\n",
            "982:\tlearn: 0.0000080\ttotal: 40.5s\tremaining: 700ms\n",
            "983:\tlearn: 0.0000080\ttotal: 40.5s\tremaining: 658ms\n",
            "984:\tlearn: 0.0000080\ttotal: 40.5s\tremaining: 617ms\n",
            "985:\tlearn: 0.0000080\ttotal: 40.6s\tremaining: 576ms\n",
            "986:\tlearn: 0.0000080\ttotal: 40.6s\tremaining: 535ms\n",
            "987:\tlearn: 0.0000080\ttotal: 40.7s\tremaining: 494ms\n",
            "988:\tlearn: 0.0000079\ttotal: 40.7s\tremaining: 453ms\n",
            "989:\tlearn: 0.0000079\ttotal: 40.7s\tremaining: 412ms\n",
            "990:\tlearn: 0.0000079\ttotal: 40.8s\tremaining: 370ms\n",
            "991:\tlearn: 0.0000079\ttotal: 40.8s\tremaining: 329ms\n",
            "992:\tlearn: 0.0000079\ttotal: 40.9s\tremaining: 288ms\n",
            "993:\tlearn: 0.0000079\ttotal: 40.9s\tremaining: 247ms\n",
            "994:\tlearn: 0.0000079\ttotal: 40.9s\tremaining: 206ms\n",
            "995:\tlearn: 0.0000079\ttotal: 41s\tremaining: 165ms\n",
            "996:\tlearn: 0.0000079\ttotal: 41s\tremaining: 123ms\n",
            "997:\tlearn: 0.0000079\ttotal: 41.1s\tremaining: 82.3ms\n",
            "998:\tlearn: 0.0000078\ttotal: 41.1s\tremaining: 41.2ms\n",
            "999:\tlearn: 0.0000078\ttotal: 41.2s\tremaining: 0us\n",
            "Accuracy of Model: 0.9999705665930831\n",
            "Precision of Model: 0.9999705764009493\n",
            "Recall of Model: 0.9999705665930831\n",
            "F1-score of Model: 0.999970567253579\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       1.00      1.00      1.00      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      1.00      1.00      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           1.00     33975\n",
            "   macro avg       1.00      1.00      1.00     33975\n",
            "weighted avg       1.00      1.00      1.00     33975\n",
            "\n",
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  cat_model_stack.sav\n",
            "Accuracy Model:  0.9999705665930831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN8SUGt4lFok",
        "outputId": "f730b95d-6364-43b0-e6f3-e5d75a001c0f"
      },
      "source": [
        "build_model_new (light, X_train_stack , y_train5 , X_test_stack , y_test5)\n",
        "light_train_stack , light_test_stack = save_model_new(light,'light_model_stack', X_train_stack , y_train5 , X_test_stack , y_test5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
            "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
            "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
            "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
            "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
            "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
            "Accuracy of Model: 1.0\n",
            "Precision of Model: 1.0\n",
            "Recall of Model: 1.0\n",
            "F1-score of Model: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       1.00      1.00      1.00      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      1.00      1.00      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           1.00     33975\n",
            "   macro avg       1.00      1.00      1.00     33975\n",
            "weighted avg       1.00      1.00      1.00     33975\n",
            "\n",
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  light_model_stack.sav\n",
            "Accuracy Model:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQvSHCX0Fx_8",
        "outputId": "1007350b-21fb-4c77-a1d6-5d06244ec944"
      },
      "source": [
        "build_model_new (light, X_train_stack , y_train5 , X_test_stack , y_test5)\n",
        "light_train_stack , light_test_stack = save_model_new(light,'light_model_stack', X_train_stack , y_train5 , X_test_stack , y_test5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
            "               colsample_bytree=0.6810492267417421, importance_type='split',\n",
            "               learning_rate=0.1, max_depth=-1, min_child_samples=272,\n",
            "               min_child_weight=1, min_split_gain=0.0, n_estimators=100,\n",
            "               n_jobs=-1, num_leaves=26, objective=None, random_state=None,\n",
            "               reg_alpha=0.1, reg_lambda=0.1, silent=True,\n",
            "               subsample=0.48511105572491486, subsample_for_bin=200000,\n",
            "               subsample_freq=0)\n",
            "Accuracy of Model: 0.9999705665930831\n",
            "Precision of Model: 0.9999705764009493\n",
            "Recall of Model: 0.9999705665930831\n",
            "F1-score of Model: 0.999970567253579\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       1.00      1.00      1.00      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      1.00      1.00      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           1.00     33975\n",
            "   macro avg       1.00      1.00      1.00     33975\n",
            "weighted avg       1.00      1.00      1.00     33975\n",
            "\n",
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "Done save model in drive:  light_model_stack.sav\n",
            "Accuracy Model:  0.9999705665930831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVcloohDHuwG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGT0yEaTJ7Wg"
      },
      "source": [
        "### 12. Compare Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnarc0KH0e7n"
      },
      "source": [
        "##### 12.0 Load Model and Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rmzUSus8ke_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9RATLOdpfUY"
      },
      "source": [
        "def load_model_and_report (file, X_traino, X_testo, y_traino, y_testo):\n",
        "    loaded_model = pickle.load(open('./result_model/' + file , 'rb'))\n",
        "    #result = loaded_model.score(X_testo, y_testo)\n",
        "    fiting = loaded_model.fit(X_traino, y_traino)\n",
        "\n",
        "    #-----------------------------------------------------------------------------\n",
        "    train = loaded_model.predict(X_traino)\n",
        "    y_predict = loaded_model.predict(X_testo)\n",
        "    #-----------------------------------------------------------------------------\n",
        "    model_score = loaded_model.score(X_testo,y_testo)\n",
        "    y_true = y_test4\n",
        "    #-----------------------------------------------------------------------------\n",
        "    #report_acc (model_score, y_true, y_predict)\n",
        "    print (file)\n",
        "    print('Accuracy of Model: ' + str(model_score))\n",
        "    precision,recall,fscore,none= precision_recall_fscore_support(y_true, y_predict, average='weighted') \n",
        "    print('Precision of Model: ' + (str(precision)))\n",
        "    print('Recall of Model: ' + (str(recall)))\n",
        "    print('F1-score of Model: ' + (str(fscore)))\n",
        "    print(classification_report(y_true,y_predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyPjloqAZx1H",
        "outputId": "8d2c99e0-fdff-4b8f-ac83-10acfa54c8ee"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Newi/2021Project3/result_model\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3/result_model\n",
            "cat_model_new.sav     light_model.sav\t     xg_model_new.sav\n",
            "cat_model_para.sav    light_model_stack.sav  xg_model_para.sav\n",
            "cat_model.sav\t      rf_model_new.sav\t     xg_model.sav\n",
            "cat_model_stack.sav   rf_model_para.sav      xg_model_stack.sav\n",
            "light_model_new.sav   rf_model.sav\n",
            "light_model_para.sav  rf_model_stack.sav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXE7g46KZ-qm",
        "outputId": "ebd04f69-bb54-4bd2-b913-cd46177ebd7d"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Newi/2021Project3/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Newi/2021Project3\n",
            "01_data_cleanup.ipynb\t\t      catboost_info  models5\n",
            "02_data_analysis.ipynb\t\t      Data\t     processed\n",
            "03_ml_classifier.ipynb\t\t      Dataupdate     result_model\n",
            "04_dl_anomaly_detection.ipynb\t      mlids\t     tmp\n",
            "05_dl_classifier.ipynb\t\t      models\n",
            "6_binary_classifier_comparison.ipynb  models4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDdmmlz4yb_y"
      },
      "source": [
        "##### 12.1 Models Param:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPKI4FqgpOJy",
        "outputId": "72f8de21-4feb-413e-ca03-70eed6417439"
      },
      "source": [
        "load_model_and_report('rf_model_para.sav', X_train5, X_test5, y_train5, y_test5)\n",
        "load_model_and_report('xg_model_para.sav', X_train5, X_test5, y_train5, y_test5)\n",
        "load_model_and_report('cat_model_para.sav', X_train5, X_test5, y_train5, y_test5)\n",
        "load_model_and_report('light_model_para.sav', X_train5, X_test5, y_train5, y_test5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rf_model_para.sav\n",
            "Accuracy of Model: 0.9860485651214128\n",
            "Precision of Model: 0.9879521287749679\n",
            "Recall of Model: 0.9860485651214128\n",
            "F1-score of Model: 0.9859596452146835\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       0.86      1.00      0.93      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      0.84      0.91      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           0.99     33975\n",
            "   macro avg       0.99      0.99      0.99     33975\n",
            "weighted avg       0.99      0.99      0.99     33975\n",
            "\n",
            "xg_model_para.sav\n",
            "Accuracy of Model: 0.9859896983075791\n",
            "Precision of Model: 0.9878933011795313\n",
            "Recall of Model: 0.9859896983075791\n",
            "F1-score of Model: 0.9859007810403266\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       0.86      1.00      0.93      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      0.84      0.91      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           0.99     33975\n",
            "   macro avg       0.99      0.99      0.99     33975\n",
            "weighted avg       0.99      0.99      0.99     33975\n",
            "\n",
            "cat_model_para.sav\n",
            "Accuracy of Model: 0.986019131714496\n",
            "Precision of Model: 0.9879227051759173\n",
            "Recall of Model: 0.986019131714496\n",
            "F1-score of Model: 0.9859302124682627\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       0.86      1.00      0.93      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      0.84      0.91      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           0.99     33975\n",
            "   macro avg       0.99      0.99      0.99     33975\n",
            "weighted avg       0.99      0.99      0.99     33975\n",
            "\n",
            "light_model_para.sav\n",
            "Accuracy of Model: 0.9999411331861663\n",
            "Precision of Model: 0.999941152801899\n",
            "Recall of Model: 0.9999411331861663\n",
            "F1-score of Model: 0.9999411309225273\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       1.00      1.00      1.00      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      1.00      1.00      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           1.00     33975\n",
            "   macro avg       1.00      1.00      1.00     33975\n",
            "weighted avg       1.00      1.00      1.00     33975\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kOXwchaylzp"
      },
      "source": [
        "##### 12.2 Models Stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS6UKP4DkhYt",
        "outputId": "694cb9d7-e697-4527-caaa-3d7253823e59"
      },
      "source": [
        "load_model_and_report('rf_model_stack.sav', X_train_stack, X_test_stack, y_train5, y_test5)\n",
        "load_model_and_report('xg_model_stack.sav', X_train_stack, X_test_stack, y_train5, y_test5)\n",
        "load_model_and_report('cat_model_stack.sav', X_train_stack, X_test_stack, y_train5, y_test5)\n",
        "load_model_and_report('light_model_stack.sav', X_train_stack, X_test_stack, y_train5, y_test5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rf_model_stack.sav\n",
            "Accuracy of Model: 0.9999705665930831\n",
            "Precision of Model: 0.9999705764009493\n",
            "Recall of Model: 0.9999705665930831\n",
            "F1-score of Model: 0.999970567253579\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       1.00      1.00      1.00      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      1.00      1.00      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           1.00     33975\n",
            "   macro avg       1.00      1.00      1.00     33975\n",
            "weighted avg       1.00      1.00      1.00     33975\n",
            "\n",
            "xg_model_stack.sav\n",
            "Accuracy of Model: 1.0\n",
            "Precision of Model: 1.0\n",
            "Recall of Model: 1.0\n",
            "F1-score of Model: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       1.00      1.00      1.00      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      1.00      1.00      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           1.00     33975\n",
            "   macro avg       1.00      1.00      1.00     33975\n",
            "weighted avg       1.00      1.00      1.00     33975\n",
            "\n",
            "cat_model_stack.sav\n",
            "Accuracy of Model: 0.9999705665930831\n",
            "Precision of Model: 0.9999705764009493\n",
            "Recall of Model: 0.9999705665930831\n",
            "F1-score of Model: 0.999970567253579\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       1.00      1.00      1.00      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      1.00      1.00      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           1.00     33975\n",
            "   macro avg       1.00      1.00      1.00     33975\n",
            "weighted avg       1.00      1.00      1.00     33975\n",
            "\n",
            "light_model_stack.sav\n",
            "Accuracy of Model: 1.0\n",
            "Precision of Model: 1.0\n",
            "Recall of Model: 1.0\n",
            "F1-score of Model: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4107\n",
            "           1       1.00      1.00      1.00      3000\n",
            "           4       1.00      1.00      1.00      3000\n",
            "           6       1.00      1.00      1.00      2993\n",
            "           7       1.00      1.00      1.00      2881\n",
            "           8       1.00      1.00      1.00      3000\n",
            "           9       1.00      1.00      1.00      3000\n",
            "          10       1.00      1.00      1.00      3000\n",
            "          11       1.00      1.00      1.00      2994\n",
            "          12       1.00      1.00      1.00      3000\n",
            "          14       1.00      1.00      1.00      3000\n",
            "\n",
            "    accuracy                           1.00     33975\n",
            "   macro avg       1.00      1.00      1.00     33975\n",
            "weighted avg       1.00      1.00      1.00     33975\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmOQWiIvzIlP"
      },
      "source": [
        "##### 12.3 Models New - After Features Selection and SMOTE ENN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjGuZH45n8lP",
        "outputId": "94c24639-70fa-4120-e882-f486f7bcbd33"
      },
      "source": [
        "load_model_and_report('rf_model_new.sav', X_train4, X_test4, y_train4, y_test4)\n",
        "load_model_and_report('xg_model_new.sav', X_train4, X_test4, y_train4, y_test4)\n",
        "load_model_and_report('cat_model_new.sav', X_train4, X_test4, y_train4, y_test4)\n",
        "load_model_and_report('light_model_new.sav', X_train4, X_test4, y_train4, y_test4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rf_model_new.sav\n",
            "Accuracy of Model: 0.9733184460786201\n",
            "Precision of Model: 0.968698323661959\n",
            "Recall of Model: 0.9733184460786201\n",
            "F1-score of Model: 0.9696937480917386\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99     21576\n",
            "           1       0.92      0.96      0.94       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           4       0.92      0.91      0.91      1098\n",
            "           5       0.75      1.00      0.86         3\n",
            "           6       0.98      0.97      0.97       922\n",
            "           7       0.98      0.98      0.98        66\n",
            "           8       0.99      0.99      0.99       739\n",
            "           9       0.78      0.48      0.59       224\n",
            "          10       1.00      0.94      0.97        18\n",
            "          11       0.70      0.90      0.79       309\n",
            "          12       0.35      0.08      0.13       259\n",
            "          14       1.00      1.00      1.00       300\n",
            "\n",
            "    accuracy                           0.97     25973\n",
            "   macro avg       0.80      0.79      0.78     25973\n",
            "weighted avg       0.97      0.97      0.97     25973\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xg_model_new.sav\n",
            "Accuracy of Model: 0.9595734031494244\n",
            "Precision of Model: 0.9517398530579302\n",
            "Recall of Model: 0.9595734031494244\n",
            "F1-score of Model: 0.9520262752108889\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     21576\n",
            "           1       1.00      0.49      0.66       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           4       0.89      0.95      0.92      1098\n",
            "           5       0.75      1.00      0.86         3\n",
            "           6       0.94      0.98      0.96       922\n",
            "           7       1.00      0.14      0.24        66\n",
            "           8       0.98      0.94      0.96       739\n",
            "           9       0.76      0.43      0.55       224\n",
            "          10       1.00      0.67      0.80        18\n",
            "          11       0.71      0.80      0.75       309\n",
            "          12       0.14      0.00      0.01       259\n",
            "          14       1.00      0.98      0.99       300\n",
            "\n",
            "    accuracy                           0.96     25973\n",
            "   macro avg       0.78      0.64      0.67     25973\n",
            "weighted avg       0.95      0.96      0.95     25973\n",
            "\n",
            "cat_model_new.sav\n",
            "Accuracy of Model: 0.9777076194509683\n",
            "Precision of Model: 0.974293595107081\n",
            "Recall of Model: 0.9777076194509683\n",
            "F1-score of Model: 0.9739388418021312\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     21576\n",
            "           1       0.91      0.99      0.95       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           4       0.92      0.97      0.94      1098\n",
            "           5       0.75      1.00      0.86         3\n",
            "           6       0.99      0.98      0.98       922\n",
            "           7       0.97      0.98      0.98        66\n",
            "           8       0.99      1.00      1.00       739\n",
            "           9       0.79      0.48      0.60       224\n",
            "          10       0.95      1.00      0.97        18\n",
            "          11       0.71      0.91      0.79       309\n",
            "          12       0.51      0.08      0.15       259\n",
            "          14       1.00      1.00      1.00       300\n",
            "\n",
            "    accuracy                           0.98     25973\n",
            "   macro avg       0.81      0.80      0.79     25973\n",
            "weighted avg       0.97      0.98      0.97     25973\n",
            "\n",
            "light_model_new.sav\n",
            "Accuracy of Model: 0.8565433334616718\n",
            "Precision of Model: 0.8485782065202214\n",
            "Recall of Model: 0.8565433334616718\n",
            "F1-score of Model: 0.8495370007548657\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.92     21576\n",
            "           1       0.28      0.50      0.36       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           3       0.00      0.00      0.00         0\n",
            "           4       0.66      0.92      0.77      1098\n",
            "           5       0.00      0.00      0.00         3\n",
            "           6       0.64      0.37      0.47       922\n",
            "           7       0.00      0.00      0.00        66\n",
            "           8       0.82      0.75      0.78       739\n",
            "           9       0.00      0.00      0.00       224\n",
            "          10       0.00      0.00      0.00        18\n",
            "          11       0.00      0.00      0.00       309\n",
            "          12       0.00      0.00      0.00       259\n",
            "          14       0.69      0.48      0.57       300\n",
            "\n",
            "    accuracy                           0.86     25973\n",
            "   macro avg       0.29      0.28      0.28     25973\n",
            "weighted avg       0.85      0.86      0.85     25973\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQEp4kcX03Gw"
      },
      "source": [
        "##### 12.4 Model - Without Features Selection and SMOTE ENN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49VTyNJqn8jG",
        "outputId": "b19667b1-9283-4f08-b611-12c872436fe1"
      },
      "source": [
        "load_model_and_report('rf_model.sav', X_train3, X_test3, y_train3, y_test3)\n",
        "load_model_and_report('xg_model.sav', X_train3, X_test3, y_train3, y_test3)\n",
        "load_model_and_report('cat_model.sav', X_train3, X_test3, y_train3, y_test3)\n",
        "load_model_and_report('light_model.sav', X_train3, X_test3, y_train3, y_test3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rf_model.sav\n",
            "Accuracy of Model: 0.9822122973857468\n",
            "Precision of Model: 0.9777006737780845\n",
            "Recall of Model: 0.9822122973857468\n",
            "F1-score of Model: 0.9788298310442757\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     21576\n",
            "           1       0.98      1.00      0.99       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           4       1.00      1.00      1.00      1098\n",
            "           5       0.75      1.00      0.86         3\n",
            "           6       1.00      0.99      0.99       922\n",
            "           7       0.98      0.98      0.98        66\n",
            "           8       1.00      1.00      1.00       739\n",
            "           9       0.78      0.48      0.59       224\n",
            "          10       1.00      0.94      0.97        18\n",
            "          11       0.70      0.90      0.79       309\n",
            "          12       0.33      0.09      0.14       259\n",
            "          14       1.00      1.00      1.00       300\n",
            "\n",
            "    accuracy                           0.98     25973\n",
            "   macro avg       0.81      0.80      0.79     25973\n",
            "weighted avg       0.98      0.98      0.98     25973\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "xg_model.sav\n",
            "Accuracy of Model: 0.9727024217456589\n",
            "Precision of Model: 0.963110078914485\n",
            "Recall of Model: 0.9727024217456589\n",
            "F1-score of Model: 0.9660995799292463\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99     21576\n",
            "           1       1.00      0.48      0.65       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           4       1.00      1.00      1.00      1098\n",
            "           5       0.75      1.00      0.86         3\n",
            "           6       0.99      0.98      0.99       922\n",
            "           7       0.95      0.94      0.95        66\n",
            "           8       0.99      0.99      0.99       739\n",
            "           9       0.66      0.57      0.61       224\n",
            "          10       1.00      0.67      0.80        18\n",
            "          11       0.72      0.79      0.75       309\n",
            "          12       0.00      0.00      0.00       259\n",
            "          14       0.99      0.99      0.99       300\n",
            "\n",
            "    accuracy                           0.97     25973\n",
            "   macro avg       0.77      0.72      0.74     25973\n",
            "weighted avg       0.96      0.97      0.97     25973\n",
            "\n",
            "Learning rate set to 0.100269\n",
            "0:\tlearn: 1.5693948\ttotal: 302ms\tremaining: 5m 1s\n",
            "1:\tlearn: 1.2611272\ttotal: 434ms\tremaining: 3m 36s\n",
            "2:\tlearn: 1.0635590\ttotal: 548ms\tremaining: 3m 2s\n",
            "3:\tlearn: 0.9165500\ttotal: 661ms\tremaining: 2m 44s\n",
            "4:\tlearn: 0.7930125\ttotal: 778ms\tremaining: 2m 34s\n",
            "5:\tlearn: 0.6947223\ttotal: 895ms\tremaining: 2m 28s\n",
            "6:\tlearn: 0.6170233\ttotal: 1.02s\tremaining: 2m 24s\n",
            "7:\tlearn: 0.5525697\ttotal: 1.14s\tremaining: 2m 21s\n",
            "8:\tlearn: 0.4979633\ttotal: 1.25s\tremaining: 2m 18s\n",
            "9:\tlearn: 0.4516341\ttotal: 1.37s\tremaining: 2m 15s\n",
            "10:\tlearn: 0.4097827\ttotal: 1.49s\tremaining: 2m 14s\n",
            "11:\tlearn: 0.3751337\ttotal: 1.61s\tremaining: 2m 12s\n",
            "12:\tlearn: 0.3439940\ttotal: 1.73s\tremaining: 2m 11s\n",
            "13:\tlearn: 0.3180090\ttotal: 1.85s\tremaining: 2m 10s\n",
            "14:\tlearn: 0.2933681\ttotal: 1.98s\tremaining: 2m 9s\n",
            "15:\tlearn: 0.2731135\ttotal: 2.1s\tremaining: 2m 9s\n",
            "16:\tlearn: 0.2533203\ttotal: 2.22s\tremaining: 2m 8s\n",
            "17:\tlearn: 0.2365758\ttotal: 2.34s\tremaining: 2m 7s\n",
            "18:\tlearn: 0.2223525\ttotal: 2.46s\tremaining: 2m 6s\n",
            "19:\tlearn: 0.2091342\ttotal: 2.57s\tremaining: 2m 6s\n",
            "20:\tlearn: 0.1975005\ttotal: 2.7s\tremaining: 2m 5s\n",
            "21:\tlearn: 0.1873058\ttotal: 2.82s\tremaining: 2m 5s\n",
            "22:\tlearn: 0.1770506\ttotal: 2.95s\tremaining: 2m 5s\n",
            "23:\tlearn: 0.1685618\ttotal: 3.07s\tremaining: 2m 4s\n",
            "24:\tlearn: 0.1614607\ttotal: 3.19s\tremaining: 2m 4s\n",
            "25:\tlearn: 0.1549871\ttotal: 3.31s\tremaining: 2m 4s\n",
            "26:\tlearn: 0.1486726\ttotal: 3.44s\tremaining: 2m 3s\n",
            "27:\tlearn: 0.1419576\ttotal: 3.55s\tremaining: 2m 3s\n",
            "28:\tlearn: 0.1360255\ttotal: 3.67s\tremaining: 2m 3s\n",
            "29:\tlearn: 0.1313139\ttotal: 3.8s\tremaining: 2m 2s\n",
            "30:\tlearn: 0.1269563\ttotal: 3.91s\tremaining: 2m 2s\n",
            "31:\tlearn: 0.1230126\ttotal: 4.03s\tremaining: 2m 1s\n",
            "32:\tlearn: 0.1198870\ttotal: 4.14s\tremaining: 2m 1s\n",
            "33:\tlearn: 0.1167235\ttotal: 4.26s\tremaining: 2m 1s\n",
            "34:\tlearn: 0.1139142\ttotal: 4.38s\tremaining: 2m\n",
            "35:\tlearn: 0.1114318\ttotal: 4.49s\tremaining: 2m\n",
            "36:\tlearn: 0.1088264\ttotal: 4.61s\tremaining: 1m 59s\n",
            "37:\tlearn: 0.1060023\ttotal: 4.73s\tremaining: 1m 59s\n",
            "38:\tlearn: 0.1034412\ttotal: 4.85s\tremaining: 1m 59s\n",
            "39:\tlearn: 0.1019235\ttotal: 4.98s\tremaining: 1m 59s\n",
            "40:\tlearn: 0.1003644\ttotal: 5.1s\tremaining: 1m 59s\n",
            "41:\tlearn: 0.0987936\ttotal: 5.22s\tremaining: 1m 59s\n",
            "42:\tlearn: 0.0972644\ttotal: 5.33s\tremaining: 1m 58s\n",
            "43:\tlearn: 0.0953976\ttotal: 5.45s\tremaining: 1m 58s\n",
            "44:\tlearn: 0.0940168\ttotal: 5.57s\tremaining: 1m 58s\n",
            "45:\tlearn: 0.0925251\ttotal: 5.68s\tremaining: 1m 57s\n",
            "46:\tlearn: 0.0914805\ttotal: 5.8s\tremaining: 1m 57s\n",
            "47:\tlearn: 0.0903986\ttotal: 5.92s\tremaining: 1m 57s\n",
            "48:\tlearn: 0.0894463\ttotal: 6.04s\tremaining: 1m 57s\n",
            "49:\tlearn: 0.0884555\ttotal: 6.16s\tremaining: 1m 57s\n",
            "50:\tlearn: 0.0875575\ttotal: 6.28s\tremaining: 1m 56s\n",
            "51:\tlearn: 0.0863762\ttotal: 6.39s\tremaining: 1m 56s\n",
            "52:\tlearn: 0.0854340\ttotal: 6.51s\tremaining: 1m 56s\n",
            "53:\tlearn: 0.0847399\ttotal: 6.62s\tremaining: 1m 56s\n",
            "54:\tlearn: 0.0839033\ttotal: 6.74s\tremaining: 1m 55s\n",
            "55:\tlearn: 0.0832636\ttotal: 6.85s\tremaining: 1m 55s\n",
            "56:\tlearn: 0.0826277\ttotal: 6.97s\tremaining: 1m 55s\n",
            "57:\tlearn: 0.0818338\ttotal: 7.08s\tremaining: 1m 55s\n",
            "58:\tlearn: 0.0812533\ttotal: 7.2s\tremaining: 1m 54s\n",
            "59:\tlearn: 0.0806993\ttotal: 7.32s\tremaining: 1m 54s\n",
            "60:\tlearn: 0.0799619\ttotal: 7.43s\tremaining: 1m 54s\n",
            "61:\tlearn: 0.0794373\ttotal: 7.55s\tremaining: 1m 54s\n",
            "62:\tlearn: 0.0788244\ttotal: 7.67s\tremaining: 1m 54s\n",
            "63:\tlearn: 0.0785006\ttotal: 7.8s\tremaining: 1m 54s\n",
            "64:\tlearn: 0.0782283\ttotal: 7.92s\tremaining: 1m 53s\n",
            "65:\tlearn: 0.0776909\ttotal: 8.04s\tremaining: 1m 53s\n",
            "66:\tlearn: 0.0773367\ttotal: 8.17s\tremaining: 1m 53s\n",
            "67:\tlearn: 0.0767838\ttotal: 8.3s\tremaining: 1m 53s\n",
            "68:\tlearn: 0.0763583\ttotal: 8.44s\tremaining: 1m 53s\n",
            "69:\tlearn: 0.0758787\ttotal: 8.56s\tremaining: 1m 53s\n",
            "70:\tlearn: 0.0756466\ttotal: 8.68s\tremaining: 1m 53s\n",
            "71:\tlearn: 0.0754450\ttotal: 8.81s\tremaining: 1m 53s\n",
            "72:\tlearn: 0.0750737\ttotal: 8.94s\tremaining: 1m 53s\n",
            "73:\tlearn: 0.0748601\ttotal: 9.06s\tremaining: 1m 53s\n",
            "74:\tlearn: 0.0746337\ttotal: 9.18s\tremaining: 1m 53s\n",
            "75:\tlearn: 0.0743869\ttotal: 9.31s\tremaining: 1m 53s\n",
            "76:\tlearn: 0.0739774\ttotal: 9.43s\tremaining: 1m 52s\n",
            "77:\tlearn: 0.0738179\ttotal: 9.55s\tremaining: 1m 52s\n",
            "78:\tlearn: 0.0737048\ttotal: 9.67s\tremaining: 1m 52s\n",
            "79:\tlearn: 0.0735103\ttotal: 9.79s\tremaining: 1m 52s\n",
            "80:\tlearn: 0.0731586\ttotal: 9.91s\tremaining: 1m 52s\n",
            "81:\tlearn: 0.0729757\ttotal: 10s\tremaining: 1m 52s\n",
            "82:\tlearn: 0.0727132\ttotal: 10.2s\tremaining: 1m 52s\n",
            "83:\tlearn: 0.0724300\ttotal: 10.3s\tremaining: 1m 52s\n",
            "84:\tlearn: 0.0722522\ttotal: 10.4s\tremaining: 1m 51s\n",
            "85:\tlearn: 0.0719397\ttotal: 10.5s\tremaining: 1m 51s\n",
            "86:\tlearn: 0.0717413\ttotal: 10.6s\tremaining: 1m 51s\n",
            "87:\tlearn: 0.0716238\ttotal: 10.8s\tremaining: 1m 51s\n",
            "88:\tlearn: 0.0713996\ttotal: 10.9s\tremaining: 1m 51s\n",
            "89:\tlearn: 0.0712130\ttotal: 11s\tremaining: 1m 51s\n",
            "90:\tlearn: 0.0711562\ttotal: 11.1s\tremaining: 1m 50s\n",
            "91:\tlearn: 0.0710013\ttotal: 11.2s\tremaining: 1m 50s\n",
            "92:\tlearn: 0.0708827\ttotal: 11.4s\tremaining: 1m 50s\n",
            "93:\tlearn: 0.0707196\ttotal: 11.5s\tremaining: 1m 50s\n",
            "94:\tlearn: 0.0706435\ttotal: 11.6s\tremaining: 1m 50s\n",
            "95:\tlearn: 0.0705451\ttotal: 11.7s\tremaining: 1m 50s\n",
            "96:\tlearn: 0.0704288\ttotal: 11.8s\tremaining: 1m 50s\n",
            "97:\tlearn: 0.0703017\ttotal: 11.9s\tremaining: 1m 49s\n",
            "98:\tlearn: 0.0701940\ttotal: 12.1s\tremaining: 1m 49s\n",
            "99:\tlearn: 0.0700899\ttotal: 12.2s\tremaining: 1m 49s\n",
            "100:\tlearn: 0.0699920\ttotal: 12.3s\tremaining: 1m 49s\n",
            "101:\tlearn: 0.0698693\ttotal: 12.4s\tremaining: 1m 49s\n",
            "102:\tlearn: 0.0697608\ttotal: 12.5s\tremaining: 1m 49s\n",
            "103:\tlearn: 0.0696159\ttotal: 12.6s\tremaining: 1m 48s\n",
            "104:\tlearn: 0.0695332\ttotal: 12.8s\tremaining: 1m 48s\n",
            "105:\tlearn: 0.0693767\ttotal: 12.9s\tremaining: 1m 48s\n",
            "106:\tlearn: 0.0692751\ttotal: 13s\tremaining: 1m 48s\n",
            "107:\tlearn: 0.0691894\ttotal: 13.1s\tremaining: 1m 48s\n",
            "108:\tlearn: 0.0690024\ttotal: 13.2s\tremaining: 1m 48s\n",
            "109:\tlearn: 0.0687993\ttotal: 13.3s\tremaining: 1m 47s\n",
            "110:\tlearn: 0.0686550\ttotal: 13.5s\tremaining: 1m 47s\n",
            "111:\tlearn: 0.0685172\ttotal: 13.6s\tremaining: 1m 47s\n",
            "112:\tlearn: 0.0683958\ttotal: 13.7s\tremaining: 1m 47s\n",
            "113:\tlearn: 0.0682698\ttotal: 13.8s\tremaining: 1m 47s\n",
            "114:\tlearn: 0.0681934\ttotal: 13.9s\tremaining: 1m 47s\n",
            "115:\tlearn: 0.0681415\ttotal: 14.1s\tremaining: 1m 47s\n",
            "116:\tlearn: 0.0679925\ttotal: 14.2s\tremaining: 1m 47s\n",
            "117:\tlearn: 0.0678766\ttotal: 14.3s\tremaining: 1m 47s\n",
            "118:\tlearn: 0.0678461\ttotal: 14.4s\tremaining: 1m 46s\n",
            "119:\tlearn: 0.0677828\ttotal: 14.6s\tremaining: 1m 46s\n",
            "120:\tlearn: 0.0675968\ttotal: 14.7s\tremaining: 1m 46s\n",
            "121:\tlearn: 0.0675523\ttotal: 14.8s\tremaining: 1m 46s\n",
            "122:\tlearn: 0.0675051\ttotal: 14.9s\tremaining: 1m 46s\n",
            "123:\tlearn: 0.0674660\ttotal: 15.1s\tremaining: 1m 46s\n",
            "124:\tlearn: 0.0673565\ttotal: 15.2s\tremaining: 1m 46s\n",
            "125:\tlearn: 0.0673241\ttotal: 15.3s\tremaining: 1m 46s\n",
            "126:\tlearn: 0.0672448\ttotal: 15.4s\tremaining: 1m 46s\n",
            "127:\tlearn: 0.0671398\ttotal: 15.6s\tremaining: 1m 45s\n",
            "128:\tlearn: 0.0670858\ttotal: 15.7s\tremaining: 1m 45s\n",
            "129:\tlearn: 0.0670403\ttotal: 15.8s\tremaining: 1m 45s\n",
            "130:\tlearn: 0.0669666\ttotal: 15.9s\tremaining: 1m 45s\n",
            "131:\tlearn: 0.0669444\ttotal: 16.1s\tremaining: 1m 45s\n",
            "132:\tlearn: 0.0668906\ttotal: 16.2s\tremaining: 1m 45s\n",
            "133:\tlearn: 0.0668462\ttotal: 16.3s\tremaining: 1m 45s\n",
            "134:\tlearn: 0.0667266\ttotal: 16.4s\tremaining: 1m 45s\n",
            "135:\tlearn: 0.0666737\ttotal: 16.6s\tremaining: 1m 45s\n",
            "136:\tlearn: 0.0666243\ttotal: 16.7s\tremaining: 1m 45s\n",
            "137:\tlearn: 0.0665629\ttotal: 16.8s\tremaining: 1m 44s\n",
            "138:\tlearn: 0.0665171\ttotal: 16.9s\tremaining: 1m 44s\n",
            "139:\tlearn: 0.0664866\ttotal: 17.1s\tremaining: 1m 44s\n",
            "140:\tlearn: 0.0664517\ttotal: 17.2s\tremaining: 1m 44s\n",
            "141:\tlearn: 0.0662318\ttotal: 17.3s\tremaining: 1m 44s\n",
            "142:\tlearn: 0.0662062\ttotal: 17.4s\tremaining: 1m 44s\n",
            "143:\tlearn: 0.0661530\ttotal: 17.5s\tremaining: 1m 44s\n",
            "144:\tlearn: 0.0660978\ttotal: 17.7s\tremaining: 1m 44s\n",
            "145:\tlearn: 0.0660692\ttotal: 17.8s\tremaining: 1m 44s\n",
            "146:\tlearn: 0.0660086\ttotal: 17.9s\tremaining: 1m 43s\n",
            "147:\tlearn: 0.0659399\ttotal: 18s\tremaining: 1m 43s\n",
            "148:\tlearn: 0.0658671\ttotal: 18.2s\tremaining: 1m 43s\n",
            "149:\tlearn: 0.0657484\ttotal: 18.3s\tremaining: 1m 43s\n",
            "150:\tlearn: 0.0656735\ttotal: 18.4s\tremaining: 1m 43s\n",
            "151:\tlearn: 0.0656130\ttotal: 18.5s\tremaining: 1m 43s\n",
            "152:\tlearn: 0.0655678\ttotal: 18.6s\tremaining: 1m 43s\n",
            "153:\tlearn: 0.0655130\ttotal: 18.8s\tremaining: 1m 43s\n",
            "154:\tlearn: 0.0654664\ttotal: 18.9s\tremaining: 1m 42s\n",
            "155:\tlearn: 0.0653418\ttotal: 19s\tremaining: 1m 42s\n",
            "156:\tlearn: 0.0652875\ttotal: 19.1s\tremaining: 1m 42s\n",
            "157:\tlearn: 0.0652012\ttotal: 19.2s\tremaining: 1m 42s\n",
            "158:\tlearn: 0.0651439\ttotal: 19.4s\tremaining: 1m 42s\n",
            "159:\tlearn: 0.0651051\ttotal: 19.5s\tremaining: 1m 42s\n",
            "160:\tlearn: 0.0650711\ttotal: 19.6s\tremaining: 1m 42s\n",
            "161:\tlearn: 0.0650089\ttotal: 19.7s\tremaining: 1m 41s\n",
            "162:\tlearn: 0.0649231\ttotal: 19.8s\tremaining: 1m 41s\n",
            "163:\tlearn: 0.0648039\ttotal: 19.9s\tremaining: 1m 41s\n",
            "164:\tlearn: 0.0647840\ttotal: 20s\tremaining: 1m 41s\n",
            "165:\tlearn: 0.0646876\ttotal: 20.2s\tremaining: 1m 41s\n",
            "166:\tlearn: 0.0646416\ttotal: 20.3s\tremaining: 1m 41s\n",
            "167:\tlearn: 0.0646127\ttotal: 20.4s\tremaining: 1m 40s\n",
            "168:\tlearn: 0.0645581\ttotal: 20.5s\tremaining: 1m 40s\n",
            "169:\tlearn: 0.0644925\ttotal: 20.6s\tremaining: 1m 40s\n",
            "170:\tlearn: 0.0644685\ttotal: 20.7s\tremaining: 1m 40s\n",
            "171:\tlearn: 0.0644096\ttotal: 20.8s\tremaining: 1m 40s\n",
            "172:\tlearn: 0.0643873\ttotal: 20.9s\tremaining: 1m 40s\n",
            "173:\tlearn: 0.0643234\ttotal: 21.1s\tremaining: 1m 39s\n",
            "174:\tlearn: 0.0643121\ttotal: 21.2s\tremaining: 1m 39s\n",
            "175:\tlearn: 0.0642668\ttotal: 21.3s\tremaining: 1m 39s\n",
            "176:\tlearn: 0.0642078\ttotal: 21.4s\tremaining: 1m 39s\n",
            "177:\tlearn: 0.0641955\ttotal: 21.5s\tremaining: 1m 39s\n",
            "178:\tlearn: 0.0641340\ttotal: 21.6s\tremaining: 1m 39s\n",
            "179:\tlearn: 0.0640933\ttotal: 21.7s\tremaining: 1m 39s\n",
            "180:\tlearn: 0.0640677\ttotal: 21.9s\tremaining: 1m 38s\n",
            "181:\tlearn: 0.0640551\ttotal: 22s\tremaining: 1m 38s\n",
            "182:\tlearn: 0.0639964\ttotal: 22.1s\tremaining: 1m 38s\n",
            "183:\tlearn: 0.0639843\ttotal: 22.2s\tremaining: 1m 38s\n",
            "184:\tlearn: 0.0639330\ttotal: 22.3s\tremaining: 1m 38s\n",
            "185:\tlearn: 0.0638490\ttotal: 22.4s\tremaining: 1m 38s\n",
            "186:\tlearn: 0.0638131\ttotal: 22.5s\tremaining: 1m 37s\n",
            "187:\tlearn: 0.0637219\ttotal: 22.6s\tremaining: 1m 37s\n",
            "188:\tlearn: 0.0636909\ttotal: 22.7s\tremaining: 1m 37s\n",
            "189:\tlearn: 0.0636814\ttotal: 22.8s\tremaining: 1m 37s\n",
            "190:\tlearn: 0.0636409\ttotal: 23s\tremaining: 1m 37s\n",
            "191:\tlearn: 0.0636249\ttotal: 23.1s\tremaining: 1m 37s\n",
            "192:\tlearn: 0.0636050\ttotal: 23.2s\tremaining: 1m 36s\n",
            "193:\tlearn: 0.0635554\ttotal: 23.3s\tremaining: 1m 36s\n",
            "194:\tlearn: 0.0635220\ttotal: 23.4s\tremaining: 1m 36s\n",
            "195:\tlearn: 0.0634847\ttotal: 23.5s\tremaining: 1m 36s\n",
            "196:\tlearn: 0.0633558\ttotal: 23.6s\tremaining: 1m 36s\n",
            "197:\tlearn: 0.0633349\ttotal: 23.7s\tremaining: 1m 36s\n",
            "198:\tlearn: 0.0633116\ttotal: 23.9s\tremaining: 1m 36s\n",
            "199:\tlearn: 0.0632827\ttotal: 24s\tremaining: 1m 35s\n",
            "200:\tlearn: 0.0632572\ttotal: 24.1s\tremaining: 1m 35s\n",
            "201:\tlearn: 0.0632412\ttotal: 24.2s\tremaining: 1m 35s\n",
            "202:\tlearn: 0.0631955\ttotal: 24.3s\tremaining: 1m 35s\n",
            "203:\tlearn: 0.0631758\ttotal: 24.4s\tremaining: 1m 35s\n",
            "204:\tlearn: 0.0631510\ttotal: 24.5s\tremaining: 1m 35s\n",
            "205:\tlearn: 0.0631007\ttotal: 24.6s\tremaining: 1m 34s\n",
            "206:\tlearn: 0.0630836\ttotal: 24.7s\tremaining: 1m 34s\n",
            "207:\tlearn: 0.0630372\ttotal: 24.8s\tremaining: 1m 34s\n",
            "208:\tlearn: 0.0629899\ttotal: 24.9s\tremaining: 1m 34s\n",
            "209:\tlearn: 0.0629642\ttotal: 25s\tremaining: 1m 34s\n",
            "210:\tlearn: 0.0629425\ttotal: 25.1s\tremaining: 1m 34s\n",
            "211:\tlearn: 0.0629220\ttotal: 25.2s\tremaining: 1m 33s\n",
            "212:\tlearn: 0.0629078\ttotal: 25.3s\tremaining: 1m 33s\n",
            "213:\tlearn: 0.0628927\ttotal: 25.5s\tremaining: 1m 33s\n",
            "214:\tlearn: 0.0628745\ttotal: 25.6s\tremaining: 1m 33s\n",
            "215:\tlearn: 0.0628545\ttotal: 25.7s\tremaining: 1m 33s\n",
            "216:\tlearn: 0.0628476\ttotal: 25.8s\tremaining: 1m 32s\n",
            "217:\tlearn: 0.0628306\ttotal: 25.9s\tremaining: 1m 32s\n",
            "218:\tlearn: 0.0627789\ttotal: 26s\tremaining: 1m 32s\n",
            "219:\tlearn: 0.0627564\ttotal: 26.1s\tremaining: 1m 32s\n",
            "220:\tlearn: 0.0626651\ttotal: 26.2s\tremaining: 1m 32s\n",
            "221:\tlearn: 0.0625868\ttotal: 26.3s\tremaining: 1m 32s\n",
            "222:\tlearn: 0.0625553\ttotal: 26.4s\tremaining: 1m 31s\n",
            "223:\tlearn: 0.0625410\ttotal: 26.5s\tremaining: 1m 31s\n",
            "224:\tlearn: 0.0625171\ttotal: 26.6s\tremaining: 1m 31s\n",
            "225:\tlearn: 0.0625069\ttotal: 26.7s\tremaining: 1m 31s\n",
            "226:\tlearn: 0.0624940\ttotal: 26.8s\tremaining: 1m 31s\n",
            "227:\tlearn: 0.0624788\ttotal: 26.9s\tremaining: 1m 31s\n",
            "228:\tlearn: 0.0624302\ttotal: 27s\tremaining: 1m 30s\n",
            "229:\tlearn: 0.0623879\ttotal: 27.1s\tremaining: 1m 30s\n",
            "230:\tlearn: 0.0623604\ttotal: 27.2s\tremaining: 1m 30s\n",
            "231:\tlearn: 0.0623449\ttotal: 27.3s\tremaining: 1m 30s\n",
            "232:\tlearn: 0.0623043\ttotal: 27.4s\tremaining: 1m 30s\n",
            "233:\tlearn: 0.0622650\ttotal: 27.5s\tremaining: 1m 30s\n",
            "234:\tlearn: 0.0622404\ttotal: 27.6s\tremaining: 1m 29s\n",
            "235:\tlearn: 0.0622162\ttotal: 27.7s\tremaining: 1m 29s\n",
            "236:\tlearn: 0.0621987\ttotal: 27.8s\tremaining: 1m 29s\n",
            "237:\tlearn: 0.0621546\ttotal: 27.9s\tremaining: 1m 29s\n",
            "238:\tlearn: 0.0621397\ttotal: 28s\tremaining: 1m 29s\n",
            "239:\tlearn: 0.0621211\ttotal: 28.1s\tremaining: 1m 29s\n",
            "240:\tlearn: 0.0620760\ttotal: 28.2s\tremaining: 1m 28s\n",
            "241:\tlearn: 0.0619773\ttotal: 28.3s\tremaining: 1m 28s\n",
            "242:\tlearn: 0.0619386\ttotal: 28.4s\tremaining: 1m 28s\n",
            "243:\tlearn: 0.0618837\ttotal: 28.5s\tremaining: 1m 28s\n",
            "244:\tlearn: 0.0618615\ttotal: 28.6s\tremaining: 1m 28s\n",
            "245:\tlearn: 0.0618337\ttotal: 28.7s\tremaining: 1m 28s\n",
            "246:\tlearn: 0.0618131\ttotal: 28.8s\tremaining: 1m 27s\n",
            "247:\tlearn: 0.0618037\ttotal: 28.9s\tremaining: 1m 27s\n",
            "248:\tlearn: 0.0617752\ttotal: 29s\tremaining: 1m 27s\n",
            "249:\tlearn: 0.0617455\ttotal: 29.1s\tremaining: 1m 27s\n",
            "250:\tlearn: 0.0617131\ttotal: 29.2s\tremaining: 1m 27s\n",
            "251:\tlearn: 0.0616830\ttotal: 29.3s\tremaining: 1m 27s\n",
            "252:\tlearn: 0.0616517\ttotal: 29.4s\tremaining: 1m 26s\n",
            "253:\tlearn: 0.0616422\ttotal: 29.5s\tremaining: 1m 26s\n",
            "254:\tlearn: 0.0616269\ttotal: 29.6s\tremaining: 1m 26s\n",
            "255:\tlearn: 0.0616054\ttotal: 29.7s\tremaining: 1m 26s\n",
            "256:\tlearn: 0.0615766\ttotal: 29.8s\tremaining: 1m 26s\n",
            "257:\tlearn: 0.0615396\ttotal: 29.9s\tremaining: 1m 26s\n",
            "258:\tlearn: 0.0615311\ttotal: 30s\tremaining: 1m 25s\n",
            "259:\tlearn: 0.0615039\ttotal: 30.1s\tremaining: 1m 25s\n",
            "260:\tlearn: 0.0614708\ttotal: 30.2s\tremaining: 1m 25s\n",
            "261:\tlearn: 0.0614352\ttotal: 30.3s\tremaining: 1m 25s\n",
            "262:\tlearn: 0.0614186\ttotal: 30.4s\tremaining: 1m 25s\n",
            "263:\tlearn: 0.0612552\ttotal: 30.5s\tremaining: 1m 25s\n",
            "264:\tlearn: 0.0612387\ttotal: 30.6s\tremaining: 1m 24s\n",
            "265:\tlearn: 0.0611881\ttotal: 30.7s\tremaining: 1m 24s\n",
            "266:\tlearn: 0.0611318\ttotal: 30.8s\tremaining: 1m 24s\n",
            "267:\tlearn: 0.0611104\ttotal: 31s\tremaining: 1m 24s\n",
            "268:\tlearn: 0.0610942\ttotal: 31.1s\tremaining: 1m 24s\n",
            "269:\tlearn: 0.0610703\ttotal: 31.2s\tremaining: 1m 24s\n",
            "270:\tlearn: 0.0610649\ttotal: 31.3s\tremaining: 1m 24s\n",
            "271:\tlearn: 0.0610487\ttotal: 31.4s\tremaining: 1m 23s\n",
            "272:\tlearn: 0.0610360\ttotal: 31.5s\tremaining: 1m 23s\n",
            "273:\tlearn: 0.0609935\ttotal: 31.6s\tremaining: 1m 23s\n",
            "274:\tlearn: 0.0609803\ttotal: 31.6s\tremaining: 1m 23s\n",
            "275:\tlearn: 0.0609319\ttotal: 31.8s\tremaining: 1m 23s\n",
            "276:\tlearn: 0.0609218\ttotal: 31.9s\tremaining: 1m 23s\n",
            "277:\tlearn: 0.0608981\ttotal: 32s\tremaining: 1m 22s\n",
            "278:\tlearn: 0.0608788\ttotal: 32.1s\tremaining: 1m 22s\n",
            "279:\tlearn: 0.0608540\ttotal: 32.2s\tremaining: 1m 22s\n",
            "280:\tlearn: 0.0608212\ttotal: 32.3s\tremaining: 1m 22s\n",
            "281:\tlearn: 0.0607845\ttotal: 32.4s\tremaining: 1m 22s\n",
            "282:\tlearn: 0.0607533\ttotal: 32.5s\tremaining: 1m 22s\n",
            "283:\tlearn: 0.0607300\ttotal: 32.6s\tremaining: 1m 22s\n",
            "284:\tlearn: 0.0606425\ttotal: 32.7s\tremaining: 1m 21s\n",
            "285:\tlearn: 0.0606255\ttotal: 32.8s\tremaining: 1m 21s\n",
            "286:\tlearn: 0.0605866\ttotal: 32.9s\tremaining: 1m 21s\n",
            "287:\tlearn: 0.0605381\ttotal: 33s\tremaining: 1m 21s\n",
            "288:\tlearn: 0.0605018\ttotal: 33.1s\tremaining: 1m 21s\n",
            "289:\tlearn: 0.0604761\ttotal: 33.2s\tremaining: 1m 21s\n",
            "290:\tlearn: 0.0604540\ttotal: 33.3s\tremaining: 1m 21s\n",
            "291:\tlearn: 0.0604354\ttotal: 33.4s\tremaining: 1m 20s\n",
            "292:\tlearn: 0.0604028\ttotal: 33.5s\tremaining: 1m 20s\n",
            "293:\tlearn: 0.0603700\ttotal: 33.6s\tremaining: 1m 20s\n",
            "294:\tlearn: 0.0603608\ttotal: 33.7s\tremaining: 1m 20s\n",
            "295:\tlearn: 0.0603453\ttotal: 33.8s\tremaining: 1m 20s\n",
            "296:\tlearn: 0.0603066\ttotal: 33.9s\tremaining: 1m 20s\n",
            "297:\tlearn: 0.0602974\ttotal: 34s\tremaining: 1m 20s\n",
            "298:\tlearn: 0.0602787\ttotal: 34.1s\tremaining: 1m 19s\n",
            "299:\tlearn: 0.0602209\ttotal: 34.2s\tremaining: 1m 19s\n",
            "300:\tlearn: 0.0601995\ttotal: 34.3s\tremaining: 1m 19s\n",
            "301:\tlearn: 0.0601840\ttotal: 34.4s\tremaining: 1m 19s\n",
            "302:\tlearn: 0.0601746\ttotal: 34.5s\tremaining: 1m 19s\n",
            "303:\tlearn: 0.0601302\ttotal: 34.6s\tremaining: 1m 19s\n",
            "304:\tlearn: 0.0601044\ttotal: 34.7s\tremaining: 1m 19s\n",
            "305:\tlearn: 0.0600982\ttotal: 34.8s\tremaining: 1m 18s\n",
            "306:\tlearn: 0.0599653\ttotal: 34.9s\tremaining: 1m 18s\n",
            "307:\tlearn: 0.0599261\ttotal: 35s\tremaining: 1m 18s\n",
            "308:\tlearn: 0.0599184\ttotal: 35.1s\tremaining: 1m 18s\n",
            "309:\tlearn: 0.0599107\ttotal: 35.2s\tremaining: 1m 18s\n",
            "310:\tlearn: 0.0598925\ttotal: 35.3s\tremaining: 1m 18s\n",
            "311:\tlearn: 0.0598588\ttotal: 35.4s\tremaining: 1m 18s\n",
            "312:\tlearn: 0.0598363\ttotal: 35.5s\tremaining: 1m 17s\n",
            "313:\tlearn: 0.0598291\ttotal: 35.6s\tremaining: 1m 17s\n",
            "314:\tlearn: 0.0598108\ttotal: 35.7s\tremaining: 1m 17s\n",
            "315:\tlearn: 0.0597878\ttotal: 35.8s\tremaining: 1m 17s\n",
            "316:\tlearn: 0.0597663\ttotal: 35.9s\tremaining: 1m 17s\n",
            "317:\tlearn: 0.0597452\ttotal: 36s\tremaining: 1m 17s\n",
            "318:\tlearn: 0.0597301\ttotal: 36.1s\tremaining: 1m 17s\n",
            "319:\tlearn: 0.0596959\ttotal: 36.2s\tremaining: 1m 16s\n",
            "320:\tlearn: 0.0596726\ttotal: 36.3s\tremaining: 1m 16s\n",
            "321:\tlearn: 0.0596522\ttotal: 36.4s\tremaining: 1m 16s\n",
            "322:\tlearn: 0.0596240\ttotal: 36.5s\tremaining: 1m 16s\n",
            "323:\tlearn: 0.0596088\ttotal: 36.6s\tremaining: 1m 16s\n",
            "324:\tlearn: 0.0595906\ttotal: 36.7s\tremaining: 1m 16s\n",
            "325:\tlearn: 0.0595760\ttotal: 36.8s\tremaining: 1m 16s\n",
            "326:\tlearn: 0.0595646\ttotal: 36.9s\tremaining: 1m 15s\n",
            "327:\tlearn: 0.0595442\ttotal: 37s\tremaining: 1m 15s\n",
            "328:\tlearn: 0.0595302\ttotal: 37.1s\tremaining: 1m 15s\n",
            "329:\tlearn: 0.0595137\ttotal: 37.2s\tremaining: 1m 15s\n",
            "330:\tlearn: 0.0594907\ttotal: 37.3s\tremaining: 1m 15s\n",
            "331:\tlearn: 0.0594524\ttotal: 37.4s\tremaining: 1m 15s\n",
            "332:\tlearn: 0.0594323\ttotal: 37.5s\tremaining: 1m 15s\n",
            "333:\tlearn: 0.0594231\ttotal: 37.6s\tremaining: 1m 14s\n",
            "334:\tlearn: 0.0594148\ttotal: 37.7s\tremaining: 1m 14s\n",
            "335:\tlearn: 0.0593720\ttotal: 37.8s\tremaining: 1m 14s\n",
            "336:\tlearn: 0.0593580\ttotal: 37.9s\tremaining: 1m 14s\n",
            "337:\tlearn: 0.0593420\ttotal: 38s\tremaining: 1m 14s\n",
            "338:\tlearn: 0.0593110\ttotal: 38.1s\tremaining: 1m 14s\n",
            "339:\tlearn: 0.0592948\ttotal: 38.2s\tremaining: 1m 14s\n",
            "340:\tlearn: 0.0592686\ttotal: 38.3s\tremaining: 1m 13s\n",
            "341:\tlearn: 0.0592409\ttotal: 38.4s\tremaining: 1m 13s\n",
            "342:\tlearn: 0.0592065\ttotal: 38.5s\tremaining: 1m 13s\n",
            "343:\tlearn: 0.0591859\ttotal: 38.6s\tremaining: 1m 13s\n",
            "344:\tlearn: 0.0591462\ttotal: 38.7s\tremaining: 1m 13s\n",
            "345:\tlearn: 0.0591342\ttotal: 38.8s\tremaining: 1m 13s\n",
            "346:\tlearn: 0.0591146\ttotal: 38.9s\tremaining: 1m 13s\n",
            "347:\tlearn: 0.0590913\ttotal: 39s\tremaining: 1m 13s\n",
            "348:\tlearn: 0.0590727\ttotal: 39.1s\tremaining: 1m 12s\n",
            "349:\tlearn: 0.0590640\ttotal: 39.2s\tremaining: 1m 12s\n",
            "350:\tlearn: 0.0590252\ttotal: 39.3s\tremaining: 1m 12s\n",
            "351:\tlearn: 0.0589955\ttotal: 39.4s\tremaining: 1m 12s\n",
            "352:\tlearn: 0.0589850\ttotal: 39.5s\tremaining: 1m 12s\n",
            "353:\tlearn: 0.0588827\ttotal: 39.6s\tremaining: 1m 12s\n",
            "354:\tlearn: 0.0588754\ttotal: 39.7s\tremaining: 1m 12s\n",
            "355:\tlearn: 0.0588713\ttotal: 39.8s\tremaining: 1m 11s\n",
            "356:\tlearn: 0.0588610\ttotal: 39.9s\tremaining: 1m 11s\n",
            "357:\tlearn: 0.0588528\ttotal: 40s\tremaining: 1m 11s\n",
            "358:\tlearn: 0.0588419\ttotal: 40.1s\tremaining: 1m 11s\n",
            "359:\tlearn: 0.0588259\ttotal: 40.2s\tremaining: 1m 11s\n",
            "360:\tlearn: 0.0588171\ttotal: 40.3s\tremaining: 1m 11s\n",
            "361:\tlearn: 0.0588088\ttotal: 40.4s\tremaining: 1m 11s\n",
            "362:\tlearn: 0.0587771\ttotal: 40.5s\tremaining: 1m 11s\n",
            "363:\tlearn: 0.0587467\ttotal: 40.6s\tremaining: 1m 10s\n",
            "364:\tlearn: 0.0587183\ttotal: 40.7s\tremaining: 1m 10s\n",
            "365:\tlearn: 0.0587026\ttotal: 40.8s\tremaining: 1m 10s\n",
            "366:\tlearn: 0.0586781\ttotal: 40.9s\tremaining: 1m 10s\n",
            "367:\tlearn: 0.0586544\ttotal: 41s\tremaining: 1m 10s\n",
            "368:\tlearn: 0.0585932\ttotal: 41.1s\tremaining: 1m 10s\n",
            "369:\tlearn: 0.0585671\ttotal: 41.2s\tremaining: 1m 10s\n",
            "370:\tlearn: 0.0585402\ttotal: 41.3s\tremaining: 1m 10s\n",
            "371:\tlearn: 0.0585212\ttotal: 41.4s\tremaining: 1m 9s\n",
            "372:\tlearn: 0.0584991\ttotal: 41.5s\tremaining: 1m 9s\n",
            "373:\tlearn: 0.0584715\ttotal: 41.6s\tremaining: 1m 9s\n",
            "374:\tlearn: 0.0584634\ttotal: 41.7s\tremaining: 1m 9s\n",
            "375:\tlearn: 0.0584460\ttotal: 41.8s\tremaining: 1m 9s\n",
            "376:\tlearn: 0.0584205\ttotal: 41.9s\tremaining: 1m 9s\n",
            "377:\tlearn: 0.0584033\ttotal: 42s\tremaining: 1m 9s\n",
            "378:\tlearn: 0.0583820\ttotal: 42.1s\tremaining: 1m 9s\n",
            "379:\tlearn: 0.0583660\ttotal: 42.2s\tremaining: 1m 8s\n",
            "380:\tlearn: 0.0583590\ttotal: 42.3s\tremaining: 1m 8s\n",
            "381:\tlearn: 0.0583469\ttotal: 42.4s\tremaining: 1m 8s\n",
            "382:\tlearn: 0.0583242\ttotal: 42.5s\tremaining: 1m 8s\n",
            "383:\tlearn: 0.0582928\ttotal: 42.6s\tremaining: 1m 8s\n",
            "384:\tlearn: 0.0582867\ttotal: 42.7s\tremaining: 1m 8s\n",
            "385:\tlearn: 0.0582698\ttotal: 42.8s\tremaining: 1m 8s\n",
            "386:\tlearn: 0.0582609\ttotal: 42.9s\tremaining: 1m 7s\n",
            "387:\tlearn: 0.0582430\ttotal: 43s\tremaining: 1m 7s\n",
            "388:\tlearn: 0.0582282\ttotal: 43.1s\tremaining: 1m 7s\n",
            "389:\tlearn: 0.0582098\ttotal: 43.2s\tremaining: 1m 7s\n",
            "390:\tlearn: 0.0581959\ttotal: 43.3s\tremaining: 1m 7s\n",
            "391:\tlearn: 0.0581729\ttotal: 43.4s\tremaining: 1m 7s\n",
            "392:\tlearn: 0.0581605\ttotal: 43.5s\tremaining: 1m 7s\n",
            "393:\tlearn: 0.0581498\ttotal: 43.6s\tremaining: 1m 7s\n",
            "394:\tlearn: 0.0581457\ttotal: 43.7s\tremaining: 1m 6s\n",
            "395:\tlearn: 0.0581408\ttotal: 43.8s\tremaining: 1m 6s\n",
            "396:\tlearn: 0.0581348\ttotal: 43.9s\tremaining: 1m 6s\n",
            "397:\tlearn: 0.0581130\ttotal: 44s\tremaining: 1m 6s\n",
            "398:\tlearn: 0.0581030\ttotal: 44.1s\tremaining: 1m 6s\n",
            "399:\tlearn: 0.0580965\ttotal: 44.2s\tremaining: 1m 6s\n",
            "400:\tlearn: 0.0580870\ttotal: 44.3s\tremaining: 1m 6s\n",
            "401:\tlearn: 0.0580733\ttotal: 44.4s\tremaining: 1m 6s\n",
            "402:\tlearn: 0.0580660\ttotal: 44.5s\tremaining: 1m 5s\n",
            "403:\tlearn: 0.0580579\ttotal: 44.6s\tremaining: 1m 5s\n",
            "404:\tlearn: 0.0580509\ttotal: 44.7s\tremaining: 1m 5s\n",
            "405:\tlearn: 0.0580131\ttotal: 44.8s\tremaining: 1m 5s\n",
            "406:\tlearn: 0.0579914\ttotal: 44.9s\tremaining: 1m 5s\n",
            "407:\tlearn: 0.0579620\ttotal: 45s\tremaining: 1m 5s\n",
            "408:\tlearn: 0.0579391\ttotal: 45.2s\tremaining: 1m 5s\n",
            "409:\tlearn: 0.0579250\ttotal: 45.3s\tremaining: 1m 5s\n",
            "410:\tlearn: 0.0579163\ttotal: 45.4s\tremaining: 1m 5s\n",
            "411:\tlearn: 0.0579013\ttotal: 45.5s\tremaining: 1m 4s\n",
            "412:\tlearn: 0.0578743\ttotal: 45.6s\tremaining: 1m 4s\n",
            "413:\tlearn: 0.0578400\ttotal: 45.7s\tremaining: 1m 4s\n",
            "414:\tlearn: 0.0578272\ttotal: 45.8s\tremaining: 1m 4s\n",
            "415:\tlearn: 0.0578022\ttotal: 45.9s\tremaining: 1m 4s\n",
            "416:\tlearn: 0.0577933\ttotal: 46s\tremaining: 1m 4s\n",
            "417:\tlearn: 0.0577821\ttotal: 46.1s\tremaining: 1m 4s\n",
            "418:\tlearn: 0.0577775\ttotal: 46.2s\tremaining: 1m 4s\n",
            "419:\tlearn: 0.0577633\ttotal: 46.3s\tremaining: 1m 3s\n",
            "420:\tlearn: 0.0576993\ttotal: 46.4s\tremaining: 1m 3s\n",
            "421:\tlearn: 0.0576931\ttotal: 46.5s\tremaining: 1m 3s\n",
            "422:\tlearn: 0.0576831\ttotal: 46.6s\tremaining: 1m 3s\n",
            "423:\tlearn: 0.0576735\ttotal: 46.7s\tremaining: 1m 3s\n",
            "424:\tlearn: 0.0576540\ttotal: 46.8s\tremaining: 1m 3s\n",
            "425:\tlearn: 0.0576387\ttotal: 46.9s\tremaining: 1m 3s\n",
            "426:\tlearn: 0.0576146\ttotal: 47s\tremaining: 1m 3s\n",
            "427:\tlearn: 0.0576087\ttotal: 47.1s\tremaining: 1m 2s\n",
            "428:\tlearn: 0.0575992\ttotal: 47.2s\tremaining: 1m 2s\n",
            "429:\tlearn: 0.0575865\ttotal: 47.3s\tremaining: 1m 2s\n",
            "430:\tlearn: 0.0575668\ttotal: 47.4s\tremaining: 1m 2s\n",
            "431:\tlearn: 0.0575512\ttotal: 47.5s\tremaining: 1m 2s\n",
            "432:\tlearn: 0.0575446\ttotal: 47.6s\tremaining: 1m 2s\n",
            "433:\tlearn: 0.0575350\ttotal: 47.7s\tremaining: 1m 2s\n",
            "434:\tlearn: 0.0574799\ttotal: 47.8s\tremaining: 1m 2s\n",
            "435:\tlearn: 0.0574766\ttotal: 47.9s\tremaining: 1m 2s\n",
            "436:\tlearn: 0.0574662\ttotal: 48s\tremaining: 1m 1s\n",
            "437:\tlearn: 0.0574522\ttotal: 48.1s\tremaining: 1m 1s\n",
            "438:\tlearn: 0.0574446\ttotal: 48.2s\tremaining: 1m 1s\n",
            "439:\tlearn: 0.0574319\ttotal: 48.3s\tremaining: 1m 1s\n",
            "440:\tlearn: 0.0574245\ttotal: 48.4s\tremaining: 1m 1s\n",
            "441:\tlearn: 0.0574092\ttotal: 48.5s\tremaining: 1m 1s\n",
            "442:\tlearn: 0.0573938\ttotal: 48.6s\tremaining: 1m 1s\n",
            "443:\tlearn: 0.0573862\ttotal: 48.7s\tremaining: 1m 1s\n",
            "444:\tlearn: 0.0573744\ttotal: 48.8s\tremaining: 1m\n",
            "445:\tlearn: 0.0573636\ttotal: 48.9s\tremaining: 1m\n",
            "446:\tlearn: 0.0573435\ttotal: 49s\tremaining: 1m\n",
            "447:\tlearn: 0.0573384\ttotal: 49.2s\tremaining: 1m\n",
            "448:\tlearn: 0.0573327\ttotal: 49.3s\tremaining: 1m\n",
            "449:\tlearn: 0.0573295\ttotal: 49.4s\tremaining: 1m\n",
            "450:\tlearn: 0.0573256\ttotal: 49.5s\tremaining: 1m\n",
            "451:\tlearn: 0.0573017\ttotal: 49.6s\tremaining: 1m\n",
            "452:\tlearn: 0.0572937\ttotal: 49.7s\tremaining: 60s\n",
            "453:\tlearn: 0.0572864\ttotal: 49.8s\tremaining: 59.8s\n",
            "454:\tlearn: 0.0572747\ttotal: 49.9s\tremaining: 59.7s\n",
            "455:\tlearn: 0.0572677\ttotal: 50s\tremaining: 59.6s\n",
            "456:\tlearn: 0.0572587\ttotal: 50.1s\tremaining: 59.5s\n",
            "457:\tlearn: 0.0572502\ttotal: 50.2s\tremaining: 59.4s\n",
            "458:\tlearn: 0.0572400\ttotal: 50.3s\tremaining: 59.3s\n",
            "459:\tlearn: 0.0572229\ttotal: 50.4s\tremaining: 59.1s\n",
            "460:\tlearn: 0.0572074\ttotal: 50.5s\tremaining: 59s\n",
            "461:\tlearn: 0.0571997\ttotal: 50.6s\tremaining: 58.9s\n",
            "462:\tlearn: 0.0571775\ttotal: 50.7s\tremaining: 58.8s\n",
            "463:\tlearn: 0.0571699\ttotal: 50.8s\tremaining: 58.7s\n",
            "464:\tlearn: 0.0571586\ttotal: 50.9s\tremaining: 58.6s\n",
            "465:\tlearn: 0.0571464\ttotal: 51s\tremaining: 58.4s\n",
            "466:\tlearn: 0.0571372\ttotal: 51.1s\tremaining: 58.3s\n",
            "467:\tlearn: 0.0571221\ttotal: 51.2s\tremaining: 58.2s\n",
            "468:\tlearn: 0.0571189\ttotal: 51.3s\tremaining: 58.1s\n",
            "469:\tlearn: 0.0571071\ttotal: 51.4s\tremaining: 58s\n",
            "470:\tlearn: 0.0571013\ttotal: 51.5s\tremaining: 57.9s\n",
            "471:\tlearn: 0.0570844\ttotal: 51.6s\tremaining: 57.7s\n",
            "472:\tlearn: 0.0570775\ttotal: 51.7s\tremaining: 57.6s\n",
            "473:\tlearn: 0.0570715\ttotal: 51.8s\tremaining: 57.5s\n",
            "474:\tlearn: 0.0570652\ttotal: 51.9s\tremaining: 57.4s\n",
            "475:\tlearn: 0.0570463\ttotal: 52s\tremaining: 57.3s\n",
            "476:\tlearn: 0.0570383\ttotal: 52.1s\tremaining: 57.1s\n",
            "477:\tlearn: 0.0570311\ttotal: 52.2s\tremaining: 57s\n",
            "478:\tlearn: 0.0570263\ttotal: 52.3s\tremaining: 56.9s\n",
            "479:\tlearn: 0.0570185\ttotal: 52.4s\tremaining: 56.8s\n",
            "480:\tlearn: 0.0570144\ttotal: 52.5s\tremaining: 56.7s\n",
            "481:\tlearn: 0.0569995\ttotal: 52.6s\tremaining: 56.6s\n",
            "482:\tlearn: 0.0569758\ttotal: 52.7s\tremaining: 56.5s\n",
            "483:\tlearn: 0.0569645\ttotal: 52.8s\tremaining: 56.3s\n",
            "484:\tlearn: 0.0569457\ttotal: 52.9s\tremaining: 56.2s\n",
            "485:\tlearn: 0.0569365\ttotal: 53.1s\tremaining: 56.1s\n",
            "486:\tlearn: 0.0569176\ttotal: 53.2s\tremaining: 56s\n",
            "487:\tlearn: 0.0568648\ttotal: 53.3s\tremaining: 55.9s\n",
            "488:\tlearn: 0.0568567\ttotal: 53.4s\tremaining: 55.8s\n",
            "489:\tlearn: 0.0568216\ttotal: 53.5s\tremaining: 55.6s\n",
            "490:\tlearn: 0.0568154\ttotal: 53.6s\tremaining: 55.5s\n",
            "491:\tlearn: 0.0567640\ttotal: 53.7s\tremaining: 55.4s\n",
            "492:\tlearn: 0.0567518\ttotal: 53.8s\tremaining: 55.3s\n",
            "493:\tlearn: 0.0567441\ttotal: 53.9s\tremaining: 55.2s\n",
            "494:\tlearn: 0.0567306\ttotal: 54s\tremaining: 55.1s\n",
            "495:\tlearn: 0.0567166\ttotal: 54.1s\tremaining: 54.9s\n",
            "496:\tlearn: 0.0567131\ttotal: 54.2s\tremaining: 54.8s\n",
            "497:\tlearn: 0.0567037\ttotal: 54.3s\tremaining: 54.7s\n",
            "498:\tlearn: 0.0566950\ttotal: 54.4s\tremaining: 54.6s\n",
            "499:\tlearn: 0.0566909\ttotal: 54.5s\tremaining: 54.5s\n",
            "500:\tlearn: 0.0566712\ttotal: 54.6s\tremaining: 54.4s\n",
            "501:\tlearn: 0.0566273\ttotal: 54.7s\tremaining: 54.3s\n",
            "502:\tlearn: 0.0566209\ttotal: 54.8s\tremaining: 54.1s\n",
            "503:\tlearn: 0.0566124\ttotal: 54.9s\tremaining: 54s\n",
            "504:\tlearn: 0.0566072\ttotal: 55s\tremaining: 53.9s\n",
            "505:\tlearn: 0.0566005\ttotal: 55.1s\tremaining: 53.8s\n",
            "506:\tlearn: 0.0565288\ttotal: 55.2s\tremaining: 53.7s\n",
            "507:\tlearn: 0.0564925\ttotal: 55.3s\tremaining: 53.6s\n",
            "508:\tlearn: 0.0564374\ttotal: 55.4s\tremaining: 53.4s\n",
            "509:\tlearn: 0.0564164\ttotal: 55.5s\tremaining: 53.3s\n",
            "510:\tlearn: 0.0564127\ttotal: 55.6s\tremaining: 53.2s\n",
            "511:\tlearn: 0.0563492\ttotal: 55.7s\tremaining: 53.1s\n",
            "512:\tlearn: 0.0563395\ttotal: 55.8s\tremaining: 53s\n",
            "513:\tlearn: 0.0563248\ttotal: 55.9s\tremaining: 52.9s\n",
            "514:\tlearn: 0.0563065\ttotal: 56s\tremaining: 52.7s\n",
            "515:\tlearn: 0.0562990\ttotal: 56.1s\tremaining: 52.6s\n",
            "516:\tlearn: 0.0562938\ttotal: 56.2s\tremaining: 52.5s\n",
            "517:\tlearn: 0.0562473\ttotal: 56.3s\tremaining: 52.4s\n",
            "518:\tlearn: 0.0562339\ttotal: 56.4s\tremaining: 52.3s\n",
            "519:\tlearn: 0.0562258\ttotal: 56.5s\tremaining: 52.2s\n",
            "520:\tlearn: 0.0562164\ttotal: 56.6s\tremaining: 52s\n",
            "521:\tlearn: 0.0562093\ttotal: 56.7s\tremaining: 51.9s\n",
            "522:\tlearn: 0.0561985\ttotal: 56.8s\tremaining: 51.8s\n",
            "523:\tlearn: 0.0561842\ttotal: 56.9s\tremaining: 51.7s\n",
            "524:\tlearn: 0.0561809\ttotal: 57s\tremaining: 51.6s\n",
            "525:\tlearn: 0.0561662\ttotal: 57.1s\tremaining: 51.5s\n",
            "526:\tlearn: 0.0561587\ttotal: 57.2s\tremaining: 51.4s\n",
            "527:\tlearn: 0.0561550\ttotal: 57.3s\tremaining: 51.2s\n",
            "528:\tlearn: 0.0561463\ttotal: 57.4s\tremaining: 51.1s\n",
            "529:\tlearn: 0.0561400\ttotal: 57.5s\tremaining: 51s\n",
            "530:\tlearn: 0.0561299\ttotal: 57.6s\tremaining: 50.9s\n",
            "531:\tlearn: 0.0561250\ttotal: 57.7s\tremaining: 50.8s\n",
            "532:\tlearn: 0.0561175\ttotal: 57.8s\tremaining: 50.7s\n",
            "533:\tlearn: 0.0561089\ttotal: 57.9s\tremaining: 50.6s\n",
            "534:\tlearn: 0.0561052\ttotal: 58s\tremaining: 50.4s\n",
            "535:\tlearn: 0.0560973\ttotal: 58.1s\tremaining: 50.3s\n",
            "536:\tlearn: 0.0560801\ttotal: 58.2s\tremaining: 50.2s\n",
            "537:\tlearn: 0.0560742\ttotal: 58.3s\tremaining: 50.1s\n",
            "538:\tlearn: 0.0560400\ttotal: 58.4s\tremaining: 50s\n",
            "539:\tlearn: 0.0560256\ttotal: 58.5s\tremaining: 49.9s\n",
            "540:\tlearn: 0.0560132\ttotal: 58.6s\tremaining: 49.8s\n",
            "541:\tlearn: 0.0560010\ttotal: 58.7s\tremaining: 49.6s\n",
            "542:\tlearn: 0.0559976\ttotal: 58.8s\tremaining: 49.5s\n",
            "543:\tlearn: 0.0559820\ttotal: 58.9s\tremaining: 49.4s\n",
            "544:\tlearn: 0.0559794\ttotal: 59s\tremaining: 49.3s\n",
            "545:\tlearn: 0.0559763\ttotal: 59.1s\tremaining: 49.2s\n",
            "546:\tlearn: 0.0559708\ttotal: 59.3s\tremaining: 49.1s\n",
            "547:\tlearn: 0.0559527\ttotal: 59.4s\tremaining: 49s\n",
            "548:\tlearn: 0.0559464\ttotal: 59.5s\tremaining: 48.8s\n",
            "549:\tlearn: 0.0559406\ttotal: 59.6s\tremaining: 48.7s\n",
            "550:\tlearn: 0.0559104\ttotal: 59.7s\tremaining: 48.6s\n",
            "551:\tlearn: 0.0558873\ttotal: 59.8s\tremaining: 48.5s\n",
            "552:\tlearn: 0.0558849\ttotal: 59.9s\tremaining: 48.4s\n",
            "553:\tlearn: 0.0558773\ttotal: 60s\tremaining: 48.3s\n",
            "554:\tlearn: 0.0558729\ttotal: 1m\tremaining: 48.2s\n",
            "555:\tlearn: 0.0558669\ttotal: 1m\tremaining: 48s\n",
            "556:\tlearn: 0.0558637\ttotal: 1m\tremaining: 47.9s\n",
            "557:\tlearn: 0.0558416\ttotal: 1m\tremaining: 47.8s\n",
            "558:\tlearn: 0.0558370\ttotal: 1m\tremaining: 47.7s\n",
            "559:\tlearn: 0.0558314\ttotal: 1m\tremaining: 47.6s\n",
            "560:\tlearn: 0.0557888\ttotal: 1m\tremaining: 47.5s\n",
            "561:\tlearn: 0.0557793\ttotal: 1m\tremaining: 47.3s\n",
            "562:\tlearn: 0.0557677\ttotal: 1m\tremaining: 47.2s\n",
            "563:\tlearn: 0.0557417\ttotal: 1m\tremaining: 47.1s\n",
            "564:\tlearn: 0.0557368\ttotal: 1m 1s\tremaining: 47s\n",
            "565:\tlearn: 0.0557238\ttotal: 1m 1s\tremaining: 46.9s\n",
            "566:\tlearn: 0.0557161\ttotal: 1m 1s\tremaining: 46.8s\n",
            "567:\tlearn: 0.0557061\ttotal: 1m 1s\tremaining: 46.7s\n",
            "568:\tlearn: 0.0556921\ttotal: 1m 1s\tremaining: 46.5s\n",
            "569:\tlearn: 0.0556882\ttotal: 1m 1s\tremaining: 46.4s\n",
            "570:\tlearn: 0.0556836\ttotal: 1m 1s\tremaining: 46.3s\n",
            "571:\tlearn: 0.0556656\ttotal: 1m 1s\tremaining: 46.2s\n",
            "572:\tlearn: 0.0556579\ttotal: 1m 1s\tremaining: 46.1s\n",
            "573:\tlearn: 0.0556494\ttotal: 1m 1s\tremaining: 46s\n",
            "574:\tlearn: 0.0556419\ttotal: 1m 2s\tremaining: 45.9s\n",
            "575:\tlearn: 0.0556403\ttotal: 1m 2s\tremaining: 45.8s\n",
            "576:\tlearn: 0.0556246\ttotal: 1m 2s\tremaining: 45.6s\n",
            "577:\tlearn: 0.0556108\ttotal: 1m 2s\tremaining: 45.5s\n",
            "578:\tlearn: 0.0556045\ttotal: 1m 2s\tremaining: 45.4s\n",
            "579:\tlearn: 0.0555972\ttotal: 1m 2s\tremaining: 45.3s\n",
            "580:\tlearn: 0.0555850\ttotal: 1m 2s\tremaining: 45.2s\n",
            "581:\tlearn: 0.0555795\ttotal: 1m 2s\tremaining: 45.1s\n",
            "582:\tlearn: 0.0555691\ttotal: 1m 2s\tremaining: 45s\n",
            "583:\tlearn: 0.0555635\ttotal: 1m 3s\tremaining: 44.9s\n",
            "584:\tlearn: 0.0555572\ttotal: 1m 3s\tremaining: 44.8s\n",
            "585:\tlearn: 0.0555533\ttotal: 1m 3s\tremaining: 44.7s\n",
            "586:\tlearn: 0.0555471\ttotal: 1m 3s\tremaining: 44.5s\n",
            "587:\tlearn: 0.0555398\ttotal: 1m 3s\tremaining: 44.4s\n",
            "588:\tlearn: 0.0555360\ttotal: 1m 3s\tremaining: 44.3s\n",
            "589:\tlearn: 0.0555263\ttotal: 1m 3s\tremaining: 44.2s\n",
            "590:\tlearn: 0.0555165\ttotal: 1m 3s\tremaining: 44.1s\n",
            "591:\tlearn: 0.0555077\ttotal: 1m 3s\tremaining: 44s\n",
            "592:\tlearn: 0.0554807\ttotal: 1m 3s\tremaining: 43.9s\n",
            "593:\tlearn: 0.0554760\ttotal: 1m 4s\tremaining: 43.8s\n",
            "594:\tlearn: 0.0554672\ttotal: 1m 4s\tremaining: 43.6s\n",
            "595:\tlearn: 0.0554608\ttotal: 1m 4s\tremaining: 43.5s\n",
            "596:\tlearn: 0.0554403\ttotal: 1m 4s\tremaining: 43.4s\n",
            "597:\tlearn: 0.0554327\ttotal: 1m 4s\tremaining: 43.3s\n",
            "598:\tlearn: 0.0554273\ttotal: 1m 4s\tremaining: 43.2s\n",
            "599:\tlearn: 0.0554225\ttotal: 1m 4s\tremaining: 43.1s\n",
            "600:\tlearn: 0.0554152\ttotal: 1m 4s\tremaining: 43s\n",
            "601:\tlearn: 0.0554032\ttotal: 1m 4s\tremaining: 42.9s\n",
            "602:\tlearn: 0.0553845\ttotal: 1m 4s\tremaining: 42.7s\n",
            "603:\tlearn: 0.0553721\ttotal: 1m 5s\tremaining: 42.6s\n",
            "604:\tlearn: 0.0553640\ttotal: 1m 5s\tremaining: 42.5s\n",
            "605:\tlearn: 0.0553587\ttotal: 1m 5s\tremaining: 42.4s\n",
            "606:\tlearn: 0.0553515\ttotal: 1m 5s\tremaining: 42.3s\n",
            "607:\tlearn: 0.0553405\ttotal: 1m 5s\tremaining: 42.2s\n",
            "608:\tlearn: 0.0553328\ttotal: 1m 5s\tremaining: 42.1s\n",
            "609:\tlearn: 0.0553047\ttotal: 1m 5s\tremaining: 42s\n",
            "610:\tlearn: 0.0552902\ttotal: 1m 5s\tremaining: 41.8s\n",
            "611:\tlearn: 0.0552788\ttotal: 1m 5s\tremaining: 41.7s\n",
            "612:\tlearn: 0.0552576\ttotal: 1m 5s\tremaining: 41.6s\n",
            "613:\tlearn: 0.0552532\ttotal: 1m 6s\tremaining: 41.5s\n",
            "614:\tlearn: 0.0552389\ttotal: 1m 6s\tremaining: 41.4s\n",
            "615:\tlearn: 0.0552359\ttotal: 1m 6s\tremaining: 41.3s\n",
            "616:\tlearn: 0.0552332\ttotal: 1m 6s\tremaining: 41.2s\n",
            "617:\tlearn: 0.0552218\ttotal: 1m 6s\tremaining: 41.1s\n",
            "618:\tlearn: 0.0552075\ttotal: 1m 6s\tremaining: 41s\n",
            "619:\tlearn: 0.0552021\ttotal: 1m 6s\tremaining: 40.8s\n",
            "620:\tlearn: 0.0552001\ttotal: 1m 6s\tremaining: 40.7s\n",
            "621:\tlearn: 0.0551940\ttotal: 1m 6s\tremaining: 40.6s\n",
            "622:\tlearn: 0.0551832\ttotal: 1m 6s\tremaining: 40.5s\n",
            "623:\tlearn: 0.0551728\ttotal: 1m 7s\tremaining: 40.4s\n",
            "624:\tlearn: 0.0551704\ttotal: 1m 7s\tremaining: 40.3s\n",
            "625:\tlearn: 0.0551621\ttotal: 1m 7s\tremaining: 40.2s\n",
            "626:\tlearn: 0.0551558\ttotal: 1m 7s\tremaining: 40.1s\n",
            "627:\tlearn: 0.0551394\ttotal: 1m 7s\tremaining: 39.9s\n",
            "628:\tlearn: 0.0551260\ttotal: 1m 7s\tremaining: 39.8s\n",
            "629:\tlearn: 0.0551202\ttotal: 1m 7s\tremaining: 39.7s\n",
            "630:\tlearn: 0.0551029\ttotal: 1m 7s\tremaining: 39.6s\n",
            "631:\tlearn: 0.0550969\ttotal: 1m 7s\tremaining: 39.5s\n",
            "632:\tlearn: 0.0550891\ttotal: 1m 7s\tremaining: 39.4s\n",
            "633:\tlearn: 0.0550785\ttotal: 1m 8s\tremaining: 39.3s\n",
            "634:\tlearn: 0.0550720\ttotal: 1m 8s\tremaining: 39.2s\n",
            "635:\tlearn: 0.0550646\ttotal: 1m 8s\tremaining: 39.1s\n",
            "636:\tlearn: 0.0550619\ttotal: 1m 8s\tremaining: 38.9s\n",
            "637:\tlearn: 0.0550555\ttotal: 1m 8s\tremaining: 38.8s\n",
            "638:\tlearn: 0.0550496\ttotal: 1m 8s\tremaining: 38.7s\n",
            "639:\tlearn: 0.0550412\ttotal: 1m 8s\tremaining: 38.6s\n",
            "640:\tlearn: 0.0550374\ttotal: 1m 8s\tremaining: 38.5s\n",
            "641:\tlearn: 0.0550298\ttotal: 1m 8s\tremaining: 38.4s\n",
            "642:\tlearn: 0.0550257\ttotal: 1m 8s\tremaining: 38.3s\n",
            "643:\tlearn: 0.0549931\ttotal: 1m 9s\tremaining: 38.2s\n",
            "644:\tlearn: 0.0549801\ttotal: 1m 9s\tremaining: 38s\n",
            "645:\tlearn: 0.0549687\ttotal: 1m 9s\tremaining: 37.9s\n",
            "646:\tlearn: 0.0549493\ttotal: 1m 9s\tremaining: 37.8s\n",
            "647:\tlearn: 0.0549446\ttotal: 1m 9s\tremaining: 37.7s\n",
            "648:\tlearn: 0.0549378\ttotal: 1m 9s\tremaining: 37.6s\n",
            "649:\tlearn: 0.0549317\ttotal: 1m 9s\tremaining: 37.5s\n",
            "650:\tlearn: 0.0549293\ttotal: 1m 9s\tremaining: 37.4s\n",
            "651:\tlearn: 0.0549223\ttotal: 1m 9s\tremaining: 37.3s\n",
            "652:\tlearn: 0.0549128\ttotal: 1m 9s\tremaining: 37.2s\n",
            "653:\tlearn: 0.0549096\ttotal: 1m 10s\tremaining: 37.1s\n",
            "654:\tlearn: 0.0548992\ttotal: 1m 10s\tremaining: 36.9s\n",
            "655:\tlearn: 0.0548930\ttotal: 1m 10s\tremaining: 36.8s\n",
            "656:\tlearn: 0.0548812\ttotal: 1m 10s\tremaining: 36.7s\n",
            "657:\tlearn: 0.0548754\ttotal: 1m 10s\tremaining: 36.6s\n",
            "658:\tlearn: 0.0548716\ttotal: 1m 10s\tremaining: 36.5s\n",
            "659:\tlearn: 0.0548648\ttotal: 1m 10s\tremaining: 36.4s\n",
            "660:\tlearn: 0.0548616\ttotal: 1m 10s\tremaining: 36.3s\n",
            "661:\tlearn: 0.0548535\ttotal: 1m 10s\tremaining: 36.2s\n",
            "662:\tlearn: 0.0548445\ttotal: 1m 10s\tremaining: 36.1s\n",
            "663:\tlearn: 0.0548371\ttotal: 1m 11s\tremaining: 35.9s\n",
            "664:\tlearn: 0.0548323\ttotal: 1m 11s\tremaining: 35.8s\n",
            "665:\tlearn: 0.0548237\ttotal: 1m 11s\tremaining: 35.7s\n",
            "666:\tlearn: 0.0548159\ttotal: 1m 11s\tremaining: 35.6s\n",
            "667:\tlearn: 0.0548141\ttotal: 1m 11s\tremaining: 35.5s\n",
            "668:\tlearn: 0.0548057\ttotal: 1m 11s\tremaining: 35.4s\n",
            "669:\tlearn: 0.0548016\ttotal: 1m 11s\tremaining: 35.3s\n",
            "670:\tlearn: 0.0547973\ttotal: 1m 11s\tremaining: 35.2s\n",
            "671:\tlearn: 0.0547941\ttotal: 1m 11s\tremaining: 35.1s\n",
            "672:\tlearn: 0.0547870\ttotal: 1m 11s\tremaining: 35s\n",
            "673:\tlearn: 0.0547743\ttotal: 1m 12s\tremaining: 34.8s\n",
            "674:\tlearn: 0.0547480\ttotal: 1m 12s\tremaining: 34.7s\n",
            "675:\tlearn: 0.0547379\ttotal: 1m 12s\tremaining: 34.6s\n",
            "676:\tlearn: 0.0547289\ttotal: 1m 12s\tremaining: 34.5s\n",
            "677:\tlearn: 0.0547222\ttotal: 1m 12s\tremaining: 34.4s\n",
            "678:\tlearn: 0.0547097\ttotal: 1m 12s\tremaining: 34.3s\n",
            "679:\tlearn: 0.0547036\ttotal: 1m 12s\tremaining: 34.2s\n",
            "680:\tlearn: 0.0546970\ttotal: 1m 12s\tremaining: 34.1s\n",
            "681:\tlearn: 0.0546853\ttotal: 1m 12s\tremaining: 34s\n",
            "682:\tlearn: 0.0546790\ttotal: 1m 12s\tremaining: 33.9s\n",
            "683:\tlearn: 0.0546737\ttotal: 1m 13s\tremaining: 33.7s\n",
            "684:\tlearn: 0.0546709\ttotal: 1m 13s\tremaining: 33.6s\n",
            "685:\tlearn: 0.0546475\ttotal: 1m 13s\tremaining: 33.5s\n",
            "686:\tlearn: 0.0546403\ttotal: 1m 13s\tremaining: 33.4s\n",
            "687:\tlearn: 0.0546336\ttotal: 1m 13s\tremaining: 33.3s\n",
            "688:\tlearn: 0.0546279\ttotal: 1m 13s\tremaining: 33.2s\n",
            "689:\tlearn: 0.0546157\ttotal: 1m 13s\tremaining: 33.1s\n",
            "690:\tlearn: 0.0545802\ttotal: 1m 13s\tremaining: 33s\n",
            "691:\tlearn: 0.0545732\ttotal: 1m 13s\tremaining: 32.9s\n",
            "692:\tlearn: 0.0545691\ttotal: 1m 13s\tremaining: 32.8s\n",
            "693:\tlearn: 0.0545481\ttotal: 1m 14s\tremaining: 32.7s\n",
            "694:\tlearn: 0.0545379\ttotal: 1m 14s\tremaining: 32.5s\n",
            "695:\tlearn: 0.0545324\ttotal: 1m 14s\tremaining: 32.4s\n",
            "696:\tlearn: 0.0545174\ttotal: 1m 14s\tremaining: 32.3s\n",
            "697:\tlearn: 0.0545101\ttotal: 1m 14s\tremaining: 32.2s\n",
            "698:\tlearn: 0.0545012\ttotal: 1m 14s\tremaining: 32.1s\n",
            "699:\tlearn: 0.0544932\ttotal: 1m 14s\tremaining: 32s\n",
            "700:\tlearn: 0.0544888\ttotal: 1m 14s\tremaining: 31.9s\n",
            "701:\tlearn: 0.0544850\ttotal: 1m 14s\tremaining: 31.8s\n",
            "702:\tlearn: 0.0544766\ttotal: 1m 14s\tremaining: 31.7s\n",
            "703:\tlearn: 0.0544609\ttotal: 1m 15s\tremaining: 31.6s\n",
            "704:\tlearn: 0.0544592\ttotal: 1m 15s\tremaining: 31.5s\n",
            "705:\tlearn: 0.0544493\ttotal: 1m 15s\tremaining: 31.4s\n",
            "706:\tlearn: 0.0544454\ttotal: 1m 15s\tremaining: 31.3s\n",
            "707:\tlearn: 0.0544424\ttotal: 1m 15s\tremaining: 31.1s\n",
            "708:\tlearn: 0.0544333\ttotal: 1m 15s\tremaining: 31s\n",
            "709:\tlearn: 0.0544222\ttotal: 1m 15s\tremaining: 30.9s\n",
            "710:\tlearn: 0.0544171\ttotal: 1m 15s\tremaining: 30.8s\n",
            "711:\tlearn: 0.0544133\ttotal: 1m 15s\tremaining: 30.7s\n",
            "712:\tlearn: 0.0544081\ttotal: 1m 16s\tremaining: 30.6s\n",
            "713:\tlearn: 0.0544006\ttotal: 1m 16s\tremaining: 30.5s\n",
            "714:\tlearn: 0.0543987\ttotal: 1m 16s\tremaining: 30.4s\n",
            "715:\tlearn: 0.0543966\ttotal: 1m 16s\tremaining: 30.3s\n",
            "716:\tlearn: 0.0543880\ttotal: 1m 16s\tremaining: 30.2s\n",
            "717:\tlearn: 0.0543759\ttotal: 1m 16s\tremaining: 30.1s\n",
            "718:\tlearn: 0.0543654\ttotal: 1m 16s\tremaining: 29.9s\n",
            "719:\tlearn: 0.0543309\ttotal: 1m 16s\tremaining: 29.8s\n",
            "720:\tlearn: 0.0543251\ttotal: 1m 16s\tremaining: 29.7s\n",
            "721:\tlearn: 0.0543164\ttotal: 1m 16s\tremaining: 29.6s\n",
            "722:\tlearn: 0.0543126\ttotal: 1m 17s\tremaining: 29.5s\n",
            "723:\tlearn: 0.0543026\ttotal: 1m 17s\tremaining: 29.4s\n",
            "724:\tlearn: 0.0542975\ttotal: 1m 17s\tremaining: 29.3s\n",
            "725:\tlearn: 0.0542925\ttotal: 1m 17s\tremaining: 29.2s\n",
            "726:\tlearn: 0.0542791\ttotal: 1m 17s\tremaining: 29.1s\n",
            "727:\tlearn: 0.0542770\ttotal: 1m 17s\tremaining: 29s\n",
            "728:\tlearn: 0.0542731\ttotal: 1m 17s\tremaining: 28.9s\n",
            "729:\tlearn: 0.0542699\ttotal: 1m 17s\tremaining: 28.8s\n",
            "730:\tlearn: 0.0542601\ttotal: 1m 17s\tremaining: 28.6s\n",
            "731:\tlearn: 0.0542371\ttotal: 1m 17s\tremaining: 28.5s\n",
            "732:\tlearn: 0.0542350\ttotal: 1m 18s\tremaining: 28.4s\n",
            "733:\tlearn: 0.0542256\ttotal: 1m 18s\tremaining: 28.3s\n",
            "734:\tlearn: 0.0542216\ttotal: 1m 18s\tremaining: 28.2s\n",
            "735:\tlearn: 0.0542090\ttotal: 1m 18s\tremaining: 28.1s\n",
            "736:\tlearn: 0.0542039\ttotal: 1m 18s\tremaining: 28s\n",
            "737:\tlearn: 0.0541952\ttotal: 1m 18s\tremaining: 27.9s\n",
            "738:\tlearn: 0.0541887\ttotal: 1m 18s\tremaining: 27.8s\n",
            "739:\tlearn: 0.0541847\ttotal: 1m 18s\tremaining: 27.7s\n",
            "740:\tlearn: 0.0541830\ttotal: 1m 18s\tremaining: 27.6s\n",
            "741:\tlearn: 0.0541737\ttotal: 1m 18s\tremaining: 27.5s\n",
            "742:\tlearn: 0.0541638\ttotal: 1m 19s\tremaining: 27.3s\n",
            "743:\tlearn: 0.0541593\ttotal: 1m 19s\tremaining: 27.2s\n",
            "744:\tlearn: 0.0541497\ttotal: 1m 19s\tremaining: 27.1s\n",
            "745:\tlearn: 0.0541466\ttotal: 1m 19s\tremaining: 27s\n",
            "746:\tlearn: 0.0541454\ttotal: 1m 19s\tremaining: 26.9s\n",
            "747:\tlearn: 0.0541367\ttotal: 1m 19s\tremaining: 26.8s\n",
            "748:\tlearn: 0.0541330\ttotal: 1m 19s\tremaining: 26.7s\n",
            "749:\tlearn: 0.0541290\ttotal: 1m 19s\tremaining: 26.6s\n",
            "750:\tlearn: 0.0541209\ttotal: 1m 19s\tremaining: 26.5s\n",
            "751:\tlearn: 0.0541172\ttotal: 1m 19s\tremaining: 26.4s\n",
            "752:\tlearn: 0.0541070\ttotal: 1m 20s\tremaining: 26.3s\n",
            "753:\tlearn: 0.0541040\ttotal: 1m 20s\tremaining: 26.2s\n",
            "754:\tlearn: 0.0540934\ttotal: 1m 20s\tremaining: 26s\n",
            "755:\tlearn: 0.0540909\ttotal: 1m 20s\tremaining: 25.9s\n",
            "756:\tlearn: 0.0540904\ttotal: 1m 20s\tremaining: 25.8s\n",
            "757:\tlearn: 0.0540871\ttotal: 1m 20s\tremaining: 25.7s\n",
            "758:\tlearn: 0.0540807\ttotal: 1m 20s\tremaining: 25.6s\n",
            "759:\tlearn: 0.0540734\ttotal: 1m 20s\tremaining: 25.5s\n",
            "760:\tlearn: 0.0540712\ttotal: 1m 20s\tremaining: 25.4s\n",
            "761:\tlearn: 0.0540663\ttotal: 1m 20s\tremaining: 25.3s\n",
            "762:\tlearn: 0.0540597\ttotal: 1m 21s\tremaining: 25.2s\n",
            "763:\tlearn: 0.0540530\ttotal: 1m 21s\tremaining: 25.1s\n",
            "764:\tlearn: 0.0540475\ttotal: 1m 21s\tremaining: 25s\n",
            "765:\tlearn: 0.0540432\ttotal: 1m 21s\tremaining: 24.9s\n",
            "766:\tlearn: 0.0540301\ttotal: 1m 21s\tremaining: 24.8s\n",
            "767:\tlearn: 0.0540271\ttotal: 1m 21s\tremaining: 24.6s\n",
            "768:\tlearn: 0.0540170\ttotal: 1m 21s\tremaining: 24.5s\n",
            "769:\tlearn: 0.0540122\ttotal: 1m 21s\tremaining: 24.4s\n",
            "770:\tlearn: 0.0540097\ttotal: 1m 21s\tremaining: 24.3s\n",
            "771:\tlearn: 0.0540056\ttotal: 1m 22s\tremaining: 24.2s\n",
            "772:\tlearn: 0.0540006\ttotal: 1m 22s\tremaining: 24.1s\n",
            "773:\tlearn: 0.0539969\ttotal: 1m 22s\tremaining: 24s\n",
            "774:\tlearn: 0.0539921\ttotal: 1m 22s\tremaining: 23.9s\n",
            "775:\tlearn: 0.0539895\ttotal: 1m 22s\tremaining: 23.8s\n",
            "776:\tlearn: 0.0539856\ttotal: 1m 22s\tremaining: 23.7s\n",
            "777:\tlearn: 0.0539682\ttotal: 1m 22s\tremaining: 23.6s\n",
            "778:\tlearn: 0.0539649\ttotal: 1m 22s\tremaining: 23.5s\n",
            "779:\tlearn: 0.0539603\ttotal: 1m 22s\tremaining: 23.4s\n",
            "780:\tlearn: 0.0539582\ttotal: 1m 22s\tremaining: 23.3s\n",
            "781:\tlearn: 0.0539448\ttotal: 1m 23s\tremaining: 23.1s\n",
            "782:\tlearn: 0.0539354\ttotal: 1m 23s\tremaining: 23s\n",
            "783:\tlearn: 0.0539330\ttotal: 1m 23s\tremaining: 22.9s\n",
            "784:\tlearn: 0.0539275\ttotal: 1m 23s\tremaining: 22.8s\n",
            "785:\tlearn: 0.0539107\ttotal: 1m 23s\tremaining: 22.7s\n",
            "786:\tlearn: 0.0539021\ttotal: 1m 23s\tremaining: 22.6s\n",
            "787:\tlearn: 0.0538975\ttotal: 1m 23s\tremaining: 22.5s\n",
            "788:\tlearn: 0.0538941\ttotal: 1m 23s\tremaining: 22.4s\n",
            "789:\tlearn: 0.0538912\ttotal: 1m 23s\tremaining: 22.3s\n",
            "790:\tlearn: 0.0538880\ttotal: 1m 23s\tremaining: 22.2s\n",
            "791:\tlearn: 0.0538865\ttotal: 1m 24s\tremaining: 22.1s\n",
            "792:\tlearn: 0.0538848\ttotal: 1m 24s\tremaining: 22s\n",
            "793:\tlearn: 0.0538775\ttotal: 1m 24s\tremaining: 21.9s\n",
            "794:\tlearn: 0.0538734\ttotal: 1m 24s\tremaining: 21.7s\n",
            "795:\tlearn: 0.0538629\ttotal: 1m 24s\tremaining: 21.6s\n",
            "796:\tlearn: 0.0538593\ttotal: 1m 24s\tremaining: 21.5s\n",
            "797:\tlearn: 0.0538562\ttotal: 1m 24s\tremaining: 21.4s\n",
            "798:\tlearn: 0.0538537\ttotal: 1m 24s\tremaining: 21.3s\n",
            "799:\tlearn: 0.0538474\ttotal: 1m 24s\tremaining: 21.2s\n",
            "800:\tlearn: 0.0538407\ttotal: 1m 24s\tremaining: 21.1s\n",
            "801:\tlearn: 0.0538355\ttotal: 1m 25s\tremaining: 21s\n",
            "802:\tlearn: 0.0538321\ttotal: 1m 25s\tremaining: 20.9s\n",
            "803:\tlearn: 0.0538290\ttotal: 1m 25s\tremaining: 20.8s\n",
            "804:\tlearn: 0.0538202\ttotal: 1m 25s\tremaining: 20.7s\n",
            "805:\tlearn: 0.0538181\ttotal: 1m 25s\tremaining: 20.6s\n",
            "806:\tlearn: 0.0538068\ttotal: 1m 25s\tremaining: 20.5s\n",
            "807:\tlearn: 0.0538033\ttotal: 1m 25s\tremaining: 20.3s\n",
            "808:\tlearn: 0.0537994\ttotal: 1m 25s\tremaining: 20.2s\n",
            "809:\tlearn: 0.0537947\ttotal: 1m 25s\tremaining: 20.1s\n",
            "810:\tlearn: 0.0537869\ttotal: 1m 25s\tremaining: 20s\n",
            "811:\tlearn: 0.0537793\ttotal: 1m 26s\tremaining: 19.9s\n",
            "812:\tlearn: 0.0537726\ttotal: 1m 26s\tremaining: 19.8s\n",
            "813:\tlearn: 0.0537647\ttotal: 1m 26s\tremaining: 19.7s\n",
            "814:\tlearn: 0.0537608\ttotal: 1m 26s\tremaining: 19.6s\n",
            "815:\tlearn: 0.0537562\ttotal: 1m 26s\tremaining: 19.5s\n",
            "816:\tlearn: 0.0537526\ttotal: 1m 26s\tremaining: 19.4s\n",
            "817:\tlearn: 0.0537461\ttotal: 1m 26s\tremaining: 19.3s\n",
            "818:\tlearn: 0.0537393\ttotal: 1m 26s\tremaining: 19.2s\n",
            "819:\tlearn: 0.0537332\ttotal: 1m 26s\tremaining: 19.1s\n",
            "820:\tlearn: 0.0537271\ttotal: 1m 26s\tremaining: 19s\n",
            "821:\tlearn: 0.0537231\ttotal: 1m 27s\tremaining: 18.8s\n",
            "822:\tlearn: 0.0537134\ttotal: 1m 27s\tremaining: 18.7s\n",
            "823:\tlearn: 0.0537115\ttotal: 1m 27s\tremaining: 18.6s\n",
            "824:\tlearn: 0.0537099\ttotal: 1m 27s\tremaining: 18.5s\n",
            "825:\tlearn: 0.0536893\ttotal: 1m 27s\tremaining: 18.4s\n",
            "826:\tlearn: 0.0536806\ttotal: 1m 27s\tremaining: 18.3s\n",
            "827:\tlearn: 0.0536765\ttotal: 1m 27s\tremaining: 18.2s\n",
            "828:\tlearn: 0.0536741\ttotal: 1m 27s\tremaining: 18.1s\n",
            "829:\tlearn: 0.0536648\ttotal: 1m 27s\tremaining: 18s\n",
            "830:\tlearn: 0.0536627\ttotal: 1m 27s\tremaining: 17.9s\n",
            "831:\tlearn: 0.0536611\ttotal: 1m 28s\tremaining: 17.8s\n",
            "832:\tlearn: 0.0536595\ttotal: 1m 28s\tremaining: 17.7s\n",
            "833:\tlearn: 0.0536480\ttotal: 1m 28s\tremaining: 17.6s\n",
            "834:\tlearn: 0.0536395\ttotal: 1m 28s\tremaining: 17.5s\n",
            "835:\tlearn: 0.0536338\ttotal: 1m 28s\tremaining: 17.4s\n",
            "836:\tlearn: 0.0536314\ttotal: 1m 28s\tremaining: 17.2s\n",
            "837:\tlearn: 0.0536095\ttotal: 1m 28s\tremaining: 17.1s\n",
            "838:\tlearn: 0.0535929\ttotal: 1m 28s\tremaining: 17s\n",
            "839:\tlearn: 0.0535878\ttotal: 1m 28s\tremaining: 16.9s\n",
            "840:\tlearn: 0.0535820\ttotal: 1m 28s\tremaining: 16.8s\n",
            "841:\tlearn: 0.0535788\ttotal: 1m 29s\tremaining: 16.7s\n",
            "842:\tlearn: 0.0535695\ttotal: 1m 29s\tremaining: 16.6s\n",
            "843:\tlearn: 0.0535427\ttotal: 1m 29s\tremaining: 16.5s\n",
            "844:\tlearn: 0.0535372\ttotal: 1m 29s\tremaining: 16.4s\n",
            "845:\tlearn: 0.0535334\ttotal: 1m 29s\tremaining: 16.3s\n",
            "846:\tlearn: 0.0535310\ttotal: 1m 29s\tremaining: 16.2s\n",
            "847:\tlearn: 0.0535216\ttotal: 1m 29s\tremaining: 16.1s\n",
            "848:\tlearn: 0.0535144\ttotal: 1m 29s\tremaining: 16s\n",
            "849:\tlearn: 0.0535124\ttotal: 1m 29s\tremaining: 15.9s\n",
            "850:\tlearn: 0.0535085\ttotal: 1m 29s\tremaining: 15.7s\n",
            "851:\tlearn: 0.0535013\ttotal: 1m 30s\tremaining: 15.6s\n",
            "852:\tlearn: 0.0534953\ttotal: 1m 30s\tremaining: 15.5s\n",
            "853:\tlearn: 0.0534852\ttotal: 1m 30s\tremaining: 15.4s\n",
            "854:\tlearn: 0.0534812\ttotal: 1m 30s\tremaining: 15.3s\n",
            "855:\tlearn: 0.0534790\ttotal: 1m 30s\tremaining: 15.2s\n",
            "856:\tlearn: 0.0534746\ttotal: 1m 30s\tremaining: 15.1s\n",
            "857:\tlearn: 0.0534708\ttotal: 1m 30s\tremaining: 15s\n",
            "858:\tlearn: 0.0534689\ttotal: 1m 30s\tremaining: 14.9s\n",
            "859:\tlearn: 0.0534628\ttotal: 1m 30s\tremaining: 14.8s\n",
            "860:\tlearn: 0.0534590\ttotal: 1m 30s\tremaining: 14.7s\n",
            "861:\tlearn: 0.0534553\ttotal: 1m 31s\tremaining: 14.6s\n",
            "862:\tlearn: 0.0534509\ttotal: 1m 31s\tremaining: 14.5s\n",
            "863:\tlearn: 0.0534426\ttotal: 1m 31s\tremaining: 14.4s\n",
            "864:\tlearn: 0.0534406\ttotal: 1m 31s\tremaining: 14.3s\n",
            "865:\tlearn: 0.0534273\ttotal: 1m 31s\tremaining: 14.1s\n",
            "866:\tlearn: 0.0534220\ttotal: 1m 31s\tremaining: 14s\n",
            "867:\tlearn: 0.0534176\ttotal: 1m 31s\tremaining: 13.9s\n",
            "868:\tlearn: 0.0534133\ttotal: 1m 31s\tremaining: 13.8s\n",
            "869:\tlearn: 0.0534053\ttotal: 1m 31s\tremaining: 13.7s\n",
            "870:\tlearn: 0.0534028\ttotal: 1m 31s\tremaining: 13.6s\n",
            "871:\tlearn: 0.0534004\ttotal: 1m 32s\tremaining: 13.5s\n",
            "872:\tlearn: 0.0533967\ttotal: 1m 32s\tremaining: 13.4s\n",
            "873:\tlearn: 0.0533874\ttotal: 1m 32s\tremaining: 13.3s\n",
            "874:\tlearn: 0.0533814\ttotal: 1m 32s\tremaining: 13.2s\n",
            "875:\tlearn: 0.0533693\ttotal: 1m 32s\tremaining: 13.1s\n",
            "876:\tlearn: 0.0533654\ttotal: 1m 32s\tremaining: 13s\n",
            "877:\tlearn: 0.0533637\ttotal: 1m 32s\tremaining: 12.9s\n",
            "878:\tlearn: 0.0533621\ttotal: 1m 32s\tremaining: 12.8s\n",
            "879:\tlearn: 0.0533600\ttotal: 1m 32s\tremaining: 12.7s\n",
            "880:\tlearn: 0.0533570\ttotal: 1m 32s\tremaining: 12.6s\n",
            "881:\tlearn: 0.0533420\ttotal: 1m 33s\tremaining: 12.4s\n",
            "882:\tlearn: 0.0533372\ttotal: 1m 33s\tremaining: 12.3s\n",
            "883:\tlearn: 0.0533335\ttotal: 1m 33s\tremaining: 12.2s\n",
            "884:\tlearn: 0.0533278\ttotal: 1m 33s\tremaining: 12.1s\n",
            "885:\tlearn: 0.0533240\ttotal: 1m 33s\tremaining: 12s\n",
            "886:\tlearn: 0.0533109\ttotal: 1m 33s\tremaining: 11.9s\n",
            "887:\tlearn: 0.0533012\ttotal: 1m 33s\tremaining: 11.8s\n",
            "888:\tlearn: 0.0532980\ttotal: 1m 33s\tremaining: 11.7s\n",
            "889:\tlearn: 0.0532890\ttotal: 1m 33s\tremaining: 11.6s\n",
            "890:\tlearn: 0.0532825\ttotal: 1m 33s\tremaining: 11.5s\n",
            "891:\tlearn: 0.0532689\ttotal: 1m 34s\tremaining: 11.4s\n",
            "892:\tlearn: 0.0532655\ttotal: 1m 34s\tremaining: 11.3s\n",
            "893:\tlearn: 0.0532631\ttotal: 1m 34s\tremaining: 11.2s\n",
            "894:\tlearn: 0.0532533\ttotal: 1m 34s\tremaining: 11.1s\n",
            "895:\tlearn: 0.0532515\ttotal: 1m 34s\tremaining: 11s\n",
            "896:\tlearn: 0.0532485\ttotal: 1m 34s\tremaining: 10.9s\n",
            "897:\tlearn: 0.0532426\ttotal: 1m 34s\tremaining: 10.7s\n",
            "898:\tlearn: 0.0532373\ttotal: 1m 34s\tremaining: 10.6s\n",
            "899:\tlearn: 0.0532149\ttotal: 1m 34s\tremaining: 10.5s\n",
            "900:\tlearn: 0.0532130\ttotal: 1m 34s\tremaining: 10.4s\n",
            "901:\tlearn: 0.0532105\ttotal: 1m 35s\tremaining: 10.3s\n",
            "902:\tlearn: 0.0532084\ttotal: 1m 35s\tremaining: 10.2s\n",
            "903:\tlearn: 0.0532019\ttotal: 1m 35s\tremaining: 10.1s\n",
            "904:\tlearn: 0.0531931\ttotal: 1m 35s\tremaining: 10s\n",
            "905:\tlearn: 0.0531922\ttotal: 1m 35s\tremaining: 9.9s\n",
            "906:\tlearn: 0.0531879\ttotal: 1m 35s\tremaining: 9.8s\n",
            "907:\tlearn: 0.0531776\ttotal: 1m 35s\tremaining: 9.69s\n",
            "908:\tlearn: 0.0531752\ttotal: 1m 35s\tremaining: 9.59s\n",
            "909:\tlearn: 0.0531665\ttotal: 1m 35s\tremaining: 9.48s\n",
            "910:\tlearn: 0.0531624\ttotal: 1m 35s\tremaining: 9.37s\n",
            "911:\tlearn: 0.0531585\ttotal: 1m 36s\tremaining: 9.27s\n",
            "912:\tlearn: 0.0531535\ttotal: 1m 36s\tremaining: 9.16s\n",
            "913:\tlearn: 0.0531475\ttotal: 1m 36s\tremaining: 9.06s\n",
            "914:\tlearn: 0.0531428\ttotal: 1m 36s\tremaining: 8.95s\n",
            "915:\tlearn: 0.0531360\ttotal: 1m 36s\tremaining: 8.84s\n",
            "916:\tlearn: 0.0531311\ttotal: 1m 36s\tremaining: 8.74s\n",
            "917:\tlearn: 0.0531277\ttotal: 1m 36s\tremaining: 8.63s\n",
            "918:\tlearn: 0.0531174\ttotal: 1m 36s\tremaining: 8.53s\n",
            "919:\tlearn: 0.0531123\ttotal: 1m 36s\tremaining: 8.42s\n",
            "920:\tlearn: 0.0530991\ttotal: 1m 36s\tremaining: 8.32s\n",
            "921:\tlearn: 0.0530932\ttotal: 1m 37s\tremaining: 8.21s\n",
            "922:\tlearn: 0.0530858\ttotal: 1m 37s\tremaining: 8.11s\n",
            "923:\tlearn: 0.0530753\ttotal: 1m 37s\tremaining: 8s\n",
            "924:\tlearn: 0.0530729\ttotal: 1m 37s\tremaining: 7.89s\n",
            "925:\tlearn: 0.0530666\ttotal: 1m 37s\tremaining: 7.79s\n",
            "926:\tlearn: 0.0530613\ttotal: 1m 37s\tremaining: 7.68s\n",
            "927:\tlearn: 0.0530603\ttotal: 1m 37s\tremaining: 7.58s\n",
            "928:\tlearn: 0.0530578\ttotal: 1m 37s\tremaining: 7.47s\n",
            "929:\tlearn: 0.0530539\ttotal: 1m 37s\tremaining: 7.37s\n",
            "930:\tlearn: 0.0530479\ttotal: 1m 37s\tremaining: 7.26s\n",
            "931:\tlearn: 0.0530427\ttotal: 1m 38s\tremaining: 7.15s\n",
            "932:\tlearn: 0.0530329\ttotal: 1m 38s\tremaining: 7.05s\n",
            "933:\tlearn: 0.0530254\ttotal: 1m 38s\tremaining: 6.94s\n",
            "934:\tlearn: 0.0530190\ttotal: 1m 38s\tremaining: 6.84s\n",
            "935:\tlearn: 0.0530097\ttotal: 1m 38s\tremaining: 6.73s\n",
            "936:\tlearn: 0.0529756\ttotal: 1m 38s\tremaining: 6.63s\n",
            "937:\tlearn: 0.0529716\ttotal: 1m 38s\tremaining: 6.52s\n",
            "938:\tlearn: 0.0529605\ttotal: 1m 38s\tremaining: 6.42s\n",
            "939:\tlearn: 0.0529586\ttotal: 1m 38s\tremaining: 6.31s\n",
            "940:\tlearn: 0.0529573\ttotal: 1m 38s\tremaining: 6.2s\n",
            "941:\tlearn: 0.0529493\ttotal: 1m 39s\tremaining: 6.1s\n",
            "942:\tlearn: 0.0529466\ttotal: 1m 39s\tremaining: 5.99s\n",
            "943:\tlearn: 0.0529459\ttotal: 1m 39s\tremaining: 5.89s\n",
            "944:\tlearn: 0.0529403\ttotal: 1m 39s\tremaining: 5.78s\n",
            "945:\tlearn: 0.0529372\ttotal: 1m 39s\tremaining: 5.68s\n",
            "946:\tlearn: 0.0529337\ttotal: 1m 39s\tremaining: 5.57s\n",
            "947:\tlearn: 0.0529309\ttotal: 1m 39s\tremaining: 5.46s\n",
            "948:\tlearn: 0.0529253\ttotal: 1m 39s\tremaining: 5.36s\n",
            "949:\tlearn: 0.0529229\ttotal: 1m 39s\tremaining: 5.25s\n",
            "950:\tlearn: 0.0529187\ttotal: 1m 39s\tremaining: 5.15s\n",
            "951:\tlearn: 0.0529142\ttotal: 1m 40s\tremaining: 5.04s\n",
            "952:\tlearn: 0.0529115\ttotal: 1m 40s\tremaining: 4.94s\n",
            "953:\tlearn: 0.0529085\ttotal: 1m 40s\tremaining: 4.83s\n",
            "954:\tlearn: 0.0529022\ttotal: 1m 40s\tremaining: 4.73s\n",
            "955:\tlearn: 0.0528941\ttotal: 1m 40s\tremaining: 4.62s\n",
            "956:\tlearn: 0.0528894\ttotal: 1m 40s\tremaining: 4.52s\n",
            "957:\tlearn: 0.0528753\ttotal: 1m 40s\tremaining: 4.41s\n",
            "958:\tlearn: 0.0528744\ttotal: 1m 40s\tremaining: 4.31s\n",
            "959:\tlearn: 0.0528731\ttotal: 1m 40s\tremaining: 4.2s\n",
            "960:\tlearn: 0.0528673\ttotal: 1m 40s\tremaining: 4.1s\n",
            "961:\tlearn: 0.0528616\ttotal: 1m 41s\tremaining: 3.99s\n",
            "962:\tlearn: 0.0528607\ttotal: 1m 41s\tremaining: 3.88s\n",
            "963:\tlearn: 0.0528586\ttotal: 1m 41s\tremaining: 3.78s\n",
            "964:\tlearn: 0.0528535\ttotal: 1m 41s\tremaining: 3.67s\n",
            "965:\tlearn: 0.0528524\ttotal: 1m 41s\tremaining: 3.57s\n",
            "966:\tlearn: 0.0528507\ttotal: 1m 41s\tremaining: 3.46s\n",
            "967:\tlearn: 0.0528495\ttotal: 1m 41s\tremaining: 3.36s\n",
            "968:\tlearn: 0.0528428\ttotal: 1m 41s\tremaining: 3.25s\n",
            "969:\tlearn: 0.0528369\ttotal: 1m 41s\tremaining: 3.15s\n",
            "970:\tlearn: 0.0528338\ttotal: 1m 41s\tremaining: 3.04s\n",
            "971:\tlearn: 0.0528150\ttotal: 1m 42s\tremaining: 2.94s\n",
            "972:\tlearn: 0.0528134\ttotal: 1m 42s\tremaining: 2.83s\n",
            "973:\tlearn: 0.0528099\ttotal: 1m 42s\tremaining: 2.73s\n",
            "974:\tlearn: 0.0528088\ttotal: 1m 42s\tremaining: 2.62s\n",
            "975:\tlearn: 0.0528078\ttotal: 1m 42s\tremaining: 2.52s\n",
            "976:\tlearn: 0.0528051\ttotal: 1m 42s\tremaining: 2.41s\n",
            "977:\tlearn: 0.0528029\ttotal: 1m 42s\tremaining: 2.31s\n",
            "978:\tlearn: 0.0528004\ttotal: 1m 42s\tremaining: 2.2s\n",
            "979:\tlearn: 0.0527931\ttotal: 1m 42s\tremaining: 2.1s\n",
            "980:\tlearn: 0.0527869\ttotal: 1m 42s\tremaining: 1.99s\n",
            "981:\tlearn: 0.0527842\ttotal: 1m 43s\tremaining: 1.89s\n",
            "982:\tlearn: 0.0527708\ttotal: 1m 43s\tremaining: 1.78s\n",
            "983:\tlearn: 0.0527658\ttotal: 1m 43s\tremaining: 1.68s\n",
            "984:\tlearn: 0.0527635\ttotal: 1m 43s\tremaining: 1.57s\n",
            "985:\tlearn: 0.0527462\ttotal: 1m 43s\tremaining: 1.47s\n",
            "986:\tlearn: 0.0527429\ttotal: 1m 43s\tremaining: 1.36s\n",
            "987:\tlearn: 0.0527397\ttotal: 1m 43s\tremaining: 1.26s\n",
            "988:\tlearn: 0.0527383\ttotal: 1m 43s\tremaining: 1.15s\n",
            "989:\tlearn: 0.0527353\ttotal: 1m 43s\tremaining: 1.05s\n",
            "990:\tlearn: 0.0527307\ttotal: 1m 43s\tremaining: 944ms\n",
            "991:\tlearn: 0.0527224\ttotal: 1m 44s\tremaining: 839ms\n",
            "992:\tlearn: 0.0527176\ttotal: 1m 44s\tremaining: 734ms\n",
            "993:\tlearn: 0.0527161\ttotal: 1m 44s\tremaining: 629ms\n",
            "994:\tlearn: 0.0527095\ttotal: 1m 44s\tremaining: 524ms\n",
            "995:\tlearn: 0.0527054\ttotal: 1m 44s\tremaining: 420ms\n",
            "996:\tlearn: 0.0527042\ttotal: 1m 44s\tremaining: 315ms\n",
            "997:\tlearn: 0.0527012\ttotal: 1m 44s\tremaining: 210ms\n",
            "998:\tlearn: 0.0526979\ttotal: 1m 44s\tremaining: 105ms\n",
            "999:\tlearn: 0.0526851\ttotal: 1m 44s\tremaining: 0us\n",
            "cat_model.sav\n",
            "Accuracy of Model: 0.9837523582181497\n",
            "Precision of Model: 0.9799815512719356\n",
            "Recall of Model: 0.9837523582181497\n",
            "F1-score of Model: 0.9798182222387176\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     21576\n",
            "           1       0.99      1.00      1.00       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           4       1.00      1.00      1.00      1098\n",
            "           5       0.75      1.00      0.86         3\n",
            "           6       1.00      1.00      1.00       922\n",
            "           7       1.00      0.98      0.99        66\n",
            "           8       1.00      1.00      1.00       739\n",
            "           9       0.78      0.48      0.59       224\n",
            "          10       1.00      0.89      0.94        18\n",
            "          11       0.70      0.90      0.79       309\n",
            "          12       0.51      0.08      0.14       259\n",
            "          14       1.00      1.00      1.00       300\n",
            "\n",
            "    accuracy                           0.98     25973\n",
            "   macro avg       0.82      0.79      0.79     25973\n",
            "weighted avg       0.98      0.98      0.98     25973\n",
            "\n",
            "light_model.sav\n",
            "Accuracy of Model: 0.8622030570207523\n",
            "Precision of Model: 0.8234053988782165\n",
            "Recall of Model: 0.8622030570207523\n",
            "F1-score of Model: 0.8407571631800697\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94     21576\n",
            "           1       0.47      0.87      0.61       458\n",
            "           2       0.00      0.00      0.00         1\n",
            "           3       0.00      0.00      0.00         0\n",
            "           4       0.02      0.00      0.00      1098\n",
            "           5       0.00      0.00      0.00         3\n",
            "           6       0.73      0.62      0.67       922\n",
            "           7       0.00      0.00      0.00        66\n",
            "           8       0.80      0.91      0.85       739\n",
            "           9       0.00      0.00      0.00       224\n",
            "          10       0.00      0.00      0.00        18\n",
            "          11       0.00      0.00      0.00       309\n",
            "          12       0.01      0.01      0.01       259\n",
            "          14       0.00      0.00      0.00       300\n",
            "\n",
            "    accuracy                           0.86     25973\n",
            "   macro avg       0.21      0.24      0.22     25973\n",
            "weighted avg       0.82      0.86      0.84     25973\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9J4bi6X0Gjh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhf0uMi2_3r5"
      },
      "source": [
        "The End ..."
      ]
    }
  ]
}